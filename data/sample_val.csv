paper,question,answer,reference_figure,reference_figure_caption,generated_answer
1705.08868v2,Which objective function resulted in the highest MODE score?,ADV,1705.08868v2-Table1-1.png, Best MODE scores and test negative log-likelihood estimates for Flow-GAN models on MNIST.,
1705.08868v2,Which objective function achieved the best Inception score?,"The ADV objective function achieved the best Inception score, with a score of 5.76.",1705.08868v2-Table2-1.png, Best Inception scores and test negative loglikelihood estimates for Flow-GAN models on CIFAR-10.,
1705.08868v2,Do Gaussian Mixture Models or adversarially learned models perform better on CIFAR-10?,Gaussian Mixture Models.,1705.08868v2-Figure3-1.png, Gaussian Mixture Models outperform adversarially learned models on both held-out log-likelihoods and sampling metrics on CIFAR-10 (green shaded region).,
1705.08868v2,Which method produced the generator with the most singular values close to zero?,The hybrid method.,1705.08868v2-Figure4-1.png, CDF of the singular values magnitudes for the Jacobian of the generator functions trained on MNIST.,
1705.08868v2,Which objective function resulted in the lowest negative log-likelihood (NLL) for the Flow-GAN model on MNIST?,The hybrid objective function resulted in the lowest NLL for the Flow-GAN model on MNIST.,1705.08868v2-Table3-1.png, Comparison of inference techniques for negative log-likelihood estimation of Flow-GAN models on MNIST.,
1705.08868v2,"Which method produces the most stable generator, as measured by the magnitude of the singular values of its Jacobian?",The Hybrid method.,1705.08868v2-Figure8-1.png, CDF of singular values magnitudes for the Jacobian of the generator function on CIFAR-10.,
1705.08868v2,"Which model performs better in terms of validation NLL, the ADV model or the Gaussian Mixture Model?",The Gaussian Mixture Model performs better in terms of validation NLL.,1705.08868v2-Figure6-1.png, Gaussian Mixture Models outperform adversarially learned models on both held-out log-likelihoods and sampling metrics on MNIST (green shaded region).,
1705.08868v2,What is the difference between the two images?,"The top image is a sample from a Gaussian Mixture Model (GMM) trained on the MNIST dataset, while the bottom image is a sample from a GMM trained on the CIFAR-10 dataset. The MNIST dataset consists of handwritten digits, while the CIFAR-10 dataset consists of natural images. This is reflected in the different types of images generated by the two GMMs.",1705.08868v2-Figure7-1.png, Samples from the Gaussian Mixture Model baseline for MNIST (top) and CIFAR-10 (bottom) with better MODE/Inception scores than ADV models.,
1705.08868v2,"What is the difference between the samples generated by the MLE, ADV, and Hybrid Flow-GAN models?",The MLE model generates samples that are more realistic than the ADV and Hybrid models. The ADV model generates samples that are more diverse than the MLE and Hybrid models. The Hybrid model generates samples that are both realistic and diverse.,1705.08868v2-Figure1-1.png, Samples generated by Flow-GAN models with different objectives for MNIST (top) and CIFAR-10 (bottom).,
1705.08868v2,How does the performance of the ADV model compare to the MLE model on the MNIST and CIFAR-10 datasets?,The ADV model outperforms the MLE model on both the MNIST and CIFAR-10 datasets.,1705.08868v2-Figure5-1.png, Sample quality curves during training.,
1705.08868v2,How does the NLL of the training and validation sets compare for the MLE and ADV models on the MNIST dataset?,"The NLL of the training and validation sets for the MLE model on the MNIST dataset are very close to each other, while the NLL of the training and validation sets for the ADV model on the MNIST dataset are further apart.",1705.08868v2-Figure2-1.png," Learning curves for negative log-likelihood (NLL) evaluation on MNIST (top, in nats) and CIFAR (bottom, in bits/dim). Lower NLLs are better.",
1711.05435v1,How does the calculation time of TransE and TorusE change with increasing dimension of the models?,The calculation time of both TransE and TorusE increases with increasing dimension of the models.,1711.05435v1-Figure4-1.png, Calculation time of TorusE and TransE on WN18 and FB15K,
1711.05435v1,"Which embedding method performed best for the relation ""similar_to""?","Both TorusE and ComplEx performed equally well for the relation ""similar_to"", with an MRR of 1.000.",1711.05435v1-Table4-1.png, Details of “filtered” MRR on WN18. The results are listed separately for each relation contained in the dataset.,
1711.05435v1,Which model performed best on the FB15K dataset according to the MRR metric?,ComplEx,1711.05435v1-Table3-1.png," Results of the link prediction tasks by Mean Reciprocal Rank (MRR) and HITS@n on WN18 and FB15K datasets. MRRs are calculated by using the raw and filtered ranking and HITS@n are calculated by using the filtered ranking. The dimension of TransE was set to 10,000, and the best hyperparameters were chosen by using the validation set. The results of TransR and RESCAL were reported by Nickel et al. (2016), the results of DistMult and ComplEx were reported by Trouillon et al. (2016)",
1711.05435v1,"What is the relationship between the points A, B, and C and their corresponding points A', B', and C'?","The points A, B, and C are related to their corresponding points A', B', and C' by a translation.",1711.05435v1-Figure1-1.png," The image of embeddings obtained by TransE when n is 2. It is assumed that (A, r, A′), (B, r, B′) and (C, r,C′) hold.",
1711.05435v1,How does the derivative of the scoring function f_L1 compare to the derivative of the scoring function f_eL2?,The derivative of the scoring function f_L1 is always greater than the derivative of the scoring function f_eL2.,1711.05435v1-Figure2-1.png," The graphs of scoring functions and their derivatives for TorusE when n = 1. f ′L1 , f ′L2 and f ′eL2 are derivatives of the scoring functions.",
1711.05435v1,Which of the scoring functions listed in the table has the highest space complexity?,O_space.,1711.05435v1-Table1-1.png," Scoring functions for triple (h, r, t), parameters and complexity of related work.",
1711.05435v1,Which dataset has more relations?,FB15K,1711.05435v1-Table2-1.png, Statistics of the datasets.,
1711.05435v1,What is the relationship between the embeddings of A and A' and B and B' on the torus?,The embeddings of A and A' and B and B' are similar on the torus.,1711.05435v1-Figure3-1.png," The image of embeddings on 2-dimensional torus obtained by TorusE. Embeddings of the triples (A, r, A′) and (B, r, B′) are illustrated. Note that [A′] − [A] and [B′] − [B] are similar on the torus.",
1711.05953v4, What is the purpose of the FaceLFnet in the proposed pipeline for 3D face reconstruction from a single light field image?, The FaceLFnet is used to regress 3D facial curves over horizontal and vertical EPIs (epipolar plane images) of the input light field image.,1711.05953v4-Figure1-1.png," Proposed pipeline for 3D face reconstruction from a single light field image. Using synthetic light field face images, we train two FaceLFnets for regressing 3D facial curves over their respective horizontal and vertical EPIs. The estimated depth maps are combined, using camera parameters, into a single pointcloud to which a surface is fitted to get the final 3D face.",
1711.05953v4,What does the second row of the figure show?,The second row of the figure shows that the method is robust to illumination variations and also works well in the case of dark skin.,1711.05953v4-Figure8-1.png, Invariance to illumination and skin color. Our method is robust to illumination variations and also works well in the case of dark skin (second row).,
1711.05953v4,What does the image show?,"The image shows the results of a 3D face reconstruction method. The first column in each row shows the input image, the second column shows the ground truth 3D face, the third column shows the reconstructed 3D face, and the fourth column shows the ground truth and reconstructed 3D faces overlaid on each other.",1711.05953v4-Figure6-1.png," Pose invariance. Columns one to four in each row respectively depict the input central view of the light field image, the ground truth 3D face, the reconstructed 3D face by our proposed method and the last two overlaid on each other.",
1711.05953v4,How does the proposed method handle exaggerated expressions?,"The proposed method can handle exaggerated expressions by accurately reconstructing the 3D face even when the expression is exaggerated. This is evident from the comparison of the ground truth 3D face and the reconstructed 3D face in the figure. Even though the expressions in the input images are exaggerated, the reconstructed 3D faces closely resemble the ground truth 3D faces.",1711.05953v4-Figure7-1.png," Expression invariance. As shown, our method can handle exaggerated expressions. Columns one to four in each row respectively depict the input central view of the light field image, the ground truth 3D face, the reconstructed 3D face by our proposed method and the last two overlaid on each other.",
1711.05953v4,What are some of the variations that can be seen in the generated light field images?,"The generated light field images use random backgrounds and differ extensively in ethnicity, gender, age, pose and illumination.",1711.05953v4-Figure2-1.png," Central view examples of our rendered light field images. The ground truth 3D scans are aligned with the central view. To make the dataset rich in variations, the generated light field images use random backgrounds and differ extensively in ethnicity, gender, age, pose and illumination.",
1711.05953v4,Which method achieved the lowest reconstruction error on the BU-4DFE dataset?,Ours,1711.05953v4-Table3-1.png, Reconstruction errors on the BU-4DFE dataset [60] in terms of NME defined in Eq. (2). ICP has been used to align the reconstructed face to the ground truth similar to [25].,
1711.05953v4,Which method has the smallest mean error?,Ours.,1711.05953v4-Table2-1.png," Comparative results on the BU-3DFE dataset [58]. The absolute RMSE between ground truth and predicted shapes evaluated by mean, standard deviation, median and the average ninety percent largest error of the different methods are presented.",
1711.05953v4,Which facial expression has the highest reconstruction error?,Sad,1711.05953v4-Figure11-1.png, Reconstruction errors for different facial expressions on the BU-4DFE dataset [60]. The RMSE increases from 2.49 to 2.98 (by only 0.49 mm) under extreme expression variations. Sad has the highest error whereas surprise has the lowest because of more edges around the lips which favors EPI based reconstruction.,
1711.05953v4,What are the differences between the ground truth 3D face and the 3D face reconstructed by the proposed method?,The ground truth 3D face is more detailed and has sharper features than the 3D face reconstructed by the proposed method.,1711.05953v4-Figure9-1.png," Qualitative results. The columns contain (in order) central view image, the ground truth 3D face, 3D face reconstructed by our method and 3D face reconstructed by Sela et al. [44].",
1711.05953v4,Which facial pose has the highest reconstruction error?,Left-30°,1711.05953v4-Figure10-1.png, Reconstruction errors for different facial poses on the BU-3DFE dataset [58]. Note that the RMSE increases from 2.62 to 2.93 (by 0.31 mm only) under extreme pose variations.,
1711.05953v4,What is the relationship between the EPIs and the 3D face curves?,The EPIs are used to generate the 3D face curves.,1711.05953v4-Figure4-1.png, Examples of EPIs and their corresponding 3D face curves.(a) Horizontal EPIs. (b) Vertical EPIs.,
1711.05953v4,How does the depth of a point on the face relate to the slope of the corresponding line in the horizontal and vertical EPIs?,"The depth of a point on the face is inversely proportional to the slope of the corresponding line in the horizontal and vertical EPIs. In other words, the closer a point is to the camera, the steeper the slope of the corresponding line will be.",1711.05953v4-Figure3-1.png, EPIs corresponding to the 3D face curves. (a) Horizontal and vertical EPIs are obtained between the central view and sub-aperture images that are in the same row and column. (b) and (c) Visualization of the relationship between depth curves and slopes of lines in horizontal and vertical EPIs respectively.,
1711.05953v4, What is the role of the transition layers in the FaceLFnet architecture? , The transition layers change the feature map sizes via convolution and pooling.,1711.05953v4-Figure5-1.png," Our proposed FaceLFnet for learning 3D face curves from EPIs. It contains 4 dense blocks, followed by two fully connected layers. The layers between two neighboring blocks are defined as transition layers and change feature map sizes via convolution and pooling [23].",
1711.05953v4,What is the output size of the third transition layer?,4 x 100,1711.05953v4-Table1-1.png, Our proposed FaceLFnet architecture. Note that each convolutional layer in the dense block corresponds to the sequence BN-ReLU. The growth rate of the four blocks is k = 12.,
1711.09573v2," 
Based on the figure, which model performs the best in predicting the out-of-vocabulary value `employee_id`? "," 
The Pointer Mixture Network performs the best in predicting the out-of-vocabulary value `employee_id`.",1711.09573v2-Figure4-1.png, A code completion example showing predicting an OoV value.,
1711.09573v2,Which model performed the best on the JS_1k dataset?,The Pointer Mixture Network performed the best on the JS_1k dataset.,1711.09573v2-Table4-1.png, Showing why pointer mixture network works.,
1711.09573v2,"What does the ""NameStore: my_salary"" node in the AST represent?","The ""NameStore: my_salary"" node represents the assignment of the value 0 to the variable ""my_salary"".",1711.09573v2-Figure1-1.png, A Python program and its corresponding abstract syntax tree (AST). The dashed arrow points to a parent node.,
1711.09573v2,What is the role of the attention scores in the attentional LSTM model?,"The attention scores are used to weight the hidden states of the previous LSTM cells, which are then used to compute the context vector. The context vector is then used as input to the current LSTM cell.",1711.09573v2-Figure2-1.png, The attentional LSTM. The inputs fed to each LSTM cell are composed of two kinds of embeddings (green for Type and yellow for Value). Here ⊗ represents the element-wise multiplication.,
1711.09573v2,What is the role of the switcher in the pointer mixture network?,The switcher produces a value st between 0 and 1 that balances the pointer distribution lt and the RNN distribution wt.,1711.09573v2-Figure3-1.png," The pointer mixture network. We reuse the attention scores αt (see Figure 2) as the pointer distribution lt. The switcher produces st ∈ [0, 1] to balance lt and wt. The final distribution is generated by concatenating the two scaled distributions. Here ⊕ indicates the concatenation operation.",
1711.09573v2,Which model performs the best on the JS_1k dataset?,Pointer Mixture Network,1711.09573v2-Table1-1.png, Accuracies on next value prediction with different vocabulary sizes. The out-of-vocabulary (OoV) rate denotes the percentage of AST nodes whose value is beyond the global vocabulary. Localness is the percentage of values who are OoV but occur in the context window.,
1711.09573v2,Which dataset has more training queries?,JS has more training queries.,1711.09573v2-Table2-1.png, Dataset Statistics,
1711.09573v2,Which model achieved the highest accuracy for next node type prediction?,"The Attentional LSTM (ours) model achieved the highest accuracy for next node type prediction, with an accuracy of 88.6%.",1711.09573v2-Table3-1.png, Comparisons against the state-of-the-arts. The upper part is the results from our experiments while the lower part is the results from the prior work. TYPE means next node type prediction and VALUE means next node value prediction.,
1711.11191v1,What is the recall of the model when N is 5k?,88.39,1711.11191v1-Table4-1.png, Ground truth word coverage.,
1711.11191v1,How does the performance of the three algorithms change as the number of content words increases?,The performance of all three algorithms decreases as the number of content words increases.,1711.11191v1-Table5-1.png, Performance in terms of content word number.,
1711.11191v1,Which method is the most efficient for beam search when implemented on a GPU?,S2S-A is the most efficient method for beam search when implemented on a GPU.,1711.11191v1-Figure2-1.png, Efficiency comparison,
1711.11191v1,Which model has the highest average BLEU score?,DVS2S,1711.11191v1-Table1-1.png," Automatic evaluation results. Numbers in bold mean that improvement from the model on that metric is statistically significant over the baseline methods (t-test, p-value < 0.01).",
1711.11191v1,Which model had the lowest accuracy on class 0?,DVS2S,1711.11191v1-Table2-1.png, Human evaluation results. The ratios are calculated by combining the annotations from the three judges together.,
1711.11191v1,How does the DVS2S model determine the probability of generating a word?,"The DVS2S model uses a combination of a static list and a dynamic vocabulary to determine the probability of generating a word. The static list contains a fixed set of common words, while the dynamic vocabulary is constructed based on the input sentence. The model uses an attention mechanism to focus on relevant parts of the input sentence, and then uses this information to compute the probability of generating each word in the vocabulary.",1711.11191v1-Figure1-1.png, Architecture of DVS2S. We translate a Chinese example into English and show the generation process of the word “lab”. Words in blue squares refers to those chosen by the dynamic vocabulary otherwise they are in black squares.,
1801.09041v1,What is the relationship between sentence accuracy and sentence-question relevance on VQA performance?,"Sentence accuracy and sentence-question relevance are both positively correlated with VQA performance. As sentence accuracy and sentence-question relevance increase, VQA performance also increases.",1801.09041v1-Figure6-1.png, The comparison of the impact of sentence accuracy and sentence-question relevance on VQA performance.,
1801.09041v1,"Which caption source results in the highest accuracy for the ""All"" category?","The relevant groundtruth caption source results in the highest accuracy for the ""All"" category, with an accuracy of 56.05%.",1801.09041v1-Table3-1.png, Performance comparison on the validation split of VQA-real open-ended task when the sentence-based VQA model uses different sources of captions. (accuray in %),
1801.09041v1,What sport are the people in the image playing?,The people in the image are playing frisbee.,1801.09041v1-Figure7-1.png," A control case for comparing the accuracy when inputting captions of different quality. When getting a caption of high quality (the first one), the system can answer the question correctly. If we manually change the “frisbee” to “soccer”, a wrong answer is predicted. When using an empty sentence, the system predicts the most popular answer “tennis” for this question.",
1801.09041v1,What is the difference between a high relevance and a low relevance answer?,"A high relevance answer is one that is directly related to the image, while a low relevance answer is one that is only tangentially related to the image.",1801.09041v1-Figure8-1.png," Four types of cases in our results: 1). high relevance and correct answer; 2). low relevance and wrong answer; 3). high relevance but wrong answer; 4). low relevance but correct answer. “(*,*)” behind the explanations (attributes/caption)",
1801.09041v1,What is the percentage of questions with correct answers?,16.7%.,1801.09041v1-Figure9-1.png, Dataset dissection acc rding to the four types of cases. We define that the answer is guessed when the explanations are irrelevant to the question and otherwise reliable. The case numbers in the third row correspond to these in Fig.8. QA: all questions and answers. CA: questions with cor ect answers. WA: questions with wrong answers. GA: questions with guessed answers. R : ti it reliable s rs. Y/N: answer type “yes/no”. O: answer types other than “yes/no”.,
1801.09041v1,What is the difference between the single-label and multi-label images?,"Single-label images have only one label associated with them, while multi-label images have multiple labels associated with them.",1801.09041v1-Figure4-1.png, Word prediction CNN: the model is firstly initialized from ResNet-152 pre-trained on ImageNet. Then the model is fine-tuned on our image-words dataset built from MS COCO captions.,
1801.09041v1,What are the three modules of the proposed framework for VQA?,"The three modules of the proposed framework for VQA are word prediction, sentence generation, and answer reasoning.",1801.09041v1-Figure3-1.png," An overview of the proposed framework for VQA with three modules: word prediction (upper left), sentence generation (lower left), answer reasoning (right). Explaining: in word prediction, the image is fed into pre-trained visual detectors to extract word-level explanation, which is represented by probability vector vw; in sentence generation, we input the image to pre-trained captioning model to generate a sentence-level explanation. Reasoning: the caption and question are encoded by two different LSTMs into vs and vq , respectively. Then vq,vw and vs are concatenated and fed to a fully connected layer with softmax to predict an answer.",
1801.09041v1,Does word accuracy have a greater impact on VQA accuracy or W-Q relevance?,Word accuracy has a greater impact on VQA accuracy than W-Q relevance.,1801.09041v1-Table1-1.png, The relationship between word quality and VQA accuracy (%).,
1801.09041v1,"Which metric has a greater impact on VQA performance, word accuracy or word-question relevance?",Word-question relevance has a greater impact on VQA performance.,1801.09041v1-Figure5-1.png, The comparison of the impact of word accuracy and word-question relevance on VQA performance.,
1801.09041v1,Does VQA accuracy increase with increasing sentence accuracy?,"Yes, VQA accuracy increases with increasing sentence accuracy.",1801.09041v1-Table2-1.png, The relationship between sentence quality and VQA accuracy (%).,
1803.05112v5,Which method performs the best on the Email data set?,The proposed method (MinMaxGau) performs the best on the Email data set.,1803.05112v5-Figure3-1.png, Average uplifts as well as their standard errors on real-world data sets.,
1803.05112v5,Which model appears to have the most accurate uplift estimates?,The proposed model (MinMaxGau) appears to have the most accurate uplift estimates.,1803.05112v5-Figure2-1.png," The plots show the squared errors of the estimated individual uplifts on the synthetic data with b = 1. Each point is darker-colored when |p1(t = 1 | x)− p2(t = 1 | x)| is smaller, and lighter-colored otherwise.",
1805.11204v2,Which model has the highest test accuracy?,"SPD-SRU, TT-GRU, and TT-LSTM all have the highest test accuracy of 0.78.",1805.11204v2-Table2-1.png, Comparative results on UCF11 data,
1805.11204v2,Which method is most robust to changes in orientation?,SPD-SRU.,1805.11204v2-Figure2-1.png, Comparison of testing accuracies with varying orientations,
1805.11204v2,Which model performs best in terms of accuracy on Moving MNIST for all orientation ranges?,SPD-SRU,1805.11204v2-Table1-1.png, Comparative results on Moving MNIST,
1805.11204v2,What is the relationship between the input variables and the output variables in the SPD-SRU and SRU layer?,"The input variables (X_t) are used to calculate the transition function (T_t), which is then used to update the internal state (S_t). The output variables (O_t) are calculated from the internal state (S_t) and the emission function (Phi_t).",1805.11204v2-Figure1-1.png, Sketch of an SPD-SRU and SRU layer (dashed line represnets dependence on the previous time point).,
1805.11686v3,What are the differences between the approaches shown in the top and bottom halves of the figure?,"The top half of the figure shows supervised learning approaches, while the bottom half shows imitation learning approaches. In supervised learning, the agent is given full expert demonstrations of how to complete the task. In imitation learning, the agent is only given the final desired outcome of the task.",1805.11686v3-Figure1-1.png," Standard IRL requires full expert demonstrations and aims to produce an agent that mimics the expert. VICE generalizes IRL to cases where we only observe final desired outcomes, which does not require the expert to actually know how to perform the task.",
1805.11686v3,What type of neural network architecture is used in the framework to learn event probabilities?,A convolutional neural network (CNN) followed by fully-connected layers.,1805.11686v3-Figure2-1.png," Our framework learns event probabilities from data. We use neural networks as function approximators to model this distribution, which allows us to work with high dimensional observations like images.",
1805.11686v3,What are the two tasks shown in the figure?,The two tasks shown in the figure are HalfCheetah and Lobber.,1805.11686v3-Figure4-1.png, HalfCheetah and Lobber tasks.,
1805.11686v3,What is the difference between the ANY and ALL queries?,"The ALL query results in superior returns, but the ANY query results in the agent reaching the target more accurately.",1805.11686v3-Table1-1.png," Results on HalfCheetah and Lobber tasks (5 trials). The ALL query generally results in superior returns, but the ANY query results in the agent reaching the target more accurately. Random refers to a random gaussian policy.",
1805.11686v3,Which method performs better on the Ant and Pusher tasks?,"VICE performs better than the classifier-based setup on all the tasks, and the performance is substantially better for the Ant and Pusher tasks.",1805.11686v3-Table2-1.png," Results on Maze, Ant and Pusher environments (5 trials). The metric reported is the final distance to the goal state (lower is better). VICE performs better than the classifier-based setup on all the tasks, and the performance is substantially better for the Ant and Pusher task. Detailed learning curves are provided in Appendix G.",
1805.11686v3,"In this figure, what do the variables $s_1, s_2, ..., s_T$ represent?","The variables $s_1, s_2, ..., s_T$ represent the states of the system at each time step.",1805.11686v3-Figure3-1.png," A graphical model framework for control. In maximum entropy reinforcement learning, we observe e1:T = 1 and can perform inference on the trajectory to obtain a policy.",
1805.11686v3,Which task requires the most demonstrations?,Pusher,1805.11686v3-Table3-1.png," Hyperparameters used for VICE on the Ant,Maze, and Pusher tasks",
1805.11686v3,What are the goals of the agent in each of the tasks shown in the figure?,"In the Maze and Ant tasks, the agent seeks to reach a pre-specified goal position. In the Pusher task, the agent seeks to place a block at the goal position.",1805.11686v3-Figure5-1.png," Visualizations of the Pusher, Maze, and Ant tasks. In the Maze and Ant tasks, the agent seeks to reach a pre-specified goal position. In the Pusher task, the agent seeks to place a block at the goal position.",
1805.11686v3,Which reward shaping method performed the best on the Pusher task?,VICE-ALL.,1805.11686v3-Figure6-1.png," Results on the Pusher task (lower is better), averaged across five random seeds. VICE significantly outperforms the naive classifier and true binary event indicators. Further, the performance is comparable to learning from an oracle hand-engineered reward (denoted in dashed lines). Curves for the Ant and Maze tasks can be seen in Appendix G.",
1805.11686v3,Which method performs the best on the Ant task?,VICE-ALL and VICE-ANY.,1805.11686v3-Figure7-1.png," Learning curves for the various methods for the Ant,Maze, and Pusher tasks, averaged across five random seeds. On all three domains, VICE-ALL and VICE-ANY successfully solve the task consistently, while the naive classifier fails often. Although the binary indicator works reasonably on the Maze task, it fails to solve the task in the more challenging environments.",
1805.11686v3,"How do the learning curves for the Ant, Maze, and Pusher tasks differ between the VICE-ALL, VICE-ANY, and CLS-ALL conditions?","The learning curves for the Ant, Maze, and Pusher tasks differ between the VICE-ALL, VICE-ANY, and CLS-ALL conditions in terms of their overall trend and the amount of variability across runs. 

- In the VICE-ALL condition, the learning curves generally show a downward trend, indicating that the agent is learning to solve the task over time. However, there is a considerable amount of variability across runs, especially for the Ant and Maze tasks. 

- In the VICE-ANY condition, the learning curves are more erratic and do not show a clear downward trend. This suggests that the agent is not learning as effectively in this condition. 

- In the CLS-ALL condition, the learning curves show a more consistent downward trend than in the other two conditions, and there is less variability across runs. This suggests that the agent is learning more effectively in this condition.",1805.11686v3-Figure8-1.png," Learning curves for all methods on each of the five random seeds for the Ant,Maze, and Pusher tasks. The mean across the five runs is depicted in bold.",
1806.00250v1,Which dataset is the easiest for the DCN to classify?,MNIST,1806.00250v1-Figure3-1.png, List of image classification datasets used for characterization. The datasets are sorted by the DCN value from the easiest (left) to the hardest (right).,
1806.00250v1,Which method has the best predictive performance in Scenario A?,TAP.,1806.00250v1-Figure4-1.png," Superior predictive performance of TAP compared with state-of-the-art methods, both when trained on only one dataset (Scenario A) or on multiple datasets (Scenario B).",
1806.00250v1,How does the TAPAS workflow use past experiments to predict the accuracy of a new network architecture on a new dataset?,"The TAPAS workflow uses past experiments to predict the accuracy of a new network architecture on a new dataset by first characterizing the difficulty of the new dataset using the Dataset Characterization Number (DCN). This number is then used to select a subset of experiments executed on similarly difficult datasets from the Lifelong Database of Experiments (LDE). These filtered experiments are then used to train the Train-less Accuracy Predictor (TAP), which takes the network architecture structure and the dataset DCN and predicts the peak accuracy reachable after training.",1806.00250v1-Figure1-1.png," Schematic TAPAS workflow. First row: the Dataset Characterization (DC) takes a new, unseen dataset and characterizes its difficulty by computing the Dataset Characterization Number (DCN). This number is then used to select a subset of experiments executed on similarly difficult datasets from the Lifelong Database of Experiments (LDE). Subsequently, the filtered experiments are used to train the Train-less Accuracy Predictor (TAP), an operation that takes up to a few minutes. Second row: the trained TAP takes the network architecture structure and the dataset DCN and predict the peak accuracy reachable after training. This phase scales very efficiently in a few seconds over a large number of networks.",
1806.00250v1,Which model performs best on the unseen dataset?,TAP trained only on LDE experiments.,1806.00250v1-Figure5-1.png," Predicted vs real performance (i.e., after training) for Scenario C. Left plot: TAP trained without DCN or LDE pre-filtering. Middle plot: TAP trained with DCN, but LDE is not pre-filtered. Right plot: TAP trained only on LDE experiments with similar dataset difficulty, according to (1).",
1806.00250v1,How does the accuracy of the predicted networks compare to the trained networks and the reference work?,"The predicted networks are slightly more accurate than the trained networks, but both are less accurate than the reference work.",1806.00250v1-Figure6-1.png," Simulation of large-scale evolution, with 20k mutations. The table compares top three networks (predicted and trained accuracy) with reference work [16]. The simulations require only 2 × 1011 FLOPs per dataset, while training the top-three networks for 100 epochs is an additional 3× 1015 FLOPs, causing a 6 hour runtime on a single GPU. The reference work employs 9× 1019 (CIFAR-10) and 2× 1020 FLOPs (CIFAR-100) causing a runtime of 256 hours on 250 GPUs.",
1806.00250v1,What is the purpose of the encoding vector?,The encoding vector is used to iteratively predict the accuracy of the network.,1806.00250v1-Figure2-1.png," Encoding vector structure and its usage in the iterative prediction. a) The encoding vector contains two blocks: i-layer information and from input to i-layer sub-network information. b) The encoding vector is used by the TAP following an iterative scheme. Starting from Layer 1 (input) we encode and concatenate two layers at a time and feed them to the TAP. In the concatenated vector, the Accuracy field Ai of li is set to the predicted accuracy obtained from the previous TAP evaluation, whereas the one of Ai+1 corresponding to li+1 is always set to zero. For the input layer, we set A0 to 1/Nc, where Nc is the number of classes, assuming a random distribution. The final predicted accuracy ANl is the accuracy of the complete network.",
1806.07370v5,Which building block has the fastest inference time and what is its value?,1B-ASL-1 has the fastest inference time of 10.6 ms.,1806.07370v5-Table2-1.png," Comparison with networks for depthwise convolution. ASL makes the network faster and provides better results. B and DW denote the BN-ReLU layer and depthwise convolution, respectively. For a fair comparison of BN, we also conducted experiments on a network without BN-ReLU between depthwise convolution and last 1×1 convolution(1B-DW3-1).",
1806.07370v5,Which network architecture achieves the highest accuracy on the C10 dataset?,ASNNet(ours) achieves the highest accuracy on the C10 dataset.,1806.07370v5-Table3-1.png, Comparison with ShiftNet. Our results are better by a large margin. The last row shows that our result is better with a smaller number of parameters and depth.,
1806.07370v5,How do the shifted values help the network cover multiple receptive fields?,The shifted values help the network cover multiple receptive fields by allowing the network to learn features at different locations in the input image. This is shown in the figure by the fact that the shifted values are scattered to various positions.,1806.07370v5-Figure3-1.png, Trained shift values of each layer. Shifted values are scattered to various positions. This enables the network to cover multiple receptive fields.,
1806.07370v5,Which method resulted in the highest Top-1 accuracy on ImageNet?,Training Real,1806.07370v5-Table4-1.png, Ablation study using AS-ResNet-w32 on ImageNet. The improvement by using ASL originated from expanding the domain of a shift parameter from integer to real and learning shifts.,
1806.07370v5,Which network has the highest Top-1 accuracy?,AS-ResNet-w68,1806.07370v5-Table5-1.png," Comparison with other networks. Our networks achieved better results with similar number of parameters. Compared to MobileNetV2, our network runs faster although it has a larger number of FLOPs. Table is sorted by descending order of the number of parameters.",
1806.07370v5,Which network is faster at inference time?,Both MobileNetV2 and AS-ResNet have the same inference time.,1806.07370v5-Figure4-1.png, Comparison with MobileNetV2[18]. Our network achieves better results with the same inference time.,
1806.07370v5, What is the difference between Grouped Shift and ASL?," Grouped Shift applies a fixed shift amount to each group of channels, while ASL applies a different shift amount to each individual channel, and these shift amounts are learned during training. ",1806.07370v5-Figure1-1.png, Comparison of shift operation: (a) shifting is applied to each group and the shift amount is assigned heuristically[23] and (b) shifting is applied to each channel using shift parameters and they are optimized by training.,
1806.07370v5,How does the network width change as we go deeper into the network?,The network width increases as we go deeper into the network.,1806.07370v5-Table6-1.png, Network structure for AS-ResNet. Network width is controlled by base width w,
1806.07370v5,Which layer in the table has the fastest inference time?,The Elementwise Sum layer has the fastest inference time at 2 ms.,1806.07370v5-Table1-1.png, Comparison of inference time vs. number of FLOPs. A smaller number of FLOPs does not guarantee fast inference.,
1806.07370v5,Which of the following operations is the least efficient in terms of inference time per FLOP?,dw-conv3,1806.07370v5-Figure2-1.png, Ratio of time to FLOPs. This represents the inference time per 1M FLOPs. A lower value means that the unit runs efficiently. Time is measured using an Intel i7-5930K CPU with a single thread and averaged over 100 repetitions.,
1807.09601v1,How does LSN compare to HED and SRN in terms of the quality of the output feature vectors and subspaces?,LSN learns better feature vectors and subspaces than HED and SRN.,1807.09601v1-Figure5-1.png," Comparison of output feature vectors of HED [24], SRN [5], and LSN(From left to right results are listed in a deep-to-shallow manner). By comparing (a) and (c), (b) and (d), one can see that LSN can learn better feature vectors and subspaces(basis) to span the output space. It enforces the representative capacity of convolutional features to fit complex outputs with limited convolutional layers.",
1807.09601v1, What is the purpose of the 1x1 convolutional layer in the Linear Span Unit (LSU)?, The 1x1 convolutional layer in the LSU is used to perform linear reconstruction of the input features. ,1807.09601v1-Figure3-1.png," Linear Span Unit, which is used in both feature linear span and subspace linear span. In LSU, the operation of linear reconstruction is implemented by a concatenation layer and a 1× 1 convolutional layer.",
1807.09601v1,Which LSN architecture performed the best on the SK-LARGE dataset?,LSN-3,1807.09601v1-Table1-1.png, The performance of different LSN implementations on the SK-LARGE dataset. LSN 3 that fuses an adjacent layer of higher resolution and an adjacent layer of lower resolution reported the best performance.,
1807.09601v1,Which training strategy had the highest F-measure?,iter3,1807.09601v1-Table2-1.png, The performance for different training strategies.,
1807.09601v1,What is the difference between the skeleton detection results of SRN and LSN?,"The red boxes in the SRN results indicate false positives or dismissals, while the green boxes in the LSN results indicate correct reconstruction skeletons at the same position.",1807.09601v1-Figure7-1.png," Skeleton detection examples by state-of-the-art approaches including HED [24], FSDS [18], SRN [5], and LSN. The red boxes are false positive or dismiss in SRN, while the blue ones are correct reconstruction skeletons in LSN at the same position. (Best viewed in color with zoon-in.)",
1807.09601v1,What is the purpose of the resolution alignment component in the Linear Span Network (LSN)?,The resolution alignment component unifies the resolution among multi-stages in the network.,1807.09601v1-Figure4-1.png," The architecture of the proposed Linear Span Network (LSN), which leverages Linear Span Units (LSUs) to implement three components of the feature linear span, the resolution alignment, and the subspace linear span. The feature linear span uses convolutional features to build subspaces. The LSU is re-used to unify the resolution among multi-stages in resolution alignment. The subspace linear span summarizes the subspaces to fit the ground-truth space.",
1807.09601v1,How can you tell from the figure whether the set of vectors is dependent or independent?,"The set of vectors is dependent if and only if one of the vectors can be written as a linear combination of the others. In the figure, the set of vectors is dependent in (a) because vector v3 can be written as a linear combination of v1 and v2. In (b), the set of vectors is independent because no vector can be written as a linear combination of the others.",1807.09601v1-Figure2-1.png, Schematic of linear span with a set of dependent vectors (a) and independent vectors (b).,
1807.09601v1,Which algorithm achieved the highest accuracy on the WH-SYMMAX dataset?,LSN (ours),1807.09601v1-Table4-1.png," Performance comparison of the state-of-the-art approaches on the public WH-SYMMAX [15], SK-SMALL [18], SYMMAX [21], and Sym-PASCAL [5] datasets.",
1808.09588v1,What information does the system need to understand in order to automatically generate the add method?,"The system needs to understand that vecElements is the vector to be augmented, and that the method must take in a scalar parameter as the element to be added. The model also needs to disambiguate between the member variables vecElements and weights.",1808.09588v1-Figure1-1.png," Code generation based on the class environment and method documentation. The figure shows a class where the programmer wants to automatically generate the add method from documentation, assuming the rest of the class is already written. The system needs to understand that vecElements is the vector to be augmented, and that the method must take in a scalar parameter as the element to be added. The model also needs to disambiguate between the member variables vecElements and weights.",
1808.09588v1,Which model feature contributes the most to the Exact score?,"The ""Variables"" feature contributes the most to the Exact score.",1808.09588v1-Table3-1.png, Ablation of model features on the development set.,
1808.09588v1,Which model performs the best according to the Exact match accuracy metric?,"The model labeled ""Ours"" performs the best according to the Exact match accuracy metric.",1808.09588v1-Table2-1.png," Exact match accuracy and BLEU score (for partial credit) on the test (development) set, comprising 2000 examples from previously unseen repositories.",
1808.09588v1,"What is the purpose of the ""add"" method?","The ""add"" method adds a scalar to each element of the vector in place.",1808.09588v1-Figure2-1.png," Our task involves generating the derivation of the source code of a method based on the NL documentation, class member variables (names and data types), and other class member methods (method names and return types), which form the code environment.",
1808.09588v1,In which case did the model produce a better solution than the reference?,"In case (f), the model produced a better solution than the reference.",1808.09588v1-Figure5-1.png," Analysis of our model output on development set examples. Some environment variables and methods are omitted for space. (a)-(d) represent cases where the model exactly produced the reference output. (e)-(g) are cases where the model output is very reasonable for a practical setting. In (f), the model produces a better solution than the reference. In (h), the context lacks information to produce the reference code. The model chooses the wrong element in (i) and could be improved by better encoder representations.",
1808.09588v1,What is the purpose of the Bi-LSTM stack in the encoder?,The Bi-LSTM stack is used to initialize the decoder.,1808.09588v1-Figure3-1.png," The encoder creates contextual representations of the NL (a), the variables and the methods (b). Variable (method) names are split based on camel-casing and encoded using a BiLSTM. The variable (method) type and name are further contextualized using another BiLSTM.",
1808.09588v1,What percentage of responses were either Mostly Correct or Exact Match?,27%,1808.09588v1-Table4-1.png, Qualitative distribution of errors on the development set.,
1808.09588v1,What is the role of the LSTM in the decoder?,"The LSTM is used to generate the hidden state st, which is a function of the previous hidden state, current non-terminal, previous production rule, parent rule, and the parent hidden state.",1808.09588v1-Figure4-1.png," The hidden state st of our decoder is a function of the previous hidden state, current non-terminal, previous production rule, parent rule, and the parent hidden state. st is used to attend on the NL and compress it into zt, which is then used to attend over the environment variables and methods to generate et. The decoder uses all these context vectors to produce a distribution over valid right hand side values of the current non-terminal, and also learns to copy from the environment.",
1808.09588v1,What is the average number of environment variables in the dataset?,4.89,1808.09588v1-Table1-1.png," Statistics of our dataset of (NL, context and code) tuples collected from Github. *Does not include rules that generate identifiers.",
1808.10143v2,Which model achieved the highest test perplexity on the PTB dataset?,AWD-LSTM-DOC (ensemble) x 5,1808.10143v2-Table7-1.png, Perplexities of each method on the PTB dataset.,
1808.10143v2,Which model has the best test performance on the WikiText-2 dataset?,The proposed method (ensemble): AWD-LSTM-DOC (fin) x 5 has the best test performance with a perplexity of 53.09.,1808.10143v2-Table8-1.png, Perplexities of each method on the WikiText-2 dataset.,
1808.10143v2,What is the effect of adding DOC to the EncDec model?,Adding DOC to the EncDec model generally improves the BLEU scores across all language pairs.,1808.10143v2-Table9-1.png, BLEU scores on test sets in the IWSLT 2016 dataset. We report averages of three runs.,
1808.10143v2,Which model achieved the highest ROUGE F1 score on the RG-1 metric?,EncDec+DOC (i_3 = i_2 = 2),1808.10143v2-Table10-1.png," ROUGE F1 scores in headline generation test data provided by Zhou et al. (2017). RG in table denotes ROUGE. For our implementations (the upper part), we report averages of three runs.",
1808.10143v2,How does the proposed method use one-hot vectors and embedding layers to represent input words?,"The proposed method first converts each input word into a one-hot vector. This vector is then fed into an embedding layer, which transforms it into a lower-dimensional vector that captures the semantic meaning of the word. This process allows the model to learn from the relationships between words and their meanings.",1808.10143v2-Figure1-1.png, Overview of the proposed method: DOC. This figure represents the example of N = 2 and i0 = i1 = i2 = 3.,
1808.10143v2,Which model achieved the highest F1 score when reranked with a single model?,AWD-LSTM-DOC,1808.10143v2-Table11-1.png, Bracketing F1 scores on the PTB test set (Section 23). This table includes reranking models trained on the PTB without external data.,
1808.10143v2,What is the effect of increasing the number of probability distributions from each layer in situation J = 20 on the perplexity of AWD-LSTM with DOC on the PTB dataset?,Increasing the number of probability distributions from each layer in situation J = 20 generally decreases the perplexity of AWD-LSTM with DOC on the PTB dataset.,1808.10143v2-Table3-1.png, Perplexities of AWD-LSTM with DOC on the PTB dataset. We varied the number of probability distributions from each layer in situation J = 20 except for the top row. The top row (†) represents MoS scores reported in Yang et al. (2018) as a baseline. ‡ represents the perplexity obtained by the implementation of Yang et al. (2018)6 with identical hyperparameters except for i3.,
1808.10143v2,"Which dataset has the larger vocabulary size, PTB or WikiText-2?",WikiText-2 has a larger vocabulary size than PTB.,1808.10143v2-Table1-1.png, Statistics of PTB and WikiText-2.,
1808.10143v2,"What is the dropout rate for h3,ht used in the PTB dataset?",0.225,1808.10143v2-Table2-1.png, Hyperparameters used for training DOC.,
1808.10143v2,How does the coefficient of variation of Equation 10 change as the value of  λ_β increases?,The coefficient of variation of Equation 10 decreases as the value of  λ_β increases.,1808.10143v2-Table4-1.png, Coefficient of variation of Equation 10: √ β in validation and test sets of PTB.,
1808.10143v2,What is the rank of the output matrix for the AWD-LSTM model on the PTB dataset?,401,1808.10143v2-Table5-1.png, Rank of output matrix (Ã in Equation 9) on the PTB dataset. D3 of AWD-LSTM is 400.,
1808.10143v2,Which model performed the best on the PTB dataset?,The AWD-LSTM+ model performed the best on the PTB dataset.,1808.10143v2-Table6-1.png," Perplexities of our implementations and reruns on the PTB dataset. We set the non-monotone interval to 60. † represents results obtained by original implementations with identical hyperparameters except for non-monotone interval. ‡ indicates the result obtained by our AWD-LSTM-MoS implementation with identical dropout rates as AWD-LSTM-DOC. For (fin), we repeated fine-tuning until convergence.",
1808.10143v2,Which model performed the best on the PTB validation set?,The proposed AWD-LSTM-DOC model performed the best.,1808.10143v2-Figure2-1.png, Perplexities of each method on the PTB validation set.,
1809.09478v3,What is the difference between classical adversarial loss and self-adaptive adversarial loss?,"Classical adversarial loss ignores semantic consistency when pursuing marginal distribution alignment, while self-adaptive adversarial loss reweights the adversarial loss for each feature by a local alignment score. This means that self-adaptive adversarial loss is more likely to preserve the semantic relationships between features, even when they are mapped to different joint distributions.",1809.09478v3-Figure1-1.png," (Best viewed in color.) Illustration of traditional and the proposed adversarial learning. The size of the solid gray arrow represents the weight of the adversarial loss. (a) Traditional adversarial learning ignores the semantic consistency when pursuing the marginal distribution alignment. As a result, the global movement might cause the well-aligned features (class A) to be mapped onto different joint distributions (negative transfer). (b) The proposed self-adaptive adversarial learning reweights the adversarial loss for each feature by a local alignment score. Our method reduces the influence of the adversaries when discovers a high semantic alignment score on a feature, and vice versa. As is shown, the proposed strategy encourages a category-level joint distribution alignment for both class A and class B.",
1809.09478v3,Which method achieves the highest mean IoU when adapting from GTA5 to Cityscapes?,CLAN with ResNet101 backbone and adversarial learning achieves the highest mean IoU of 87.0.,1809.09478v3-Table1-1.png," Adaptation from GTA5 [30] to Cityscapes [8]. We present per-class IoU and mean IoU. “V” and “R” represent the VGG16-FCN8s and ResNet101 backbones, respectively. “ST” and “AT” represent two lines of method, i.e., self training- and adversarial learning-based DA. We highlight the best result in each column in bold. To clearly showcase the effect of CLAN on infrequent classes, we highlight these classes in blue. Gain indicates the mIoU improvement over using the source only.",
1809.09478v3,"Which method performs the best on the ""rider"" class when using the ResNet backbone?","CLAN performs the best on the ""rider"" class when using the ResNet backbone.",1809.09478v3-Table2-1.png," Adaptation from SYNTHIA [31] to Cityscapes [8]. We present per-class IoU and mean IoU for evaluation. CLAN and state-of-theart domain adaptation methods are compared. For each backbone, the best accuracy is highlighted in bold. To clearly showcase the effect of CLAN on infrequent classes, we highlight these classes in blue. Gain indicates the mIoU improvement over using the source only.",
1809.09478v3,"Which of the two models, TAN or CLAN, achieves a lower center distance after 10 epochs of training?",CLAN,1809.09478v3-Figure4-1.png, Left: Cluster center distance variation as training goes on. Right: Mean IoU (see bars & left y axis) and convergence performance (see lines & right y axis) variation when training with different λlocal and ε.,
1809.09478v3, What is the difference between the non-adapted and adapted results for the first image?," The non-adapted result is much more noisy and inaccurate than the adapted result. For example, the non-adapted result incorrectly classifies many of the pixels in the background as belonging to the foreground objects, while the adapted result is much more accurate.",1809.09478v3-Figure5-1.png," Qualitative results of UDA segmentation for GTA5 → Cityscapes. For each target image, we show the non-adapted (source only) result, adapted result with CLAN and the ground truth label map, respectively.",
1809.09478v3,"Which method has the most similar feature cluster centers between source and target domain for the ""person"" class?",The pre-trained model.,1809.09478v3-Figure6-1.png," Quantitative analysis of the feature joint distributions. For each class, we show the distance of the feature cluster centers between source domain and target domain. These results are from 1) the model pre-trained on ImageNet [9] without any fine-tuning, 2) the model fine-tuned with source images only, 3) the adapted model using TAN and 4) the adapted model using CLAN, respectively.",
1809.09478v3, How does the network calculate the category-level adversarial loss? ," The network calculates the category-level adversarial loss by first generating a raw adversarial loss map from the discriminator D, which is fed with the sum of the two prediction maps from the target flow. Then, the network calculates a local alignment score map using the discrepancy between the two prediction maps. This score map evaluates the category-level alignment degree of each feature and is used to adaptively weight the raw adversarial loss map. ",1809.09478v3-Figure2-1.png," Overview of the proposed category-level adversarial network. It consists of a feature extractor E, two classifiers C1 and C2, and a discriminator D. C1 and C2 are fed with the deep feature map extracted from E and predict semantic labels for each pixel from diverse views. In source flow, the sum of the two prediction maps is used to calculate a segmentation loss as well as an adversarial loss from D. In target flow, the sum of the two prediction maps is forwarded to D to produce a raw adversarial loss map. Additionally, we adopt the discrepancy of the two prediction maps to produce a local alignment score map. This map evaluates the category-level alignment degree of each feature and is used to adaptively weight the raw adversarial loss map.",
1809.09478v3,Which of the following techniques preserves the original well-segmented class?,CLAN,1809.09478v3-Figure3-1.png," A contrastive analysis of CLAN and traditional adversarial network (TAN). (a): A target image, and we focus on the poles and traffic signs in orange boxes. (b): A non-adapted segmentation result. Although the global segmentation result is poor, the poles and traffic signs can be correctly segmented. It indicates that some classes are originally aligned between domains, even without any domain adaptation. (c): Adapted result of TAN, in which a decent segmentation map is produced but poles and traffic signs are poorly segmented. The reason is that the global alignment strategy tends to assign a conservative prediction to a feature and would lead some features to be predicted to other prevalent classes [11, 18], thus causing those infrequent features being negatively transferred. (d): Adapted result from CLAN. CLAN reduces the weight of adversarial loss for those aligned features. As a result, the original well-segmented class are well preserved. We then map the high-dimensional features of (b), (c) and (d) to a 2-D space with t-SNE [28] shown in (e), (f) and (g). The comparison of feature distributions further proves that CLAN can enforce category-level alignment during the trend of global alignment. (For a clear illustration, we only show 4 related classes, i.e., building in blue, traffic sign in orange, pole in red and vegetation in green.)",
1810.05270v2,What is the order of steps in a typical three-stage network pruning pipeline?,"Training, pruning, and fine-tuning.",1810.05270v2-Figure1-1.png, A typical three-stage network pruning pipeline.,
1810.05270v2,What is the effect of pruning on the accuracy of different models on CIFAR-10 and CIFAR-100 datasets?,"Pruning generally decreases the accuracy of the models on both datasets, but the decrease is not significant. For example, on CIFAR-10, the accuracy of VGG-19 decreases from 93.53% to 93.60% when the prune ratio is 70%.",1810.05270v2-Table4-1.png," Results (accuracy) for Network Slimming (Liu et al., 2017). “Prune ratio” stands for total percentage of channels that are pruned in the whole network. The same ratios for each model are used as the original paper.",
1810.05270v2,Which strategy resulted in the smallest decrease in accuracy for ResNet-50?,Scratch-B,1810.05270v2-Table3-1.png," Results (accuracy) for Regression based Feature Reconstruction (He et al., 2017b). Pruned models such as “VGG-16-5x” are defined in He et al. (2017b). Similar to Table 2, we compare relative accuracy drop from unpruned large models.",
1810.05270v2,What is the relative accuracy drop for the pruned model VGG-Conv compared to the unpruned VGG-16 model when using the Scratch-E strategy?,The relative accuracy drop for the pruned model VGG-Conv compared to the unpruned VGG-16 model when using the Scratch-E strategy is 2.75%.,1810.05270v2-Table2-1.png," Results (accuracy) for ThiNet (Luo et al., 2017). Names such as “VGG-GAP” and “ResNet50-30%” are pruned models whose configurations are defined in Luo et al. (2017). To accommodate the effects of different frameworks between our implementation and the original paper’s, we compare relative accuracy drop from the unpruned large model. For example, for the pruned model VGG-Conv, −1.23 is relative to 71.03 on the left, which is the reported accuracy of the unpruned large model VGG-16 in the original paper; −2.75 is relative to 71.51 on the left, which is VGG-16’s accuracy in our implementation.",
1810.05270v2," Based on the table, which model performs the best when the initial learning rate is 0.1?", The unpruned VGG-16 model performs the best with an accuracy of 93.63 (±0.16). ,1810.05270v2-Table8-1.png," Comparisons with the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) on a structured pruning method (L1-norm based filter pruning (Li et al., 2017)) with two initial learning rates: 0.1 and 0.01. In both cases, using winning tickets does not bring improvement on accuracy.",
1810.05270v2,What is the effect of using a small learning rate (0.01) with the winning ticket initialization?,Using a small learning rate (0.01) with the winning ticket initialization leads to a lower accuracy than the widely used large learning rate (0.1).,1810.05270v2-Figure7-1.png," Comparisons with the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) for iterative/one-shot unstructured pruning (Han et al., 2015) with two initial learning rates 0.1 and 0.01, on CIFAR-10 dataset. Each point is averaged over 5 runs. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).",
1810.05270v2,Which stage of VGG-16 has the most redundancy at a 40% prune ratio?,Stage 4,1810.05270v2-Table21-1.png," Sparsity patterns of VGG-16 pruned on CIFAR-10 by Network Slimming shown in Figure 3 (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. For each prune ratio, the latter stages tend to have more redundancy than earlier stages.",
1810.05270v2,Which stage of the PreResNet-110 model has the most uniform sparsity pattern across different prune ratios?,Stage 1.,1810.05270v2-Table19-1.png," Average sparsity patterns of 3×3 kernels of PreResNet-110 pruned on CIFAR-100 by unstructured pruning shown in Figure 5 (middle) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).",
1810.05270v2,How does the sparsity pattern of the DenseNet-40 kernels change as the prune ratio increases?,The sparsity pattern becomes more uniform across stages.,1810.05270v2-Table20-1.png," Average sparsity patterns of 3×3 kernels of DenseNet-40 pruned on CIFAR-100 by unstructured pruning shown in Figure 5 (right) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).",
1810.05270v2,What is the difference between predefined and automatically discovered target architectures in channel pruning?,"In predefined channel pruning, the user specifies the percentage of channels to be pruned in each layer. In automatically discovered target architectures, the pruning algorithm determines the percentage of channels to be pruned in each layer.",1810.05270v2-Figure2-1.png," Difference between predefined and automatically discovered target architectures, in channel pruning as an example. The pruning ratio x is userspecified, while a, b, c, d are determined by the pruning algorithm. Unstructured sparse pruning can also be viewed as automatic.",
1810.05270v2,How does pruning affect the accuracy of ResNet models on the ImageNet dataset?,"Pruning generally decreases the accuracy of ResNet models, but the decrease is relatively small. For example, the accuracy of ResNet-50 decreases from 76.12% to 74.18% after pruning.",1810.05270v2-Table5-1.png," Results (accuracy) for residual block pruning using Sparse Structure Selection (Huang & Wang, 2018). In the original paper no fine-tuning is required so there is a “Pruned” column instead of “Fine-tuned” as before.",
1810.05270v2,Which model performs best on CIFAR-100 with a prune ratio of 50%?,DenseNet-BC-100,1810.05270v2-Table6-1.png," Results (accuracy) for unstructured pruning (Han et al., 2015). “Prune Ratio” denotes the percentage of parameters pruned in the set of all convolutional weights.",
1810.05270v2,What is the effect of increasing the prune ratio on the accuracy of the PreResNet-56 model when fine-tuned on the CIFAR-10 dataset?,Increasing the prune ratio decreases the accuracy of the PreResNet-56 model when fine-tuned on the CIFAR-10 dataset.,1810.05270v2-Table13-1.png," Results (accuracy) for Network Slimming (Liu et al., 2017) when the models are aggressively pruned. “Prune ratio” stands for total percentage of channels that are pruned in the whole network. Larger ratios are used than the original paper of Liu et al. (2017).",
1810.05270v2,What is the effect of pruning on the accuracy of the ResNet-56 model on the CIFAR-10 dataset?,Pruning reduces the accuracy of the ResNet-56 model by about 4%.,1810.05270v2-Table14-1.png," Results (accuracy) for L1-norm based filter pruning (Li et al., 2017) when the models are aggressively pruned.",
1810.05270v2,Which pruning method resulted in the highest mAP for ResNet34-A on the PASCAL VOC 07 dataset?,Prune-D,1810.05270v2-Table12-1.png," Results (mAP) for pruning on detection task. The pruned models are chosen from Li et al. (2017). Prune-C refers to pruning on classifcation pre-trained weights, Prune-D refers to pruning after the weights are transferred to detection task. Scratch-E/B means pre-training the pruned model from scratch on classification and transfer to detection.",
1810.05270v2,Which stage has the most sparsity?,Stage 5.,1810.05270v2-Figure4-1.png, The average sparsity pattern of all 3×3 convolutional kernels in certain layer stages in a unstructured pruned VGG-16. Darker color means higher probability of weight being kept.,
1810.05270v2,Which layer of the network has the largest number of channels?,Layer 8.,1810.05270v2-Table7-1.png," Network architectures obtained by pruning 60% channels on VGG-16 (in total 13 conv-layers) using Network Slimming. Width and Width* are number of channels in the original and pruned architectures, averaged over 5 runs.",
1810.05270v2,"Which pruning method is more efficient for VGG-16 on CIFAR-10, Network Slimming or Uniform Pruning?",Network Slimming is more efficient than Uniform Pruning.,1810.05270v2-Figure3-1.png," Pruned architectures obtained by different approaches, all trained from scratch, averaged over 5 runs. Architectures obtained by automatic pruning methods (Left: Network Slimming (Liu et al., 2017), Right: Unstructured pruning (Han et al., 2015)) have better parameter efficiency than uniformly pruning channels or sparsifying weights in the whole network.",
1810.05270v2,Which model performed the best on the CIFAR-10 dataset?,The VGG-16 model fine-tuned for 40 epochs performed the best on the CIFAR-10 dataset with an accuracy of 93.40%.,1810.05270v2-Table16-1.png," “Fine-tune-40” stands for fine-tuning 40 epochs and so on. Scratch-E models are trained for 160 epochs. We observe that fine-tuning for more epochs does not help improve the accuracy much, and models trained from scratch can still perform on par with fine-tuned models.",
1810.05270v2,What is the accuracy of the VGG-19 model on CIFAR-100 when it is pruned by 95%?,70.22%,1810.05270v2-Table15-1.png," Results (accuracy) for unstructured pruning (Han et al., 2015) when the models are aggressively pruned.",
1810.05270v2,Which model architecture performed the best on the CIFAR-10 dataset when trained from scratch with an extended training schedule of 300 epochs?,ResNet-110-B,1810.05270v2-Table17-1.png," Results for L1-norm filter pruning (Li et al., 2017) when the training schedule of the large model is extended from 160 to 300 epochs.",
1810.05270v2,Which model has the highest accuracy on the CIFAR-10 dataset?,The Scratch-E model with a 40% prune ratio has the highest accuracy on the CIFAR-10 dataset.,1810.05270v2-Table11-1.png," Results (accuracy) for Soft Filter Pruning (He et al., 2018a) using pretrained models.",
1810.05270v2,Which ResNet model has the highest accuracy on the CIFAR-10 dataset after pruning 30% of the filters?,ResNet-32,1810.05270v2-Table10-1.png," Results (accuracy) for Soft Filter Pruning (He et al., 2018a) without pretrained models.",
1810.05270v2,"Which method performs better, ""Winning Ticket"" or ""Random Init""?","The answer depends on the initial learning rate. When the initial learning rate is 0.1, ""Random Init"" performs better. When the initial learning rate is 0.01, ""Winning Ticket"" performs better.",1810.05270v2-Table9-1.png," Comparisons with the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) for one-shot unstructured pruning (Han et al., 2015) with two initial learning rates: 0.1 and 0.01. The same results are visualized in Figure 7b. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).",
1810.05270v2,Which pruning approach achieves the highest test accuracy on CIFAR-100?,Unstructured Pruning,1810.05270v2-Figure5-1.png," Pruned architectures obtained by different approaches, all trained from scratch, averaged over 5 runs. Left: Results for PreResNet-164 pruned on CIFAR-10 by Network Slimming (Liu et al., 2017). Middle and Right: Results for PreResNet-110 and DenseNet-40 pruned on CIFAR-100 by unstructured pruning (Han et al., 2015).",
1810.05270v2,Which pruning approach is the most effective in terms of test accuracy for VGG-19 on CIFAR-100?,Transferred Guided Pruning,1810.05270v2-Figure6-1.png," Pruned architectures obtained by different approaches, all trained from scratch, averaged over 5 runs. “Guided Pruning/Sparsification” means using the average sparsity patterns in each layer stage to design the network; “Transferred Guided Pruning/Sparsification” means using the sparsity patterns obtained by a pruned VGG-16 on CIFAR-10, to design the network for VGG-19 on CIFAR-100. Following the design guidelines provided by the pruned architectures, we achieve better parameter efficiency, even when the guidelines are transferred from another dataset and model.",
1810.05270v2,How does the sparsity pattern of PreResNet-164 change as the prune ratio increases?,The sparsity pattern becomes more uniform across stages.,1810.05270v2-Table18-1.png," Sparsity patterns of PreResNet-164 pruned on CIFAR-10 by Network Slimming shown in Figure 5 (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).",
1810.05270v2,How does the distribution of weights change for different pruning methods and training methods?,The distribution of weights becomes more concentrated around zero for both structured and unstructured pruning methods. This effect is more pronounced for fine-tuned models than for scratch-trained models.,1810.05270v2-Figure8-1.png," Weight distribution of convolutional layers for different pruning methods. We use VGG-16 and CIFAR-10 for this visualization. We compare the weight distribution of unpruned models, fine-tuned models and scratch-trained models. Top: Results for Network Slimming (Liu et al., 2017). Bottom: Results for unstructured pruning (Han et al., 2015).",
1810.05270v2,Which model has the highest accuracy on CIFAR-10?,ResNet-110,1810.05270v2-Table1-1.png," Results (accuracy) for L1-norm based filter pruning (Li et al., 2017). “Pruned Model” is the model pruned from the large model. Configurations of Model and Pruned Model are both from the original paper.",
1811.00458v4,Where are eBird observations most concentrated in the continental U.S.?,eBird observations are most concentrated in or near urban areas and along major roads.,1811.00458v4-Figure1-1.png, Highly biased distribution of eBird observations in the continental U.S. Submissions are concentrated in or near urban areas and along major roads.,
1811.00458v4,Which method performs best according to the F1 score metric?,The SCN method performs best according to the F1 score metric.,1811.00458v4-Table2-1.png," Comparison of predictive performance of different methods under three different metrics. (The larger, the better.)",
1811.00458v4,Which model performs the best and which performs the worst?,"The model that performs the best is KLIEP, and the model that performs the worst is SCN.",1811.00458v4-Figure3-1.png," The learning curves of all models. The vertical axis shows the cross-entropy loss, and the horizontal axis shows the number of iterations.",
1811.00458v4,What is the role of the feature extractor in the Shift Compensation Network?,The feature extractor is responsible for extracting features from the input data. These features are then used by the discriminator and classifier to perform their respective tasks.,1811.00458v4-Figure2-1.png, Overview of the Shift Compensation Network,
1811.00458v4,How does the distribution of observations change after applying the shift factor?,The distribution of observations becomes more concentrated in the central and eastern parts of the state after applying the shift factor.,1811.00458v4-Figure4-1.png," Heatmap of the observation records for the month of May in New York State, where the left panel shows the distribution of the original samples and the right one shows the distribution weighted with the shift factor",
1811.00458v4,Which model has the lowest feature space discrepancy between the weighted training data and the test data?,KLIEP,1811.00458v4-Table4-1.png, Feature space discrepancy between the weighted training data and the test data,
1811.00458v4,Which SCN variant performs the best in terms of AUC?,SCN performs the best in terms of AUC.,1811.00458v4-Table3-1.png, Comparison of predictive performance of the different variants of SCN,
1811.00458v4,What is the difference in dimensionality between the NLCD and Google Earth Image features?,"The NLCD features have a dimensionality of 16, while the Google Earth Image features have a dimensionality of 256 x 256 x 3.",1811.00458v4-Table1-1.png, Statistics of the eBird dataset,
1811.07039v1,"Why does the system label the claim as ""Refutes""?","The system labels the claim as ""Refutes"" because the evidence shows that Giada at Home first aired on the Food Network, which is a cable and satellite television channel. This means that the show was available on television, not just on DVD.",1811.07039v1-Figure1-1.png," Example of FEVER task. Given the claim, the system is required to find evidential sentences in the entire Wikipedia corpus and label it as “SUPPORTS”, “REFUTES”, or “NOT ENOUGH INFO” (Thorne et al. 2018).",
1811.07039v1,"Which document retrieval method performs the best on the entire development set, and what is its F1 score?","KM + Pageview + dNSMN performs the best on the entire development set, with an F1 score of 66.12 when k=5 and 64.93 when k=10.",1811.07039v1-Table2-1.png, Performance of different document retrieval methods. k indicates the number of retrieved documents. The last four columns show results on the difficult subset that includes more than 10% of dev set. dNSMN = document retrieval Neural Semantic Matching Network. ‘KM’=Keyword Matching.,
1811.07039v1,What is the difference in performance between the Final Model trained with sentences selected from sNSMN and the Final Model trained with sentences selected from TF-IDF?,"The Final Model trained with sentences selected from sNSMN achieved a FEVER score of 66.14, a label accuracy of 69.60, and an F1 score of 75.7/69.4/63.3. The Final Model trained with sentences selected from TF-IDF achieved a FEVER score of 62.48, a label accuracy of 67.23, and an F1 score of 72.6/70.4/56.3. This shows that the model trained with sentences from sNSMN performs better across all metrics.",1811.07039v1-Table4-1.png," Ablation study for verification (vNSMN). ‘WN’=WordNet feature, ‘Num’=number embedding, ‘Final Model’=vNSMN with semantic relatedness score feature only from sentence selection. ‘SRS (sent)’, ‘SRS (doc)’ = Semantic Relatedness Score from document retrieval and sentence selection modules. FEVER column shows strict FEVER score and LA column shows label accuracy without considering evidence. The last column shows F1 score of three labels. All models above line are trained with sentences selected from sNSMN for non-verifiable examples, while model below is from TF-IDF.",
1811.07039v1,What is the relationship between the threshold and the accuracy of the vNSMN verifier?,The accuracy of the vNSMN verifier decreases as the threshold decreases.,1811.07039v1-Table5-1.png," Dev set results (before evidence enhancement) for a vNSMN verifier making inference on data with different degrees of noise, by filtering with different score thresholds.",
1811.07039v1,Which combination of features achieved the highest FEVER score on the dev set?,Pageview + dNSMMN + sNSMMN + vNSMMN.,1811.07039v1-Table6-1.png, Performance of different combinations on dev set.,
1811.07039v1,Which method achieved the highest OFEVER score on the entire development set?,sNSMN w. AS,1811.07039v1-Table3-1.png," Different methods for sentence selection on dev set. ‘Enc.’= Sentence Encoder. ‘AS’= Annealed Sampling. The OFEVER column shows Oracle FEVER Score. The other three columns show the evidence accuracy, recall, and F1.",
1811.07039v1,Is Munich the capital of Germany?,"No, Berlin is the capital of Germany.",1811.07039v1-Figure4-1.png, An example of negative output. The model is able to correctly select the evidence but fail to reveal the correct logical relation between the evidence and the claim.,
1811.07039v1,Which model performed the best on the blind test results according to the F1 score?,UNC-NLP (our shared task model) performed the best with an F1 score of 52.96.,1811.07039v1-Table7-1.png, Performance of systems on blind test results.,
1811.07039v1,What is the purpose of the NSMN for Documents and NSMN for Sentences blocks in the system overview?,"The NSMN for Documents block is used to filter and rank documents based on their relatedness to the claim, while the NSMN for Sentences block is used to filter and rank sentences based on their relatedness to the claim.",1811.07039v1-Figure2-1.png," System Overview: Document Retrieval, Sentence Selection, and Claim Verification.",
1811.07039v1,Which class has the most instances in the training split?,SUPPORTS,1811.07039v1-Table8-1.png," Dataset split sizes for SUPPORTS, REFUTES and NOTENOUGHINFO (NEI) classes.",
1811.07039v1,What evidence does the model provide to support the claim that Nicholas Brody is a character on Homeland?,"The model provides two pieces of evidence to support the claim: 
1. An excerpt from the Wikipedia article on Homeland, which states that ""Homeland is the first novel in The Dark Elf Trilogy... written by R. A. Salvatore and follows the story of Drizzt Do'Urden..."". This information is not relevant to the claim, but it demonstrates that the model is searching for evidence related to ""Homeland."" 
2. An excerpt from the Wikipedia article on Nicholas Brody, which states that ""GySgt. Nicholas ""Nick"" Brody... is a fictional character on the American television series Homeland on Showtime..."". This directly supports the claim that Brody is a character on Homeland.",1811.07039v1-Figure3-1.png, An example of positive output. The model is able to correctly identify the evidence and verify the claim.,
1811.07039v1,What are the 10 indicator features in WordNet embedding?,"The 10 indicator features in WordNet embedding are:

1. Exact same lemma
2. Antonymy
3. Hyponymy
4. Hypernymy
5. Hyponymy with 1-edge distance in WN topological graph
6. Hypernymy with 1-edge distance in WN topological graph
7. Hyponymy with 2-edge distance in WN topological graph
8. Hypernymy with 2-edge distance in WN topological graph
9. Hyponymy with distance > 2 edges in WN topological graph
10. Hypernymy with distance > 2 edges in WN topological graph",1811.07039v1-Table1-1.png, 10 indicator features in WordNet embedding.,
1811.09558v1,What is the relationship between the estimated prior and the new observations in the online phase?,The estimated prior is a function that is used to predict the value of the new observations. The new observations are used to update the estimated prior.,1811.09558v1-Figure1-1.png," Our approach estimates the mean function µ̂ and kernel k̂ from functions sampled from GP (µ, k) in the offline phase. Those sampled functions are illustrated by colored lines. In the online phase, a new function f sampled from the same GP (µ, k) is given and we can estimate its posterior mean function µ̂t and covariance function k̂t which will be used for Bayesian optimization.",
1811.09558v1,What is the difference between the two instances of the picking problem shown in the image?,"The difference between the two instances is the arrangement of the obstacles on the table. In the left instance, the blue box is surrounded by green and red blocks, while in the right instance, the blue box is surrounded by black blocks.",1811.09558v1-Figure2-1.png," Two instances of a picking problem. A problem instance is defined by the arrangement and number of obstacles, which vary randomly across different instances. The objective is to select a grasp that can pick the blue box, marked with a circle, without violating kinematic and collision constraints. [27].",
1811.09558v1,Which method performs the best on the synthetic functions sampled from a GP?,PEM-BO-UCB.,1811.09558v1-Figure3-1.png, Learning curves (top) and rewards vs number of iterations (bottom) for optimizing synthetic functions sampled from a GP and two scoring functions from.,
1811.09558v1,Which optimization method performs the best for the grasp optimization problem?,TLSM-BO-PI,1811.09558v1-Figure4-1.png," Rewards vs. Number of evals for grasp optimization, grasp, base pose, and placement optimization, and synthetic function optimization problems (from top-left to bottom). 0.6xPEMBO refers to the case where we have 60 percent of the dataset missing.",
1812.01939v1,Which algorithm performed the best on the synthetic dataset with 10000 samples as training data?,TSTE-p,1812.01939v1-Table1-1.png," Performance Comparison on synthetic dataset with 200, 1000 and 10000 samples as training data, respectively",
1812.01939v1,Which algorithm performs the best when the number of samples is 5000?,GNnMDS-p,1812.01939v1-Table2-1.png," Performance Comparison on music artists dataset with 200, 500, 1000 and 5000 samples as training data.",
1812.01939v1,Which method has the lowest generalization error when the number of epochs is 5000?,TSTE,1812.01939v1-Figure3-1.png," Generalization errors of DMOE, GNMDS, STE and TSTE on the music artists dataset.",
1812.01939v1,Which of the methods has the highest generalization error when the number of training samples is 1000?,TSTE,1812.01939v1-Figure2-1.png," Generalization errors of DMOE, GNMDS-p, STEp and TSTE-p on the synthetic dataset.",
1812.01939v1,What is the relationship between the margin distribution and the loss function?,The margin distribution determines the shape of the loss function.,1812.01939v1-Figure1-1.png, (a) The Margin Distribution obtained by solving (15). (b) The loss function (16) with different ν.,
1812.11321v1,Which model performed the best in terms of AUC?,Our Model performed the best in terms of AUC.,1812.11321v1-Table1-1.png, Precisions on the NYT dataset.,
1812.11321v1,Which model performs the best according to the table?,The Rank+ExATT model performs the best according to the table.,1812.11321v1-Table2-1.png, Precisions on the Wikidata dataset.,
1812.11321v1,Which method performs the best according to the precision-recall curve?,Mintzmann.,1812.11321v1-Figure2-1.png, Performance comparison on the NYT dataset.,
1812.11321v1,Which relation has the highest probability of being predicted by all three models?,/location/capital,1812.11321v1-Figure3-1.png, Normalized output relation scores.,
1812.11321v1,What is the relationship between the choice of d and the AUC?,The AUC generally increases as the choice of d increases.,1812.11321v1-Table4-1.png, Precisions on the Wikidata dataset with different choice of d.,
1812.11321v1,What is the effect of increasing the number of dynamic routing iterations on the precision of the model?,Increasing the number of dynamic routing iterations generally increases the precision of the model.,1812.11321v1-Table5-1.png, Precisions on the Wikidata dataset with different number of dynamic routing iterations.,
1812.11321v1,Which model performs the best according to the table?,Our Model,1812.11321v1-Table3-1.png, Ablation study of capsule net and word-level attention on Wikidata dataset.,
1812.11321v1,What are the three layers of the capsule network for relation extraction?,"The three layers of the capsule network for relation extraction are the vector representation layer, the primary capsule layer, and the dynamic routing layer.",1812.11321v1-Figure1-1.png, Architecture of capsule networks for relation extraction,
1901.10422v3,What is the effect of augmenting the input space on the FID improvement of PA for SN DCGAN on the F-MNIST dataset?,The FID improvement of PA for SN DCGAN on the F-MNIST dataset is 6.2 when augmenting the input space.,1901.10422v3-Table1-1.png," FID improvement of PA across different datasets and network architectures. We experiment with augmenting the input and feature spaces, see Sec.4.1 for details.",
1901.10422v3,What is the effect of progressive augmentation on the discriminator's task?,Progressive augmentation makes the discriminator's task more difficult by making the decision boundary between real and fake samples more complex.,1901.10422v3-Figure1-1.png," Visualization of progressive augmentation. At level l = 0 (no augmentation) the discriminator D aims at classifying the samples xd and xg, respectively drawn from the data Pd and generative model Pg distributions, into true (green) and fake (blue). At single-level augmentation (l = 1) the class of the augmented sample is set based on the combination xd and xg with s, resulting in real and synthetic samples contained in both classes and leading to a harder task for D. With each extra augmentation level (l→ l + 1) the decision boundary between two classes becomes more complex and the discrimination task difficulty gradually increases. This prevents the discriminator from easily solving the task and thus leads to meaningful gradients for the generator updates.",
1901.10422v3,How does the use of PA affect the diversity of generated images?,The use of PA helps to improve the variation across interpolated samples.,1901.10422v3-Figure3-1.png," Synthetic images generated through latent space interpolation with and without using PA. PA helps to improve variation across interpolated samples, i.e., no close-by images looks alike.",
1901.10422v3,What is the role of the discriminator in PA-GAN?,The discriminator is responsible for distinguishing between real and fake data.,1901.10422v3-Figure2-1.png," PA-GAN overview. With each level of progressive augmentation l the dimensionality of s is enlarged from 1 to L, s = {s1, s2, . . . , sL}. The task difficulty of the discriminator gradually increases as the length of s grows.",
1901.10422v3,"Which regularization technique had the best performance on SN DCGAN for the ""feat"" layer?",Dropout,1901.10422v3-Table3-1.png," FID performance of PA, different regularization techniques and their combinations on CIFAR10, see Sec. 4.2 for details.",
1902.09843v1,Which network type is used for the MNIST dataset?,Feedforward,1902.09843v1-Table2-1.png, Summaries of the models utilized for our experiments.,
1902.09843v1,Which optimizer has the best performance in terms of test accuracy?,AMSGrad,1902.09843v1-Figure2-1.png, Training (left) and test accuracy (right) for feedforward neural network on MNIST.,
1902.09843v1, What is the difference between the update rules for SGD and ADAM?," SGD updates the parameters using the current gradient, while ADAM uses a weighted average of past gradients and their squares.",1902.09843v1-Table1-1.png, An overview of popular optimization methods using the generic framework.,
1902.09843v1,How does the value of α* affect the test accuracy of ADABOUND?,The test accuracy of ADABOUND increases as the value of α* increases.,1902.09843v1-Figure5-1.png, Test accuracy of ADABOUND with different β using ResNet-34 on CIFAR-10.,
1902.09843v1,What is the effect of decreasing the learning rate on the test accuracy of SGDM and ADABOUND?,Decreasing the learning rate generally leads to higher test accuracy for both SGDM and ADABOUND.,1902.09843v1-Figure6-1.png, Test accuracy of SGDM/ADABOUND with different α/α∗ using ResNet-34 on CIFAR-10. The result of SGDM with α = 1 is not shown above as its performance is too poor (lower than 70%) to be plotted together with other results in a single figure.,
1902.09843v1,How does the performance of AdaBound compare to SGDM with different values of α/α∗?,"In general, AdaBound performs better than SGDM, with higher test accuracy and less variance in the results. This is especially evident for smaller values of α/α∗.",1902.09843v1-Figure7-1.png, Comparison of test accuracy between SGDM and ADABOUND with different α/α∗.,
1902.09843v1,Which optimizer consistently achieved the lowest perplexity across all three LSTM architectures?,AMSBound,1902.09843v1-Figure4-1.png," Perplexity curves on the test set comparing SGD, ADAM, ADABOUND and AMSBOUND for the LSTM with different layers on Penn Treebank.",
1902.09843v1,Which optimizer achieves the highest test accuracy for DenseNet-121?,Adam,1902.09843v1-Figure3-1.png, Training and test accuracy for DenseNet-121 and ResNet-34 on CIFAR-10.,
1902.09843v1,Which parameter has the highest learning rate?,w7,1902.09843v1-Figure1-1.png, Learning rates of sampled parameters. Each cell contains a value obtained by conducting a logarithmic operation on the learning rate. The lighter cell stands for the smaller learning rate.,
1902.09843v1,What happens to the learning rate of the first layer (left plot) between epochs 125 and 175?,The learning rate of the first layer increases between epochs 125 and 175.,1902.09843v1-Figure8-1.png, The evolution of learning rates over time in two randomly chosen layers.,
1903.05854v1,Which of the images in (b) and (c) is semantically consistent with the text descriptions?,The image in (c) is semantically consistent with the text descriptions.,1903.05854v1-Figure1-1.png," (a) Illustration of the mirror structure that embodies the idea of learning text-to-image generation by redescription. (b)-(c) Semantically inconsistent and consistent images/redescriptions generated by [34] and the proposed MirrorGAN, respectively.",
1903.05854v1,Which model performs the best on the COCO dataset?,AttnGAN performs the best on the COCO dataset with an Inception Score of 25.89 ± 0.47.,1903.05854v1-Table1-1.png, Inception Scores of state-of-the-art methods and MirrorGAN on CUB and COCO datasets.,
1903.05854v1,How does MirrorGAN compare to AttnGAN in terms of R-precision on the CUB dataset?,MirrorGAN outperforms AttnGAN in terms of R-precision on the CUB dataset.,1903.05854v1-Table2-1.png, R-precision [%] of the state-of-the-art AttnGAN [34] and MirrorGAN on CUB and COCO datasets.,
1903.05854v1,"What are the differences between the images generated by AttnGAN, MirrorGAN Baseline, and MirrorGAN?",The images generated by AttnGAN are more detailed and realistic than the images generated by MirrorGAN Baseline. The images generated by MirrorGAN are even more detailed and realistic than the images generated by AttnGAN.,1903.05854v1-Figure3-1.png," Examples of images generated by (a) AttnGAN [34], (b) MirrorGAN Baseline, and (c) MirrorGAN conditioned on text descriptions from CUB and COCO test sets and (d) the corresponding ground truth.",
1903.05854v1,Which GAN architecture performs better in terms of authenticity?,MirrorGAN,1903.05854v1-Figure4-1.png, Results of Human perceptual test. A higher value of the Authenticity Test means more convincing images. A higher value of the Semantic Consistency Test means a closer semantics between input text and generated images.,
1903.05854v1,What is the effect of increasing the weight λ on the Inception Score and R-precision of MirrorGAN?,Increasing the weight λ generally increases the Inception Score and R-precision of MirrorGAN.,1903.05854v1-Table3-1.png, Inception Score and R-precision results of MirrorGAN with different weight settings.,
1903.05854v1,What are the three main modules of MirrorGAN and how do they interact with each other?,"The three main modules of MirrorGAN are STEM (Semantic Text Embedding Module), GLAM (Global-Local collaborative Attentive Module), and STREAM (Semantic Text REgeneration and Alignment Module). STEM encodes the input text into a semantic feature vector. GLAM uses this feature vector to generate an image. STREAM regenerates the text from the image and aligns it with the original text.",1903.05854v1-Figure2-1.png, Schematic of the proposed MirrorGAN for text-to-image generation.,
1903.05854v1,What is the purpose of the attention visualization?,The attention visualization is used to show how the model focuses on different parts of the image when generating the output image.,1903.05854v1-Figure5-1.png," Attention visualization on the CUB and the COCO test sets. The first row shows the output 64 × 64 images generated by G0, 128× 128 images generated by G1 and 256× 256 images generated by G2. And the following rows show the Global-Local attention generated in stage 1 and 2. Please refer to the supplementary material for more examples.",
1903.05854v1,What is the effect of changing the text descriptions on the generated images?,"Changing the text descriptions can change the generated images in a variety of ways, such as changing the color of the bird, the pose of the bird, or the background of the image.",1903.05854v1-Figure6-1.png, Images generated by MirrorGAN by modifying the text descriptions by a single word and the corresponding top-2 attention maps in the last stage.,
1903.11027v5,What type of information is provided by the lidar data?,"The lidar data provides information about the depth and shape of objects in the scene. This can be seen in the bottom left image, where the lidar data shows a 3D representation of the scene, with different colors representing different objects.",1903.11027v5-Figure1-1.png," An example from the nuScenes dataset. We see 6 different camera views, lidar and radar data, as well as the human annotated semantic map. At the bottom we show the human written scene description.",
1903.11027v5,Which method performs better for pedestrian detection?,PointPillars.,1903.11027v5-Table7-1.png," Detailed detection performance for PointPillars [51] (top) and MonoDIS [70] (bottom) on the test set. AP: average precision averaged over distance thresholds (%), ATE: average translation error (m), ASE: average scale error (1-IOU), AOE: average orientation error (rad), AVE: average velocity error (m/s), AAE: average attribute error (1− acc.), N/A: not applicable (Section 3.1). nuScenes Detection Score (NDS) = 45.3% (PointPillars) and 38.4% (MonoDIS).",
1903.11027v5,Which object detection method performs the best on rain data?,PP.,1903.11027v5-Table6-1.png," Object detection performance drop evaluated on subsets of the nuScenes val set. Performance is reported as the relative drop in mAP compared to evaluating on the entire val set. We evaluate the performance on Singapore data, rain data and night data for three object detection methods. Note that the MDIS results are not directly comparable to other sections of this work, as a ResNet34 [39] backbone and a different training protocol are used. (†) use only monocular camera images as input. PP uses only lidar.",
1903.11027v5,What are the different types of weather conditions shown in the images?,"The images show clear weather, nighttime, rain, and construction zones.",1903.11027v5-Figure2-1.png," Front camera images collected from clear weather (col 1), nighttime (col 2), rain (col 3) and construction zones (col 4).",
1903.11027v5,Which model performs best with the least amount of training data?,OFT,1903.11027v5-Figure6-1.png, Amount of training data vs. mean Average Precision (mAP) on the val set of nuScenes. The dashed black line corresponds to the amount of training data in KITTI [32].,
1903.11027v5,Which matching function is more suitable for detecting pedestrians and bicycles?,IOU is more suitable for detecting pedestrians and bicycles.,1903.11027v5-Figure7-1.png, Average precision vs. matching function. CD: Center distance. IOU: Intersection over union. We use IOU = 0.7 for car and IOU = 0.5 for pedestrian and bicycle following KITTI [32]. We use CD = 2m for the TP metrics in Section 3.1.,
1903.11027v5,What is the relationship between the lidar points and the camera images in the reconstructed scene?,The lidar points are projected onto an image plane and assigned colors based on the pixel color from the camera data. This allows the reconstructed scene to have both the depth information from the lidar and the color information from the camera.,1903.11027v5-Figure15-1.png, Sample scene reconstruction given lidar points and camera images. We project the lidar points in an image plane with colors assigned based on the pixel color from the camera data.,
1903.11027v5,In which direction and at what distance are bicycles most likely to be found relative to the ego-vehicle?,"Bicycles are most likely to be found directly in front of the ego-vehicle, at a distance of less than 50 meters.",1903.11027v5-Figure12-1.png," Polar log-scaled density map for box annotations where the radial axis is the distance from the ego-vehicle in meters and the polar axis is the yaw angle wrt to the ego-vehicle. The darker the bin is, the more box annotations in that area. Here, we only show the density up to 150m radial distance for all maps, but car would have annotations up to 200m.",
1903.11027v5,How does the number of lidar and radar points in an object change with increasing radial distance from the ego-vehicle?,The number of lidar and radar points in an object decreases with increasing radial distance from the ego-vehicle.,1903.11027v5-Figure14-1.png, Hexbin log-scaled density plots of the number of lidar and radar points inside a box annotation. The black line represents the mean number of points for a given distance wrt the ego-vehicle.,
1903.11027v5,Which of the three object categories has the most lidar points at a radial distance of 50 meters?,Cars,1903.11027v5-Figure13-1.png," Hexbin log-scaled density plots of the number of lidar points inside a box annotation stratified by categories (car, pedestrian and bicycle.",
1903.11027v5,Which dataset has the most 3D boxes?,Waymo Open [76],1903.11027v5-Table1-1.png," AV dataset comparison. The top part of the table indicates datasets without range data. The middle and lower parts indicate datasets (not publications) with range data released until and after the initial release of this dataset. We use bold highlights to indicate the best entries in every column among the datasets with range data. Only datasets which provide annotations for at least car, pedestrian and bicycle are included in this comparison. (†) We report numbers only for scenes annotated with cuboids. (‡) The current Waymo Open dataset size is comparable to nuScenes, but at a 5x higher annotation frequency. (††) Lidar pointcloud count collected from each lidar. (**) [41] provides static depth maps. (-) indicates that no information is provided. SG: Singapore, NY: New York, SF: San Francisco, PT: Pittsburgh, AS: ApolloScape.",
1903.11027v5,What is the most frequent category in the dataset?,Cars.,1903.11027v5-Figure8-1.png," Top: Number of annotations per category. Bottom: Attributes distribution for selected categories. Cars and adults are the most frequent categories in our dataset, while ambulance is the least frequent. The attribute plot also shows some expected patterns: construction vehicles are rarely moving, pedestrians are rarely sitting while buses are commonly moving.",
1903.11027v5,What is the most common absolute velocity for cars?,The most common absolute velocity for cars is around 5 m/s.,1903.11027v5-Figure11-1.png, Absolute velocities. We only look at moving objects with speed > 0.5m/s.,
1903.11027v5,What is the relationship between the number of annotations per keyframe and the size of the bounding box for cars?,"The number of annotations per keyframe is inversely proportional to the size of the bounding box for cars. In other words, as the size of the bounding box increases, the number of annotations per keyframe decreases.",1903.11027v5-Figure9-1.png," Left: Bounding box size distributions for car. Right: Category count in each keyframe for car, pedestrian, and bicycle.",
1903.11027v5,"Based on the figure, what is the relationship between the radial distance and the orientation of objects?","The figure shows that there is no clear relationship between the radial distance and the orientation of objects. For example, cars are found at all radial distances and orientations, and the same is true for pedestrians and bicycles.",1903.11027v5-Figure10-1.png, Top: radial distance of objects from the ego vehicle. Bottom: orientation of boxes in box coordinate frame.,
1903.11027v5,Which method achieves the best performance on the NDS metric?,Megvii [90],1903.11027v5-Table4-1.png," Object detection results on the test set of nuScenes. PointPillars, OFT and SSD+3D are baselines provided in this paper, other methods are the top submissions to the nuScenes detection challenge leaderboard. (†) use only monocular camera images as input. All other methods use lidar. PP: PointPillars [51], MDIS: MonoDIS [70].",
1903.11027v5,What is the effect of increasing the number of lidar sweeps on the performance of PointPillars?,Increasing the number of lidar sweeps leads to a significant performance increase.,1903.11027v5-Table3-1.png, PointPillars [51] detection performance on the val set. We can see that more lidar sweeps lead to a significant performance increase and that pretraining with ImageNet is on par with KITTI.,
1903.11027v5,What sensors are located on the front of the car?,The sensors located on the front of the car are two RADAR sensors and three CAM sensors.,1903.11027v5-Figure4-1.png, Sensor setup for our data collection platform.,
1903.11027v5,What is the capture frequency of the 6x Camera?,12Hz,1903.11027v5-Table2-1.png, Sensor data in nuScenes.,
1903.11027v5,What does the image show?,The image shows a semantic map of nuScenes with 11 semantic layers in different colors. The black spheres represent the path of the ego vehicle.,1903.11027v5-Figure3-1.png, Semantic map of nuScenes with 11 semantic layers in different colors. To show the path of the ego vehicle we plot each keyframe ego pose from scene-0121 with black spheres.,
1903.11027v5,"Which general nuScenes class is mapped to the ""pedestrian"" class in the tracking challenge?","adult, child, construction_worker, and police_officer",1903.11027v5-Table5-1.png, Mapping from general classes in nuScenes to the classes used in the detection and tracking challenges. Note that for brevity we omit most prefixes for the general nuScenes classes.,
1903.11027v5,What do the colors on the map represent?,The colors on the map represent the number of keyframes with ego vehicle poses within a 100m radius across all scenes.,1903.11027v5-Figure5-1.png, Spatial data coverage for two nuScenes locations. Colors indicate the number of keyframes with ego vehicle poses within a 100m radius across all scenes.,
1903.11027v5,Which method has the highest sAMOTA score?,Stan [16],1903.11027v5-Table8-1.png," Tracking results on the test set of nuScenes. PointPillars, MonoDIS (MaAB) and Megvii (MeAB) are submissions from the detection challenge, each using the AB3DMOT [77] tracking baseline. StanfordIPRL-TRI (Stan), VVte (VV-team), CenterTrack-Open (CeOp) and CenterTrack-Vision (CeVi) are the top submissions to the nuScenes tracking challenge leaderboard. (†) use only monocular camera images as input. CeOp uses lidar and camera. All other methods use only lidar.",
1903.11027v5,Which object type has the highest AP when the distance to the semantic prior map is 0 meters?,Car.,1903.11027v5-Figure16-1.png," PointPillars [51] detection performance vs. semantic prior map location on the val set. For the best lidar network (10 lidar sweeps with ImageNet pretraining), the predictions and ground truth annotations were only included if within a given distance of the semantic prior map.",
1903.11027v5,What is the effect of increasing the occlusion threshold on the recall vs precision curve for cars?,"As the occlusion threshold increases, the recall vs precision curve for cars shifts to the left, indicating that the model is able to detect fewer cars at a given level of precision.",1903.11027v5-Figure17-1.png, Per class results for PointPillars on the nuScenes test set taken from the detection leaderboard.,
1904.06493v4, What is the difference between Double-Head and Double-Head-Ext?, Double-Head-Ext extends Double-Head by introducing supervision from unfocused tasks during training and combining classification scores from both heads during inference.,1904.06493v4-Figure1-1.png," Comparison between single head and double heads, (a) a single fully connected (2-fc) head, (b) a single convolution head, (c) Double-Head, which splits classification and localization on a fully connected head and a convolution head respectively, and (d) Double-Head-Ext, which extends Double-Head by introducing supervision from unfocused tasks during training and combining classification scores from both heads during inference.",
1904.06493v4,Which model performs better in terms of classification score?,Single-Conv.,1904.06493v4-Figure6-1.png," Comparison between Single-Conv and Double-Conv. Left: mean and standard deviation of classification scores. Right: mean and standard deviation of IoUs between regressed boxes and their corresponding ground truth. Single-Conv has higher classification scores than Double-Conv, while regression results are comparable.",
1904.06493v4,Which head structure achieves the best performance according to the table?,Double-Head,1904.06493v4-Table1-1.png," Evaluations of detectors with different head structures on COCO val2017. The backbone is FPN with ResNet-50. The top group shows performances for single head detectors. The middle group shows performances for detectors with double heads. The weight for each loss (classification and bounding box regression) is set to 1.0. Compared to the middle group, the bottom group uses different loss weight for fc-head and conv-head (ωfc = 2.0, ωconv = 2.5). Clearly, Double-Head has the best performance, outperforming others by a non-negligible margin. Double-HeadReverse has the worst performance.",
1904.06493v4,Which model performs better in terms of classification score and regression results?,Double-FC.,1904.06493v4-Figure7-1.png, Comparison between Single-FC and Double-FC. (a): mean and standard deviation of classification scores. (b): zooming in of the box in plot-(a). (c): mean and standard deviation of IoUs between regressed boxes and their corresponding ground truth. (d): zooming in of the box in plot-(c). Double-FC has slightly higher classification scores and better regression results than Single-FC.,
1904.06493v4,Why does the fc-head method produce duplicate detections in both Case I and Case II?,The fc-head method produces duplicate detections because it generates inaccurate proposals and has less accurate box regression compared to the conv-head method.,1904.06493v4-FigureA.5-1.png,"Figure A.5. conv-head is more suitable for localization than fc-head. This figure includes two cases. Each case has two rows. The bottom row shows ground truth, detection results using conv-head alone, fc-head alone, and our Double-Head-Ext (from left to right). fc-head has a duplicate detection for the baseball bat in case I and for the surfing board in case II (in the red box). The duplicate detection is generated from an inaccurate proposal (shown in the top row), but is not removed by NMS due to its low IoU with other detection boxes. In contrast, conv-head has more accurate box regression for the same proposal, with higher IoU with other detection boxes. Thus it is removed by NMS, resulting no duplication.",
1904.06493v4,What is the best combination of λfc and λconv for the Double-Head-Ext model?,The best combination of λfc and λconv for the Double-Head-Ext model is λfc = 0.9 and λconv = 0.7.,1904.06493v4-Figure8-1.png," AP over balance weights λfc and λconv . For each (λfc, λconv) pair, we trained a Double-Head-Ext model. Note that the vanilla Double-Head is a special case with λfc = 1, λconv = 1. For each model, we evaluate AP in four ways: (a) using conv-head alone, (b) using fc-head alone, (c) using classification from fc-head and bounding box from conv-head, and (d) using classification fusion from both heads and bounding box from conv-head. Note that the first row in (a) and (d) is not available, due to the unavailability of classification in conv-head when λconv = 1. The last column in (b) is not available, due to the unavailability of bound box regression in fc-head when λfc = 1.",
1904.06493v4,"Which fusion method performed the best in terms of AP, AP0.5, and AP0.75?",Complementary fusion,1904.06493v4-Table3-1.png," Fusion of classifiers from both heads. Complementary fusion (Eq. 4) outperforms others. The model is trained using weights λfc = 0.7, λconv = 0.8.",
1904.06493v4,Why is fc-head more suitable for classification than conv-head?,Because fc-head can detect objects that conv-head misses.,1904.06493v4-FigureA.4-1.png,"Figure A.4. fc-head head is more suitable for classification than conv-head. This figure includes three cases. Each case has two rows. The bottom row shows ground truth, detection results using conv-head alone, fc-head alone, and our Double-Head-Ext (from left to right). The conv-head misses objects in the red box. In contrast, these missing objects are successfully detected by fc-head (shown in the corresponding green box). The top row zooms in the red and green boxes, and shows classification scores from the two heads for each object. The missed objects in conv-head have small classification scores, compared to fc-head.",
1904.06493v4,"Which of the two heads, conv-head or fc-head, has a higher spatial correlation in its output feature map?",conv-head has a higher spatial correlation in its output feature map than fc-head.,1904.06493v4-Figure4-1.png, Left: Spatial correlation in output feature map of convhead. Middle: Spatial correlation in output feature map of fc-head. Right: Spatial correlation in weight parameters of fc-head. convhead has significantly more spatial correlation in output feature map than fc-head. fc-head has a similar spatial correlation pattern in output feature map and weight parameters.,
1904.06493v4,Which head type has better regression results?,conv-head,1904.06493v4-Figure2-1.png, Comparison between fc-head and conv-head. Top row: mean and standard deviation of classification scores. Bottom row: mean and standard deviation of IoUs between regressed boxes and their corresponding ground truth. Classification scores in fc-head are more correlated to proposal IoUs than in conv-head. conv-head has better regression results than fc-head.,
1904.06493v4,"Which type of proposals, predefined or RPN-generated, have a higher Pearson correlation coefficient for small objects?",Predefined proposals.,1904.06493v4-Figure3-1.png," Pearson correlation coefficient (PCC) between classification scores and IoUs. Left: PCC of predefined proposals for large, medium and small objects. Right: PCC of proposals generated by RPN and detected boxes after NMS.",
1904.06493v4,Which method achieved the highest AP score on the COCO val2017 dataset?,Double-Head-Ext with ResNet-101 backbone.,1904.06493v4-Table5-1.png, Object detection results (bounding box AP) on COCO val2017. Note that FPN baseline only has fc-head. Our Double-Head and Double-Head-Ext outperform both Faster R-CNN and FPN baselines on two backbones (ResNet-50 and ResNet-101).,
1904.06493v4,Which method has the highest average precision (AP) across all categories?,Double-Head-Ext.,1904.06493v4-Table6-1.png," Object detection results (bounding box AP), vs. state-of-the-art on COCO test-dev. All methods are in the family of two-stage detectors with a single training stage. Our Double-Head-Ext achieves the best performance.",
1904.06493v4,How does the residual bottleneck block differ from the residual block?,"The residual bottleneck block uses 1x1 convolutions to reduce the number of channels before the 3x3 convolution, and then uses another 1x1 convolution to increase the number of channels back to the original number. This allows the network to learn more complex features with fewer parameters.",1904.06493v4-Figure5-1.png," Network architectures of three components: (a) residual block to increase the number of channels (from 256 to 1024), (b) residual bottleneck block, and (c) non-local block.",
1904.06493v4,"How does the performance of the conv-head and fc-head compare on easy, medium, and hard classes?","The conv-head performs better than the fc-head on all classes, but the difference is most pronounced on the hard classes.",1904.06493v4-FigureA.1-1.png,"Figure A.1. Comparison between fc-head and conv-head on easy, medium and hard classes. Top row: mean and standard deviation of classification scores. Bottom row: mean and standard deviation of IoUs between regressed boxes and their corresponding ground truth.",
1904.06493v4,Which head type performs better for easy classes?,The conv-head performs better for easy classes.,1904.06493v4-FigureA.2-1.png,"Figure A.2. Pearson correlation coefficient (PCC) between classification scores and IoUs of predefined proposals for easy, medium and hard classes.",
1904.06493v4,Which head performs better in terms of classification score and regressed box IoU?,Double-Head-Ext performs better than the single heads in terms of both classification score and regressed box IoU.,1904.06493v4-FigureA.3-1.png,Figure A.3. Comparison of classification and bounding box regression. Left: mean of classification scores between single heads and Double-Head-Ext. Right: mean of regressed box IoUs between single heads and Double-Head-Ext.,
1904.08993v2,"What is the input for the task ""play-52""?","The input for the task ""play-52"" is ""@B4/X;3MjKdyZzC"".",1904.08993v2-Figure4-1.png, Examples of randomly generated play tasks for the string transformation experiment.,
1904.08993v2,How does the performance of Playgol and Playgol_NH3 compare to the baseline in the 5^2 space?,Playgol and Playgol_NH3 both outperform the baseline in the 5^2 space.,1904.08993v2-Figure2-1.png, Robot experiment results. The baseline represents learning without play (i.e. Metagol).,
1904.08993v2,What is the relationship between the input and output in the table?,The output is the three-letter abbreviation of the month of birth.,1904.08993v2-Figure3-1.png, Examples for the build 95 string transformation problem.,
1904.08993v2,How does the predictive accuracy of Playgol and Playgol_nh3 compare to the baseline?,Both Playgol and Playgol_nh3 outperform the baseline.,1904.08993v2-Figure5-1.png, String experiment results.,
1904.08993v2,What is the purpose of the predicate `play_228`?,The predicate `play_228` is used to skip to the first uppercase letter in a string and then copy that letter to the output.,1904.08993v2-Figure6-1.png," Program learned by Playgol for the build task build 95 (Figure 3). The solution for build 95 reuses the solution to the play task play 228 and the sub-program play 136 1 from the play task play 136, where play 136 1 is invented. The predicate play 228 is a recursive definition that corresponds to the concept of “skip to the first uppercase letter and then copy the letter to the output”. The predicate play 228 reuses the solution for another play task play 52. Figure 4 shows these play tasks.",
1904.08993v2,Which metarule represents a chain of reasoning where one rule leads to another?,The chain metarule.,1904.08993v2-Figure1-1.png," Example metarules. The letters P , Q, and R denote second-order variables. The letters A, B, and C denote first-order variables.",
1904.10637v1,Who is blaming whom in the sentence?,John B. Taylor is blaming the Fed.,1904.10637v1-Figure1-1.png," An example sentence from our dataset containing a blame tie. The red/bold words are entities involved in a blame tie, and the blue/italic words are supporting evidence that the blame tie exists.",
1904.10637v1,Which model performed the best on the development and test sets?,ELMo,1904.10637v1-Table7-1.png, Experiment results of Context Model using different pretrained word vectors.,
1904.10637v1,Which model performs best on the KNOWN data?,The combined model.,1904.10637v1-Table8-1.png, Experiment results of baseline models and three proprosed models on KNOWN data and ALL data.,
1904.10637v1,Which model performed better on the dev/test sets?,The Combined Model (CMB) performed better on the dev/test sets.,1904.10637v1-Table9-1.png, Model comparison between Context Model (CTX) and Combined Model (CMB) on dev/test sets.,
1904.10637v1,Which model performed the best according to the dev F1 score?,BiLSTMMax,1904.10637v1-Table6-1.png, Experiment results of Context Model using different pooling functions.,
1904.10637v1,Which entity is blamed by Suzanne Mitchell?,People.,1904.10637v1-Table1-1.png," An article titled Rate Cut Has Foes on Main Street (The Wall Street Journal, September 2007). Top: paragraphs of the article containing several blame patterns. The entities are in the brackets. Bottom: blame ties extracted from the article.",
1904.10637v1,Which source entities have the most blame ties?,e3 and e6,1904.10637v1-Table2-1.png, Matrix representation of the blame ties in Table 1.,
1904.10637v1,What is the probability that the blame tie exists between e1 and e3?,0.91,1904.10637v1-Table11-1.png, Prediction result of the article in Table 10 using the Context Model. The number represents the probability that the blame ties exists between the two corresponding entities.,
1904.10637v1,What is the main point of the article?,The article reports on President Trump's accusation that Russia is helping North Korea evade sanctions imposed by the United Nations.,1904.10637v1-Table10-1.png," An article from Bloomberg published on January 18, 2018. Top: paragraphs of the article containing blame patterns. The blame entities are in bold face. Bottom: major entities appearing in the article.",
1904.10637v1,What is the average number of entities per article?,2.97,1904.10637v1-Table4-1.png, Sample statistics.,
1904.10637v1,Which newspaper has the most blame ties?,NYT,1904.10637v1-Table3-1.png, Dataset size for the three newspapers. USA: USA Today. NYT: The New York Times. WSJ: The Wall Street Journal.,
1904.10637v1,What is the purpose of the LSTM layer in the context model?,The LSTM layer is used to encode the sentences.,1904.10637v1-Figure2-1.png," Context Model. LSTM is used to encode the sentences, and the hidden vectors at the positions of the entity are pooled together into a single vector to represent the context of the entity. The source entity context vector and the target entity context vector are concatenated together to be sent to the prediction layer. The prediction layer predicts 1 if the source to target blame tie exists otherwise 0.",
1904.10637v1,What is the average number of sentences in a document?,45 sentences.,1904.10637v1-Table5-1.png, Sentence and words statistics.,
1904.11251v1,Which model performs the best on the ImageNet dataset in terms of Accuracy?,LSTM-P trained on BNC&Wiki performs the best on the ImageNet dataset in terms of Accuracy.,1904.11251v1-Table2-1.png," Novel, F1 and Accuracy scores of our proposed model and other state-of-the-art methods on ImageNet dataset. All values are reported as percentage (%).",
1904.11251v1,Which of the three models generated the most accurate sentence for the image of the woman on the tennis court?,The Ground Truth (GT) model generated the most accurate sentence for the image of the woman on the tennis court.,1904.11251v1-Figure2-1.png," Objects and sentence generation results on held-out COCO. The detected objects are predicted by MIL model in [7], and the output sentences are generated by 1) Ground Truth (GT): one ground truth sentence, 2) LRCN and 3) our LSTM-P.",
1904.11251v1,Which model performed better at generating sentences that describe the images?,LSTM-P performed better at generating sentences that describe the images.,1904.11251v1-Figure3-1.png," Objects and sentence generation results on ImageNet. GT denotes the ground truth object. The detected objects are predicted by the standard CNN architecture [23], and the output sentences are generated by 1) LRCN and 2) our LSTM-P.",
1904.11251v1,Which model performed the best overall on the held-out COCO dataset for novel object captioning?,LSTM-P,1904.11251v1-Table1-1.png," Per-object F1, averaged F1, SPICE, METEOR, and CIDEr scores of our proposed model and other state-of-the-art methods on held-out COCO dataset for novel object captioning. All values are reported as percentage (%).",
1904.11251v1,What is the role of the pointing mechanism in the LSTM-P model?,"The pointing mechanism decides whether to copy a word from the recognized objects or to generate a word from the LSTM based on the contextual information (i.e., current input word and LSTM hidden state).",1904.11251v1-Figure1-1.png," An overview of our Long Short-Term Memory with Pointing (LSTM-P) for novel object captioning (better viewed in color). The image representation extracted by CNN is firstly injected into LSTM at the initial time for triggering the standard word-by-word sentence generation. The output of LSTM is the probability distribution over all the words in the vocabulary at each decoding time. Meanwhile, the object learners pre-trained on object recognition data are utilized to detect the objects within the input image. Such predicted score distribution over objects are further injected into a copying layer along with the current hidden state of LSTM, producing the probability distribution of being copied over the recognized objects. To dynamically accommodate word generation via LSTM and word copying from learnt objects, a pointing mechanism is specially devised to elegantly point when to copy the object depending on contextual information (i.e., current input word and LSTM hidden state). The whole LSTM-P is trained by minimizing two objectives in an end-to-end manner: (1) the widely-adopted sequential loss that enforces the syntactic coherence of output sentence, and (2) the sentence-level coverage loss that encourages the maximum coverage of all objects found in the image, which is independent of the position in the sentence.",
1904.11251v1,How does the LSTM-P model use its pointer mechanism to generate sentences based on the images it is provided?,The LSTM-P model uses its pointer mechanism to generate sentences by either generating a word via LSTM or copying a word from recognized objects in the image. The weights for these two options are visualized in the bar plot at each decoding step.,1904.11251v1-Figure4-1.png, Sentence generation results with visualized weights learnt in pointer mechanism of our LSTM-P at each decoding step on held-out COCO dataset. The bar plot at each decoding step corresponds to the weights for generating a word via LSTM or copying a word from recognized objects when the corresponding word was generated.,
1904.11251v1,What is the effect of the tradeoff parameter λ on F1_average and METEOR?,"The effect of the tradeoff parameter λ on F1_average and METEOR is that it is non-monotonic. As λ increases, both F1_average and METEOR initially increase, then decrease, and then increase again.",1904.11251v1-Figure5-1.png, The effect of the tradeoff parameter λ in our LSTM-P over (a) F1average (%) and (b) METEOR (%) on held-out COCO.,
1905.12304v3,Which method achieved the highest accuracy in the gender transfer task?,Ours (style-strengthen),1905.12304v3-Table4-1.png," Evaluation results of the gender transfer task on Yelp. For our models, we report different results corresponding to different choices of hyper-parameters (λc and β) to demonstrate our models’ ability to control the trade-off between attribute transfer and content preservation. The accuracy of the classifier on the test set is 83.1%.",
1905.12304v3,Which method achieved the highest accuracy on the Yelp dataset?,The human method achieved the highest accuracy on the Yelp dataset with a score of 4.1.,1905.12304v3-Table3-1.png," Human evaluation results of the sentiment transfer tasks on Yelp and Amazon. We show average human ratings for transfer accuracy (Acc), preservation of content (Con), and fluency of sentences (Gra) on 1 to 5 score. “Suc%"" denotes the overall percentage success rate. Similar to previous works, we consider a generated output “successful"" if it is rated no less than 3 on all three criteria (Att, Con, Gra).",
1905.12304v3,Which method is the most successful at transferring sentiment from negative to positive?,Ours (style-content balance),1905.12304v3-Table2-1.png," Samples of the sentiment transfer task from ours and baselines on Yelp. The Original denotes the input sentence, and the Human denotes the human annotated sentence. The samples of the sentiment transfer from negative to positive and positive to negative are shown in top and bottom, respectively.",
1905.12304v3,What is the sentiment transfer task?,"The sentiment transfer task is the process of changing the sentiment of a sentence from negative to positive or positive to negative, while preserving the factual content of the sentence.",1905.12304v3-Table7-1.png," Samples of the sentiment transfer task from ours and baselines on Yelp. The Original denotes the input sentence, and the Human denotes the human annotated sentence. The samples of the sentiment transfer from negative to positive and positive to negative are shown in top and bottom, respectively.",
1905.12304v3,Which set of attributes being controlled by the model resulted in the highest accuracy?,Sentiment + Length↑,1905.12304v3-Table5-1.png, Results of fine-grained Attributes control on the Yelp. Different rows correspond to the set of attributes being controlled by the model.,
1905.12304v3,Which configuration resulted in the highest accuracy?,The configuration with λc = 0.05 and retraining the content predictor resulted in the highest accuracy of 94.1%.,1905.12304v3-Table6-1.png," Evaluation results of different hyper-parameters and configurations of the sentiment transfer task on Yelp. The notation ↑ means the higher is the better, while ↓ means the lower is the better.",
1905.12304v3,What is the effect of adding sentiment and keywords to the original text?,Adding sentiment and keywords to the original text can change the overall sentiment of the text.,1905.12304v3-Table10-1.png, Samples of multiple fine-grained attributes control from ours. We bold the pre-defined keyword.,
1905.12304v3," 
What is the relationship between the encoder, decoder, and the error surface of the predictor in the figure? "," 
The encoder takes an input sentence and maps it to a continuous representation in the latent space. The decoder then takes this representation and generates a new sentence. The error surface of the predictor shows how well the predictor is able to predict the sentiment, length, and content of the generated sentence. The goal is to find a representation in the latent space that minimizes the error of the predictor. ",1905.12304v3-Figure1-1.png," There is an example of content-preserving text sentiment transfer, and we hope to further increase the length of the target sentence compared with the original sentence. The original sentence x with negative sentiment is mapped to continuous representation z via encoder. Then z is revised into z∗ by minimizing the error LAttr,s1(θs1 ; s1 = {sentiment = positive}) + LAttr,s2(θs2 ; s2 = {length = 20}) + λbowLBOW(θbow;xbow = [burgers,meat]) with the sentiment predictor f1, length predictor f2, and the content predictor fbow. Afterwards the target sentence x∗ is generated by decoding z∗ with beam search via decoder [best viewed in color].",
1905.12304v3,How do the different models compare in terms of their ability to preserve the original meaning of the sentence?,"The ""Ours (balanced)"" model appears to preserve the original meaning of the sentence the best, while the ""Ours (content-strengthen)"" and ""Ours (style-strengthen)"" models sometimes introduce extraneous information or change the meaning of the sentence slightly.",1905.12304v3-Table9-1.png," Samples of the gender style transfer task from ours and baselines. The Original denotes the input sentence. The samples of the gender style transfer from male to female and female to male are shown in top and bottom, respectively.",
1905.12304v3,Which model is the most effective at transferring sentiment from negative to positive?,Our (style-strengthen) model is the most effective at transferring sentiment from negative to positive.,1905.12304v3-Table8-1.png," Samples of the sentiment transfer task from ours and baselines on Amazon. The Original denotes the input sentence, and the Human denotes the human annotated sentence. The samples of the sentiment transfer from negative to positive and positive to negative are shown in top and bottom, respectively.",
1905.12304v3,Which model performs best in terms of BLEU score on the Yelp dataset?,Our (style-strengthen) model performs best in terms of BLEU score on the Yelp dataset.,1905.12304v3-Table1-1.png," Evaluation results of the sentiment transfer tasks on Yelp (Top) and Amazon (Bottom). The notation ↑ means the higher the better, while ↓ means the lower the better. For our models, we report different results (denoted as Ours (content-strengthen), Ours (style-content balance), and Ours (style-strengthen)) corresponding to different choices of hyper-parameters (λc and β), which demonstrates our models’ ability to control the trade-off between attribute transfer and content preservation. For each evaluation criterion, we bold the best values (except for Human and Original). The accuracies of the classifier on the test set of Yelp and Amazon are 98.2% and 84.0%. Note that a good model should perform well on all metrics, we further highlight the metrics where the performances of the models are poor with underline.",
1906.02979v1,What is the relationship between the LSC and the frequency of the words in the two corpora?,The LSC is negatively correlated with the frequency of the words in the two corpora. This means that words with a higher LSC tend to have a lower frequency in the two corpora.,1906.02979v1-Table7-1.png," DURel dataset without flott, Kinderstube and Steckenpferd, which were excluded for low frequency. Ca=DTA18, Cb=DTA19. LSC denotes the inverse compare rank from (Schlechtweg et al., 2018), where high values mean high change.",
1906.02979v1,Which word has the highest frequency in the COOK corpus?,Salz,1906.02979v1-Table8-1.png," SURel dataset without Messerspitze, which was excluded for low frequency. Ca=SDEWAC, Cb=COOK. LSC denotes the inverse compare rank from (Schlechtweg et al., 2018), where high values mean high change.",
1906.02979v1,Which alignment methods are compatible with all semantic representations?,Only the CI (centroid initialization) and OP (orthogonal Procrustes) alignment methods are compatible with all semantic representations.,1906.02979v1-Table6-1.png," Combinations of semantic representation, alignment types and measures. (FD has been computed directly from the corpus.)",
1906.02979v1,"What is the highest Spearman ρ score achieved when using the L/P preprocessing method, a window size of 2, and the SGNS space?",0.851,1906.02979v1-Table2-1.png," Best results of ρ scores (Win=Window Size, Preproc=Preprocessing, Align=Alignment, k=negative sampling, t=subsampling, Spearman m(h,l): mean, highest and lowest results).",
1906.02979v1,Which corpus is the largest?,SDEWAC,1906.02979v1-Table1-1.png, Corpora and their approximate sizes.,
1906.02979v1,Which method has the highest mean ρ score for CD across the alignments?,OP+,1906.02979v1-Table4-1.png," Mean ρ scores for CD across the alignments. Applies only to RI, SVD and SGNS.",
1906.02979v1,Which representation and dataset combination achieved the highest best ρ score?,SGNS and DUREL,1906.02979v1-Table3-1.png," Best and mean ρ scores across similarity measures (CD, LND, JSD) on semantic representations.",
1906.02979v1,How does time-shuffling and downampling affect the value of ρ for SGNS+OP+CD on the DU Rel and SU Rel datasets?,"Time-shuffling decreases the value of ρ for both datasets, while downampling further decreases the value of ρ for both datasets.",1906.02979v1-Table5-1.png," ρ for SGNS+OP+CD (L/P, win=2, k=1, t=None) before (ORG) and after time-shuffling (SHF) and downampling them to the same frequency (+DWN).",
1906.03671v2,Which batch selection algorithm resulted in the highest average embedding magnitude in the selected batch?,k-means++,1906.03671v2-Figure2-1.png," A comparison of batch selection algorithms using our gradient embedding. Left and center: Plots showing the log determinant of the Gram matrix of the selected batch of gradient embeddings as learning progresses. Right: The average embedding magnitude (a measurement of predictive uncertainty) in the selected batch. The FF-k-CENTER sampler finds points that are not as diverse or high-magnitude as other samplers. Notice also that k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude than a k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Standard errors are shown by shaded regions.",
1906.03671v2,Which of the two algorithms (k-DPP or k-means++) achieves higher accuracy on the MNIST dataset with an MLP model?,k-DPP achieves higher accuracy than k-means++ on the MNIST dataset with an MLP model.,1906.03671v2-Figure30-1.png, Learning curves and running times for MNIST with MLP.,
1906.03671v2,Which acquisition function performs better when the batch size is 1000?,k-means++ performs better than k-DPP when the batch size is 1000.,1906.03671v2-Figure29-1.png, Learning curves and running times for SVHN with MLP and ResNet.,
1906.03671v2,Which algorithm requires more time to achieve a certain level of accuracy?,k-DPP requires more time to achieve a certain level of accuracy than k-means++.,1906.03671v2-Figure28-1.png, Learning curves and running times for OpenML #184 with MLP.,
1906.03671v2,Which algorithm is more accurate on CIFAR10 with an MLP and a batch size of 1000?,k-means++,1906.03671v2-Figure31-1.png, Learning curves and running times for CIFAR10 with MLP and ResNet.,
1906.03671v2,How does the batch size affect the accuracy of the model?,"The batch size affects the accuracy of the model in a non-monotonic way. For small batch sizes (e.g., 10), the accuracy is lower than for larger batch sizes (e.g., 1000). However, for very large batch sizes (e.g., 10000), the accuracy decreases again.",1906.03671v2-Figure8-1.png, Full learning curves for OpenML #156 with MLP.,
1906.03671v2,How does the batch size affect the accuracy of the model?,The batch size does not seem to have a significant impact on the accuracy of the model.,1906.03671v2-Figure9-1.png, Full learning curves for OpenML #184 with MLP.,
1906.03671v2,"Which algorithm performed the best overall, according to the figure?",Rand,1906.03671v2-Figure20-1.png," Pairwise penalty matrices of the algorithms, grouped by different batch sizes. The parenthesized number in the title is the total number of (D,B,A) combinations aggregated, which is also an upper bound on all its entries. Element (i, j) corresponds roughly to the number of times algorithm i beats algorithm j. Column-wise averages at the bottom show aggregate performance (lower is better). From left to right: batch size = 100, 1000, 10000.",
1906.03671v2,Which algorithm performed the best overall across all neural network models?,Coreset,1906.03671v2-Figure21-1.png," Pairwise penalty matrices of the algorithms, grouped by different neural network models. The parenthesized number in the title is the total number of (D,B,A) combinations aggregated, which is also an upper bound on all its entries. Element (i, j) corresponds roughly to the number of times algorithm i beats algorithm j. Column-wise averages at the bottom show aggregate performance (lower is better). From left to right: MLP, ResNet and VGG.",
1906.03671v2,Which algorithm outperforms all other algorithms the most often?,ALBL,1906.03671v2-Figure4-1.png," A pairwise penalty matrix over all experiments. Element Pi,j corresponds roughly to the number of times algorithm i outperforms algorithm j. Column-wise averages at the bottom show overall performance (lower is better).",
1906.03671v2,Which active learning method performed the best on the SVHN dataset with ResNet and a batch size of 100?,ALBL,1906.03671v2-Figure3-1.png, Active learning test accuracy versus the number of total labeled samples for a range of conditions. Standard errors are shown by shaded regions.,
1906.03671v2,Which algorithm has the best performance for a batch size of 1000?,ALBL,1906.03671v2-Figure22-1.png," CDFs of normalized errors of the algorithms, group by different batch sizes. Higher CDF indicates better performance. From left to right: batch size = 100, 1000, 10000.",
1906.03671v2,Which acquisition function consistently outperforms the others across all three neural network models?,ALBL,1906.03671v2-Figure23-1.png," CDFs of normalized errors of the algorithms, group by different neural network models. Higher CDF indicates better performance. From left to right: MLP, ResNet and VGG.",
1906.03671v2,How does the batch size affect the learning curves of the different active learning algorithms?,"The batch size has a significant impact on the learning curves of the different active learning algorithms. For example, with a batch size of 100, ALBL and Conf perform similarly, while Coreset and BADGE perform slightly worse. However, with a batch size of 10000, ALBL and Conf perform significantly better than the other algorithms. This suggests that the optimal batch size for each algorithm may be different.",1906.03671v2-Figure6-1.png, Full learning curves for OpenML #6 with MLP.,
1906.03671v2,"Which active learning strategy appears to be the most efficient for this dataset and model, based on the learning curves?",ALBL and Coreset.,1906.03671v2-Figure7-1.png, Full learning curves for OpenML #155 with MLP.,
1906.03671v2,How does the performance of different active learning strategies compare on OpenML #156 with an MLP model?,"The figure shows that ALBL and BADGE perform the best, while Margin and Random perform the worst.",1906.03671v2-Figure15-1.png, Zoomed-in learning curves for OpenML #156 with MLP.,
1906.03671v2,Which batch size led to the highest accuracy for the MLP model on OpenML #155?,10000,1906.03671v2-Figure14-1.png, Zoomed-in learning curves for OpenML #155 with MLP.,
1906.03671v2,Which acquisition function performed the best on OpenML #184 with MLP when the batch size was 10000?,BADGE,1906.03671v2-Figure16-1.png, Zoomed-in learning curves for OpenML #184 with MLP.,
1906.03671v2,Which algorithm tends to select batches that are both more diverse and higher-magnitude than a k-DPP?,k-MEANS++,1906.03671v2-Figure24-1.png," A comparison of batch selection algorithms in gradient space. Plots a and b show the log determinants of the Gram matrices of gradient embeddings within batches as learning progresses. Plots c and d show the average embedding magnitude (a measurement of predictive uncertainty) in the selected batch. The k-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Notice also that k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude than a k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Among all algorithms, CONF has the largest average norm of gradient embeddings within a batch; however, in OpenML #6, and the first few interations of SVHN, some batches have a log Gram determinant of −∞ (shown as gaps in the curve), which shows that CONF sometimes selects batches that are inferior in diversity.",
1906.03671v2,Which acquisition function performs the best in terms of normalized error?,BADGE,1906.03671v2-Figure5-1.png, The cumulative distribution function of normalized errors for all acquisition functions.,
1906.03671v2,How do the learning curves for k-MEANS++ and k-DPP sampling compare in terms of accuracy?,"The learning curves for k-MEANS++ and k-DPP sampling nearly perfectly overlap, indicating that they achieve similar accuracy.",1906.03671v2-Figure1-1.png, Left and center: Learning curves for k-MEANS++ and k-DPP sampling with gradient embeddings for different scenarios. The performance of the two sampling approaches nearly perfectly overlaps. Right: A run time comparison (seconds) corresponding to the middle scenario. Each line is the average over five independent experiments. Standard errors are shown by shaded regions.,
1906.03671v2,How does the performance of different active learning methods vary with the batch size?,"The performance of different active learning methods varies with the batch size. For example, ALBL performs better with a smaller batch size, while BADGE performs better with a larger batch size.",1906.03671v2-Figure18-1.png, Zoomed-in learning curves for MNIST with MLP.,
1906.03671v2,Which model performs best when the batch size is 100?,"SVHN, ResNet",1906.03671v2-Figure17-1.png," Zoomed-in learning curves for SVHN with MLP, ResNet and VGG.",
1906.03671v2,Which of the following combinations of network architecture and batch size had the highest accuracy?,ResNet and batch size 10000.,1906.03671v2-Figure12-1.png," Full learning curves for CIFAR10 with MLP, ResNet and VGG.",
1906.03671v2,How does the batch size affect the learning curves of the different active learning strategies?,"The batch size has a significant impact on the learning curves. For example, with a batch size of 100, the learning curves are relatively smooth and converge quickly. However, with a batch size of 10000, the learning curves are much more erratic and take longer to converge.",1906.03671v2-Figure13-1.png, Zoomed-in learning curves for OpenML #6 with MLP.,
1906.03671v2,How does the accuracy of the k-DPP algorithm compare to the accuracy of the k-means++ algorithm when the batch size is 100?,The k-DPP algorithm is more accurate than the k-means++ algorithm when the batch size is 100.,1906.03671v2-Figure25-1.png, Learning curves and running times for OpenML #6 with MLP.,
1906.03671v2,"Which method has the higher accuracy, k-DPP or k-means++?",k-DPP has the higher accuracy.,1906.03671v2-Figure27-1.png, Learning curves and running times for OpenML #156 with MLP.,
1906.03671v2,How does the batch size affect the running time of the k-means++ algorithm?,The running time of the k-means++ algorithm increases with the batch size.,1906.03671v2-Figure26-1.png, Learning curves and running times for OpenML #155 with MLP.,
1906.03671v2,Which active learning strategy performed best for CIFAR10 with ResNet and a batch size of 1000?,Coreset,1906.03671v2-Figure19-1.png," Zoomed-in learning curves for CIFAR10 with MLP, ResNet and VGG.",
1906.03671v2,How does the batch size affect the accuracy of the model?,The accuracy of the model generally increases with the batch size.,1906.03671v2-Figure11-1.png, Full learning curves for MNIST with MLP.,
1906.03671v2,Which active learning strategy achieves the highest accuracy for SVHN with ResNet and a batch size of 1000?,BADGE,1906.03671v2-Figure10-1.png," Full learning curves for SVHN with MLP, ResNet and VGG.",
1906.05478v3,Which architecture is the most robust to changes in the noise distribution?,The bias-free versions of all the architectures are more robust to changes in the noise distribution than their counterparts with bias.,1906.05478v3-Figure19-1.png," Comparisons of architectures with (red curves) and without (blue curves) a net bias for the experimental design described in Section 5. The networks are trained using i.i.d. Gaussian noise but evaluated on noise drawn i.i.d. from a uniform distribution with mean 0. The performance is quantified by the PSNR of the denoised image as a function of the input PSNR of the noisy image. All the architectures with bias perform poorly out of their training range, whereas the bias-free versions all achieve excellent generalization across noise levels, i.e. they are able to generalize across the two different noise distributions. (a) Deep Convolutional Neural Network, DnCNN (Zhang et al., 2017). (b) Recurrent architecture inspired by DURR (Zhang et al., 2018a). (c) Multiscale architecture inspired by the UNet (Ronneberger et al., 2015). (d) Architecture with multiple skip connections inspired by the DenseNet (Huang et al., 2017).",
1906.05478v3,How does the performance of the BF-CNN compare to the DnCNN when the input PSNR is high?,The BF-CNN performs similarly to the DnCNN when the input PSNR is high.,1906.05478v3-Figure18-1.png," Comparison of the performance of a CNN and a BF-CNN with the same architecture for the experimental design described in Section 5. The networks are trained using i.i.d. Gaussian noise but evaluated on noise drawn i.i.d. from a uniform distribution with mean 0. The performance is quantified by the PSNR of the denoised image as a function of the input PSNR of the noisy image. All the architectures with bias perform poorly out of their training range, whereas the bias-free versions all achieve excellent generalization across noise levels, i.e. they are able to generalize across the two different noise distributions. The CNN used for this example is DnCNN (Zhang et al., 2017); using alternative architectures yields similar results (see Figures 19).",
1906.05478v3,What happens to the net bias when the network is trained over a smaller range of noise levels?,The net bias grows explosively for noise levels beyond the training range.,1906.05478v3-Figure1-1.png," First-order analysis of the residual of a denoising convolutional neural network as a function of noise level. The plots show the norms of the residual and the net bias averaged over 100 20× 20 natural-image patches for networks trained over different training ranges. The range of noises used for training is highlighted in blue. (a) When the network is trained over the full range of noise levels (σ ∈ [0, 100]) the net bias is small, growing slightly as the noise increases. (b-c) When the network is trained over the a smaller range (σ ∈ [0, 55] and σ ∈ [0, 30]), the net bias grows explosively for noise levels beyond the training range. This coincides with a dramatic drop in performance, reflected in the difference between the magnitudes of the residual and the true noise. The CNN used for this example is DnCNN (Zhang et al., 2017); using alternative architectures yields similar results as shown in Figure 8.",
1906.05478v3,How do the linear weighting functions of a BF-DnCNN change with increasing noise levels?,The linear weighting functions of a BF-DnCNN become more diffuse and average over a larger region with increasing noise levels.,1906.05478v3-Figure14-1.png," Visualization of the linear weighting functions (rows of Ay) of a BF-DnCNN for three example pixels of a noisy input image (left). The next image is the denoised output. The three images on the right show the linear weighting functions corresponding to each of the indicated pixels (red squares). All weighting functions sum to one, and thus compute a local average (although some weights are negative, indicated in red). Their shapes vary substantially, and are adapted to the underlying image content. Each row corresponds to a noisy input with increasing σ and the filters adapt by averaging over a larger region.",
1906.05896v4,Why does the OCFusion method retain more information in the final output than the fusion by confidence method?,"The OCFusion method queries the occlusion head to determine whether one mask should be placed on top of another, while the fusion by confidence method simply occludes all subsequent instances after the ""person.""",1906.05896v4-Figure1-1.png," An illustration of fusion using masks sorted by detection confidence alone [17] vs. with the ability to query for occlusions (OCFusion; ours). Occlude(A,B) = 0 in occlusion head means mask B should be placed on top of mask A. Mask R-CNN proposes three instance masks listed with decreasing confidence. The heuristic of [17] occludes all subsequent instances after the “person”, while our method retains them in the final output by querying the occlusion head.",
1906.05896v4,"Which method performs better on the Cityscapes val dataset, according to the PQ metric?","The OCFusion method performs better on the Cityscapes val dataset, according to the PQ metric.",1906.05896v4-Table4-1.png, Comparison to our implementation of Panoptic FPN [16] baseline model on the Cityscapes val dataset. All results are based on a ResNet-50 backbone.,
1906.05896v4,Which method achieves the highest PQ score?,OCFusion* with multi-scale testing achieves the highest PQ score of 64.7.,1906.05896v4-Table5-1.png, Comparison to prior work on the Cityscapes val dataset. All results are based on a ResNet-50 backbone. m.s. stands for multi-scale testing. ∗Used deformable convolution.,
1906.05896v4,Which method achieved the highest PQ score?,AUNet achieved the highest PQ score of 55.9.,1906.05896v4-Table3-1.png, Comparison to prior work on the MS-COCO test-dev dataset. m.s. stands for multi-scale testing. ∗Used deformable convolution.,
1906.05896v4,Which method performs best on the MS-COCO val dataset when multi-scale testing is used?,OCFusion* with ResNetXt-101 backbone.,1906.05896v4-Table2-1.png, Comparison to prior work on the MS-COCO val dataset. m.s. stands for multi-scale testing. ∗Used deformable convolution.,
1906.05896v4,What is the role of the occlusion head in the thing branch of the network?,"The occlusion head is responsible for determining whether two mask proposals overlap significantly, and if so, it outputs a binary value indicating whether one mask occludes the other.",1906.05896v4-Figure2-1.png," Illustration of the overall architecture. The FPN is used as a shared backbone for both thing and stuff branches. In thing branch, Mask R-CNN will generate instance mask proposals, and the occlusion head will output binary values Occlude(Mi,Mj) (Equation 1) for each pair of mask proposalsMi andMj with appreciable overlap (larger than a threshold) to indicate occlusion relation between them. Occlusion head architecture is described in Section 2.4. Fusion process is described in 2.3.",
1906.05896v4,What is the difference between the ground truth and the Kirillov et al. and OCFusion (ours) results?,"The ground truth is a more accurate representation of the scene, with more detail and less noise. The Kirillov et al. and OCFusion results are more blurry and have less detail.",1906.05896v4-Figure4-1.png, Comparison against Kirillov et al. [16] which uses fusion by confidence.,
1906.05896v4,Which of the following images is the output of the Spatial Ranking Module?,The third image from the right.,1906.05896v4-Figure5-1.png, Comparison against Spatial Ranking Module [23].,
1906.05896v4, How does the proposed OCFusion method compare to UPSNet in terms of accuracy? ," The proposed OCFusion method appears to be more accurate than UPSNet, as it produces segmentations that are closer to the ground truth.",1906.05896v4-Figure6-1.png, Comparison against UPSNet [38].,
1906.05896v4,What is the difference between the ground truth masks and the Mask R-CNN predictions in (a) and (b)?,"In (a), the ground truth masks show the individual objects in the scene, while the Mask R-CNN predictions show the objects as a single, merged object. In (b), the ground truth mask shows the dining table as a single object, while the Mask R-CNN prediction shows the dining table as a large, occluding object that covers the entire image.",1906.05896v4-Figure3-1.png, Images and ground truth masks from the COCO dataset. (a) is an example where even predicting the ground truth mask creates ambiguity when attempting to assign pixels to instances in a greedy manner. The baseline fusion process [17] is unable to properly assign these as shown in the 2nd and 4th images of the rightmost column whereas our method is able to handle the occlusion relationship present as shown in the 1st and 3rd images of the rightmost column. (b) is an example where Mask R-CNN baseline produces an instance prediction that occludes the entire image and creates the same ambiguity in (a) despite an unambiguous ground truth annotation.,
1906.05896v4,Which hyperparameter value results in the highest performance?,The hyperparameter value of 0.05 results in the highest performance.,1906.05896v4-Table7-1.png, COCO Hyperparameter Ablation: PQ,
1906.05896v4,What is the relationship between τ and PQ?,"There is a positive relationship between τ and PQ. As τ increases, PQ also increases.",1906.05896v4-Table8-1.png, Cityscapes Hyperparameter Ablation: PQ,
1906.05896v4,"What is the effect of enabling inter-class and intra-class occlusion on the PQ, PQQth, and PQQst metrics?","Enabling inter-class and intra-class occlusion increases the PQ, PQQth, and PQQst metrics.",1906.05896v4-Table9-1.png, Ablation study on different types of occlusion on the Cityscapes val dataset. Xmeans capability enabled.,
1906.05896v4,"Which method is faster, Baseline or OCFusion?",Baseline is faster than OCFusion.,1906.05896v4-Table6-1.png, Runtime (ms) overhead per image. Runtime results are averaged over the entire COCO and Cityscapes validation dataset. We use a single GeForce GTX 1080 Ti GPU and Xeon(R) CPU E5-2687W CPU.,
1906.05896v4,What is the effect of enabling intraclass capability?,Enabling intraclass capability results in more accurate and detailed segmentation of objects in the image.,1906.05896v4-Figure7-1.png, Comparison for w/o (left) or w/ (right) intraclass capability enabled. Best viewed in color.,
1906.05896v4,What is the effect of using the OCFusion method with ResNet-101 backbone on the PQ metric?,The OCFusion method with ResNet-101 backbone improves the PQ metric by 3.2 points.,1906.05896v4-Table1-1.png, Comparison to our implementation of Panoptic FPN [16] baseline model on the MS-COCO val dataset.,
1906.06196v2,What is the relationship between the number of channels and the number of GFLOPs for the different convolution methods?,"The number of GFLOPs increases as the number of channels increases for all convolution methods. However, the rate of increase is much higher for regular 3D convolutions than for the CP-HO convolution methods.",1906.06196v2-Figure6-1.png," Comparison of the number of Giga-FLOPs between regular 3D convolutions and our proposed method. We consider inputs of size 32 × 32 × 16, and vary the numer of the input and output channels (the x-axis shows input× output channels). Our proposed CP-HO convolution, here for a rank equal to 6 and 3 times the input channels (CP-HOConv-6 and CP-HOConv-3), has significantly less FLOPs than regular convolution (3D-Conv).",
1906.06196v2,Which network has the least number of parameters optimized for video?,Ours [λ = 0.01] and Ours [λ = 0.05],1906.06196v2-Table4-1.png, Number of parameters optimized to train the temporal model,
1906.06196v2,Which method achieved higher accuracy on the LSEMSW database?,"The method labeled ""Ours"" achieved higher accuracy on the LSEMSW database.",1906.06196v2-Table5-1.png, Results on the LSEMSW database,
1906.06196v2,What are the two dimensions that the valence and arousal circumplex uses to represent emotions?,Valence and arousal.,1906.06196v2-Figure7-1.png, The valence and arousal circumplex. This dimensional model of affect covers the continuous range of emotions displayed by human on a daily basis. The images are taken from the AffectNet dataset [30],
1906.06196v2,What is the difference between the static separable (2D) convolution and the spatio-temporal (3D) separable convolution?,"The static separable (2D) convolution operates on a single static image, while the spatio-temporal (3D) separable convolution operates on a sequence of images over time.",1906.06196v2-Figure1-1.png," Overview of our method, here represented for a single channel of a single input. We start by training a 2D CNN with our proposed factorized convolutional block on static images (left). We then apply transduction to extend the model from the static to the spatio-temporal domain (right). The pretrained spatial factors (blue and red) are first kept fixed, before jointly fine-tuning all the parameters once the temporal factors (green) have been trained.",
1906.06196v2,Which network architecture achieved the highest Top-1 accuracy on the 20BN-Jester Dataset?,HO-CP ConvNet-S,1906.06196v2-Table7-1.png, Results on the 20BN-Jester Dataset,
1906.06196v2,What is the difference between the baseline and the proposed method in the 3D convolutional network architecture?,"The only difference between the baseline and the proposed method is the 3D convolutional block used (3D Conv B): for the baseline, a regular 3D conv is used, and for the proposed method, the HO-CP conv-S is used.",1906.06196v2-Figure10-1.png," Architecture of our 3D convolutional network. We employed the same architecture for both our baseline and our approach, where the only difference is the 3D convolutional block used (3D Conv B): for the baseline a regular 3D conv, and for our method, our proposed HO-CP conv-S. Each convolution is followed by a batch-normalisation, non-linearity (ELU) and a max pooling (over 2× 2× 2 non-overlapping regions).",
1906.06196v2,Which network performs best on the AFEW-VA database for Valence?,"The network ""Ours - transduction"" performs best on the AFEW-VA database for Valence.",1906.06196v2-Table3-1.png, Results on the AFEW-VA database,
1906.06196v2,Which network performed the best in terms of valence prediction?,Ours,1906.06196v2-Table1-1.png, Results on the AffectNet dataset,
1906.06196v2,Which network performs the best in terms of valence and arousal?,"Our network with transduction performs the best in terms of valence and arousal, with a PCC of 0.84 and a CCC of 0.75 for valence, and a SAGR of 0.80 and a CCC of 0.52 for arousal.",1906.06196v2-Table2-1.png, Results on the SEWA database,
1906.06196v2,What is the difference between a 1D convolution and a 2D Kruskal convolution?,"A 1D convolution is performed on a single line of data, while a 2D Kruskal convolution is performed on a two-dimensional grid of data.",1906.06196v2-Figure2-1.png, Illustration of a 2D Kruskal convolution.,
1906.06196v2,What is the relationship between the different cubes in the figure?,"The cubes in the figure represent different layers in a convolutional neural network. The first cube represents the input layer, and the last cube represents the output layer. The cubes in between represent the hidden layers. The arrows between the cubes represent the flow of information between the layers.",1906.06196v2-Figure3-1.png, Illustration of a Tucker convolution expressed as a series of small efficient convolutions. Note that this is the approach taken by ResNet for the Bottleneck blocks.,
1906.06196v2,What is the relationship between valence and arousal in the video?,"Valence and arousal are positively correlated. As arousal increases, valence also increases.",1906.06196v2-Figure8-1.png," Evolution of the ground-truth (gt) and predicted (pred) levels of valence and arousal as a function of time, for one of the test videos of the AFEW-VA dataset.",
1906.06196v2,Which network has the lower number of parameters?,Ours.,1906.06196v2-Table6-1.png, Results on the CIFAR-10 dataset,
1906.06196v2,How does the sparsity of the network change as the learning rate increases?,The sparsity of the network increases as the learning rate increases.,1906.06196v2-Figure9-1.png, Sparsity induced by the automatic rank selection at each layer of the network (ResNet-18 backbone).,
1906.06196v2, What is the difference between a MobileNet block and a CP convolution? ," A MobileNet block is a special case of CP convolution, but it does not have the first convolution and its spatial factors are combined into one. ",1906.06196v2-Figure4-1.png," MobileNet blocks are a special case of CP convolutions, without the first convolution, and with spatial factors are combined into one.",
1906.06196v2,How does MobileNet-v2 use CP convolutions?,"MobileNet-v2 blocks are a special case of CP convolutions, with the spatial factors merged into a depthwise separable convolution.",1906.06196v2-Figure5-1.png," MobileNet-v2 blocks are a special case of CP convolutions, with the spatial factors merged into a depthwise separable convolution.",
1906.07701v1,Which model performed the best on the test set 2016 according to the METEOR metric?,del+obj,1906.07701v1-Table2-1.png," Results for the test sets 2016 and 2018. M denotes METEOR, B – BLEU; * marks statistically significant changes for METEOR (p-value≤ 0.05) as compared to base, † – as compared to del. Bold highlights statistically significant improvements. We report previous state of the art results for multimodal models from (Helcl et al., 2018).",
1906.07701v1,Which of the examples shown in the figure is an example of lexical ambiguity?,The example of lexical ambiguity is shown in (a).,1906.07701v1-Figure1-1.png," Examples of lexical and gender ambiguity, and inaccurate English description where post-edits (PE) required the image to correct human translation from English (EN) to German (DE).",
1906.07701v1,Which model performs the best on average across both languages?,del1,1906.07701v1-Table3-1.png, Human ranking results: normalised rank (micro-averaged). Bold highlights best results.,
1906.07701v1,What are some of the improvements that del and del+obj offer over base+att for translating images?,"del and del+obj offer a number of improvements over base+att for translating images. These include:

* Adding details that are missing in the base+att translation. For example, in the image of the child in the kayak, del and del+obj both add the detail that the child is ""paddling"" the kayak, which is missing in the base+att translation.
* Correcting errors in the base+att translation. For example, in the image of the men working on the car, del+obj correctly translates ""Rennwagen"" as ""race car"", while base+att incorrectly translates it as ""race"".",1906.07701v1-Figure3-1.png, Examples of improvements of del and del+obj over base+att for test set 2016 for French and German. Underlined words represent some of the improvements.,
1906.07701v1,What is the difference between the PERS and AMB examples?,"The PERS example shows how a noun phrase can be translated incorrectly, while the AMB example shows how a verb phrase can be translated incorrectly.",1906.07701v1-Figure7-1.png, Examples of unresolved blanks. The underlined words denote blanked words and their translations.,
1906.07701v1,What is the difference between the base translation and the del+obj translation for the first example?,"The base translation is ""A man and a woman kiss under the woman's gaze."" The del+obj translation is ""A bride and groom kiss under the veil.""",1906.07701v1-Figure5-1.png, Examples of blanks for test set 2016 that were correctly resolved by the textual context. The underlined words denote blanked words and their translations.,
1906.07701v1,Which image caption contains an error that was corrected by del+obj?,"The image caption for the first image contains an error. The base translation incorrectly states that the woman and dog are sitting on a white sofa, but del+obj correctly identifies the object as a bench and translates the caption accordingly.",1906.07701v1-Figure6-1.png, Examples of blanks for test set 2016 that were correctly resolved by the multimodal context. The underlined words denote blanked words and their translations.,
1906.07701v1,Which model performed best on the RND test set in 2018 according to the METEOR metric?,"The ""del+obj"" model performed best on the RND test set in 2018 according to the METEOR metric.",1906.07701v1-Table4-1.png," Results for the test sets 2016 and 2018 for the three degradation configurations: RND, AMB and PERS. M denotes METEOR, B – BLEU; * marks statistically significant changes as computed for METEOR (p-value ≤ 0.05) as compared to base, † – as compared to del. Bold highlights statistically significant improvements over base.",
1906.07701v1,Which system resolved the most blanks in the PERS setup?,del1+obj,1906.07701v1-Table5-1.png," Results of human annotation of blanked translations (English-German). We report counts of blanks resolved by each system, as well as total source blank count for each selection (50 sentences selected randomly).",
1906.07701v1,What is the purpose of the deliberation decoder block?,The deliberation decoder block is used to refine the output of the first-pass decoder by conditioning it on the source and samples output from the first-pass decoder.,1906.07701v1-Figure2-1.png," Our deliberation architecture: The secondpass decoder is conditioned on the source and samples output from the first-pass decoder. The second-pass decoder has access to (a) the object based features represented by embeddings, or (b) spacial image features.",
1906.07701v1,Which of the examples shows how the multimodal context can be used to resolve a blank?,Example (b) shows how the multimodal context can be used to resolve a blank.,1906.07701v1-Figure4-1.png, Examples of resolved blanks for test set 2016. Underlined text denotes blanked words and their translations. Object field indicates the detected objects.,
1906.07701v1,Which setup has the highest percentage of sentences sent?,RND,1906.07701v1-Table1-1.png, Statistics of datasets after applying source degradation strategies,
1906.08649v1,Which algorithm performed the best on the Cheetah task?,POPLIN-P,1906.08649v1-Table1-1.png," The training time-step varies from 50,000 to 200,000 depending on the difficulty of the tasks. The performance is averaged across four random seeds with a window size of 3000 time-steps at the end of the training.",
1906.08649v1,Which algorithm performs best on the Cheetah environment?,POPLIN-P,1906.08649v1-Figure2-1.png," Performance curves of POPLIN-P, POPLIN-A and other state-of-the-art algorithms on different bench-marking environments. 4 random seeds are run for each environment, and the full figures of all 12 MuJoCo environments are summarized in appendix 8.",
1906.08649v1,What is the range of values tried for the population size hyper-parameter?,100 to 2000.,1906.08649v1-Table4-1.png, Hyper-parameter grid search options for PETS.,
1906.08649v1,What are the different types of training data that were used in the hyper-parameter grid search for POPLIN-A?,Real data and hallucination data.,1906.08649v1-Table5-1.png, Hyper-parameter grid search options for POPLIN-A.,
1906.08649v1,What hyper-parameters were explored in the POPLIN-P model training process?,"The hyper-parameters explored in the POPLIN-P model training process include: Training Data (real data vs. hallucination data), Training Variant (BC, GAN, Avg), Noise Variant (Uni, Sep), and Initial Distribution Sigma (0.001, 0.003, 0.01, 0.03, 0.1).",1906.08649v1-Table6-1.png," Hyper-parameter grid search options for POPLIN-P. We also experiment with using WGAN in [32] to train the policy network, which does not results in good performance and is not put into the article.",
1906.08649v1,How does the reward surface change as the planning iterations progress?,The reward surface becomes smoother and more concentrated around the areas with higher rewards.,1906.08649v1-Figure1-1.png," We transform each planned candidate action trajectory with PCA into a 2D blue scatter. The top and bottom figures are respectively the visualization of PETS [5] and our algorithm. The red area has higher reward. From left to right, we show how candidate trajectories are updated, across different planning iterations within one time-step. As we can see, while both reward surface is not smooth with respect to action trajectory. POPLIN, using policy networks, has much better search efficiency, while PETS is stuck around its initialization. The details are in section 5.3.",
1906.08649v1,Which algorithm performs the best when the population size is 2000?,POPLIN-P,1906.08649v1-Figure4-1.png," The performance of PETS, POPLIN-A, POPLIN-P using different population size of candidates. The variance of the candidates trajectory σ in POPLIN-P is set to 0.1.",
1906.08649v1,What does the color gradient in the image represent?,"The color gradient represents the expected reward, with blue being the lowest and red being the highest.",1906.08649v1-Figure5-1.png, The reward optimization surface in the solution space. The expected reward is higher from color blue to color red. We visualize candidates using different colors as defined in the legend. The full results can be seen in appendix A.7.,
1906.08649v1,Which training scheme for POPLIN-P consistently achieves the best performance across all four tasks?,There is no single training scheme that consistently achieves the best performance across all four tasks.,1906.08649v1-Figure3-1.png," The MPC control and policy control performance of the proposed POPLIN-A, and POPLINP with its three training schemes, which are namely behavior cloning (BC), generative adversarial network training (GAN) and setting parameter average (Avg).",
1906.08649v1,Which algorithm performs the best on the Cheetah environment?,POPLIN-P,1906.08649v1-Figure8-1.png," Full Performance of POPLIN-P, POPLIN-A and other state-of-the-art algorithms on 12 different bench-marking environments. In the figure, we include baselines such as TD3, SAC, PPO, METRPO, PETS, RS and our proposed algorithm.",
1906.08649v1,What is the relationship between the reward surface and the candidate solutions?,"The reward surface shows the expected reward for each possible solution, and the candidate solutions are the points that the algorithm has explored in the solution space.",1906.08649v1-Figure15-1.png, Reward surface in solution space (action space) for POPLIN-A-Init.,
1906.08649v1,What does the color gradient in the reward surface represent?,"The color gradient in the reward surface represents the value of the reward function. Warmer colors indicate higher rewards, while cooler colors indicate lower rewards.",1906.08649v1-Figure14-1.png, Reward surface in solution space (action space) for POPLIN-A-Replan.,
1906.08649v1,How does the reward surface change over iterations of the PETS algorithm?,The reward surface becomes more concentrated around the optimal solution.,1906.08649v1-Figure13-1.png, Reward surface in solution space (action space) for PETS algorithm.,
1906.08649v1,Which algorithm performs the best on the InvertedPendulum environment?,The RS algorithm performs the best on the InvertedPendulum environment.,1906.08649v1-Table3-1.png," Performance of each algorithm on environments based on OpenAI Gym [2] classic control environments. In the table, we record the performance at 50000 time-step.",
1906.08649v1,Which algorithm performs the best on the Ant environment?,PPO,1906.08649v1-Table2-1.png," Performance of each algorithm on environments based on OpenAI Gym [2] MuJoCo[41] environments. In the table, we record the performance at 200,000 time-step.",
1906.08649v1,Which action distribution is the most concentrated?,PETS,1906.08649v1-Figure6-1.png, The action distribution in a episode visualized in the projected 2D PCA space.,
1906.08649v1,How does the performance of POPLIN-A compare to that of POPLIN-P-Avg?,"POPLIN-P-Avg generally performs better than POPLIN-A, achieving higher rewards over the same number of timesteps.",1906.08649v1-Figure7-1.png," The performance of POPLIN-A, POPLIN-P-BC, POPLIN-P-Avg, POPLIN-P-GAN using different hyper-parameters.",
1906.08649v1,Which of the three surfaces is the smoothest?,The POLINA-L-0 surface.,1906.08649v1-Figure18-1.png," The color indicates the expected cost (negative of expected reward). We emphasis that all these figures are visualized in the action space. And all of them are very unsmooth. For the figures visualized in solution space, we refer to Figure 13.",
1906.08649v1," 

What is the relationship between the reward surface and the candidate solutions? "," 

The reward surface represents the quality of the candidate solutions. The higher the value on the reward surface, the better the solution. The candidate solutions are plotted on top of the reward surface, showing how they are distributed in the solution space. ",1906.08649v1-Figure16-1.png, Reward surface in solution space (parameter space) for POPLIN-P with 0 hidden layer.,
1906.08649v1,What does the color gradient in the reward surface represent?,"The color gradient in the reward surface represents the magnitude of the reward. Warmer colors indicate higher rewards, while cooler colors indicate lower rewards.",1906.08649v1-Figure17-1.png, Reward surface in solution space (parameter space) for POPLIN-P using 1 hidden layer.,
1906.08649v1,Which algorithm performs the best according to the figure?,POPLIN-P-ZeroWeight.,1906.08649v1-Figure12-1.png," The performance of PETS, POPLIN-A, POPLIN-P-Avg, POPLIN-P-BC and POPLINP whose network has fixed parameters of zeros. The variance of the candidates trajectory σ in POPLIN-P is set to 0.1.",
1906.08649v1,How does the performance of POPLIN-A compare to POPLIN-P and PETS across different random seeds?,POPLIN-A generally performs better than POPLIN-P and PETS across different random seeds.,1906.08649v1-Figure11-1.png," The performance of POPLIN-A, POPLIN-P, and PETS of different random seeds.",
1906.08649v1,What happens to the planned trajectories of PETS as the iterations progress?,The planned trajectories of PETS become more concentrated in the center of the image as the iterations progress.,1906.08649v1-Figure19-1.png, The figures are the planned trajectories of PETS.,
1906.08649v1,What is the relationship between the number of iterations and the complexity of the planned trajectories?,The complexity of the planned trajectories increases with the number of iterations.,1906.08649v1-Figure20-1.png, The figures are the planned trajectories of POPLIN-P using 1 hidden layer MLP.,
1906.08649v1,What is the effect of increasing the number of iterations on the planned trajectories of POPLIN-P?,Increasing the number of iterations causes the planned trajectories to become more spread out and to cover a larger area.,1906.08649v1-Figure21-1.png, The figures are the planned trajectories of POPLIN-P using 0 hidden layer MLP.,
1906.08649v1, Which algorithm achieved the highest reward in the Cheetah environment? , POPLIN-P-Avg-Test. ,1906.08649v1-Figure9-1.png," The planning performance and the testing performance of the proposed POPLIN-A, and POPLIN-P with its three training schemes, which are namely behavior cloning (BC), generative adversarial network training (GAN) and setting parameter average (Avg).",
1906.08649v1,Which algorithm and hyper-parameter combination performed the best in terms of reward?,POPLIN-P-GAN with Sep-Hyper3-Test.,1906.08649v1-Figure10-1.png," The performance of POPLIN-A, POPLIN-P-BC, POPLIN-P-Avg, POPLIN-P-GAN using different hyper-parameters.",
1906.10771v1,How does the performance of the proposed method compare to the other methods?,The proposed method achieves the lowest Top-1 error rate for a given number of GFLOPs and parameters.,1906.10771v1-Figure1-1.png, Pruning ResNets on the ImageNet dataset. The proposed method is highlighted in gray. Bottom-left is better.,
1906.10771v1,Which pruning method results in the lowest loss after pruning 5000 neurons?,Oracle - fixed (2010),1906.10771v1-Figure4-1.png, Pruning ResNet-18 trained on CIFAR-10 without finetuning. The number of neurons pruned when the loss reaches 0.5 is shown in parentheses.,
1906.10771v1,Which method has the highest Spearman correlation with the Oracle on conv2 of the Residual block?,Taylor SO,1906.10771v1-Table1-1.png," Spearman correlation of different criteria with the Oracle on CIFAR-10 with ResNet-18. (FG denotes full gradient, as described in the text).",
1906.10771v1,Which pruning strategy achieves the highest accuracy on CIFAR10 with ResNet20 when pruning 70% of the neurons?,BN-ISTA,1906.10771v1-Table4-1.png, Pruning results on ResNet20 for CIFAR10. Only the first layer in every residual block is pruned. Results are averaged over 10 seeds.,
1906.10771v1,What is the relationship between the number of parameters and the inference time of the pruned ResNet-101 models?,The inference time of the pruned ResNet-101 models generally decreases as the number of parameters decreases.,1906.10771v1-Table5-1.png," Batch inference time of models obtained by pruning ResNet-101, time is measured on NVIDIA Tesla V100 in ms with different batch sizes.",
1906.10771v1,Which criterion has the highest correlation with the oracle for ResNet-101?,"The criterion with the highest correlation with the oracle for ResNet-101 is ""Gate after BN - FG"" with a Pearson correlation coefficient of 0.946.",1906.10771v1-Table2-1.png," Correlation study of different criteria and oracle on the ImageNet dataset. Spearman and Kendall measure rank correlations. BN stands for batch-normalization, FG for full gradient.",
1906.10771v1,Which pruning setting achieved the highest accuracy while also reducing the number of parameters the most?,The iterative pruning setting achieved the highest accuracy while also reducing the number of parameters the most.,1906.10771v1-Figure6-1.png, Pruning ResNet-101 on Imagenet with 3 different settings.,
1906.10771v1,Which layer of the ResNet-101 network exhibits the most significant decrease in the variability of neuron ranks after pruning?,Layer 51,1906.10771v1-Figure5-1.png," Statistics in boxplot form of per-layer ranks before (top) and after (bottom) pruning ResNet-101 with Taylor-FO-BN-50%. First 4 layers correspond to skip connections, the rest are residual blocks represented by the first 2 convolutional layers per block. We can notice that after pruning most of neurons become more equal than before pruning.",
1906.10771v1,Which pruning method resulted in the lowest error rate for ResNet-101?,Taylor-FO-BN-75% (Ours),1906.10771v1-Table3-1.png, Pruning results on ImageNet (1-crop validation errors).,
1906.10771v1,Which pruning method has the best performance on CIFAR-10?,The Combinatorial oracle pruning method has the best performance on CIFAR-10.,1906.10771v1-Figure2-1.png, Pruning the first layer of LeNet3 on CIFAR-10 with Combinatorial oracle and Greedy oracle. Networks remain fixed and are not fine-tuned. Results for Greedy oracle are averaged over 30 seeds with mean and standard deviation shown. Best observed results for Combinatorial oracle for every seed are averaged.,
1906.10771v1,Which pruning criteria resulted in the smallest loss for a fixed number of pruned neurons?,Oracle FO on gate.,1906.10771v1-Figure3-1.png, Pruning LeNet3 on CIFAR-10 with various criteria. Network remains fixed and is not fine-tuned. Results are averaged over 50 seeds with mean and standard deviation. The number of pruned neurons when the loss reaches 1.0 is shown in parentheses.,
1907.02684v4,What are the two main differences between the constituent and dependency trees in this image?,"The constituent tree shows the hierarchical structure of the sentence, while the dependency tree shows the relationships between the words. Additionally, the constituent tree is a binary tree, while the dependency tree is not.",1907.02684v4-Figure1-1.png," Constituent, dependency and HPSG trees.",
1907.02684v4,What is the difference between the LOCAL and NONLOCAL features of the HPSG sign?,"The LOCAL features are specific to the individual word, while the NONLOCAL features are related to the word's relationships with other words in the sentence.",1907.02684v4-Figure2-1.png," HPSG sign from (Miyao et al., 2004).",
1907.02684v4,"Which parsing method performs better overall, constituent or dependency?","It depends on the value of λ. For λ values below 0.4, constituent parsing performs better. For λ values above 0.4, dependency parsing performs better.",1907.02684v4-Figure5-1.png, Balancing constituent and dependency of joint span HPSG parsing on English dev set.,
1907.02684v4,Which model performs better with 12 self-attention layers?,"The Joint Span Model performs better with 12 self-attention layers, achieving an F1 score of 93.78, compared to the Division Span Model's F1 score of 93.57.",1907.02684v4-Table1-1.png, Different self-attention layers on English dev set.,
1907.02684v4,Which model performs the best on the English dev set in terms of F1 score?,The converted dependency model with a joint span λ = 0.5.,1907.02684v4-Table2-1.png, English dev set performance of joint span HPSG parsing. The converted means the corresponding dependency parsing results are from the corresponding constituent parse tree using head rules.,
1907.02684v4,What is the difference between the division span structure and the joint span structure?,The division span structure adds token H in front of the category to distinguish whether the phrase is on the left or right of the head. Thus the head is the last one of the category with H which is marked with a box. Joint span structure contains constitute phrase and dependency arc.,1907.02684v4-Figure3-1.png," Constituent, dependency and two different simplified HPSG structures of the same sentence which is indexed from 1 to 9 and assigned interval range for each node. Dotted box represents the same part. The special category # is assigned to divide the phrase with multiple heads. Division span structure adds token H in front of the category to distinguish whether the phrase is on the left or right of the head. Thus the head is the last one of the category with H which is marked with a box. Joint span structure contains constitute phrase and dependency arc. Categ in each node represents the category of each constituent and HEAD indicates the head word.",
1907.02684v4,Which model achieved the highest LAS score on the Chinese CTB test set?,Our (Joint) + XLNet,1907.02684v4-Table4-1.png, Dependency parsing on PTB and CTB test set. * represents CTB constituent data splitting. Pointer Networks represents the results of Fernández-González and Gómez-Rodrı́guez (2019).,
1907.02684v4,Which model is the fastest according to the table?,Our (Joint),1907.02684v4-Table3-1.png, Parsing speed on the PTB dataset.,
1907.02684v4,What is the role of the scoring layer in the joint span HPSG parsing model?,The scoring layer is responsible for calculating the dependency and span scores.,1907.02684v4-Figure4-1.png, The framework of our joint span HPSG parsing model.,
1907.02684v4,"Which model performs best on the PTB test set for constituent parsing, according to the table?","The model that performs best on the PTB test set for constituent parsing, according to the table, is ""Our (Joint) + XLNet"". This model achieves an F1 score of 96.33, which is higher than any other model listed in the table.",1907.02684v4-Table5-1.png, Constituent parsing on PTB test set.,
1907.02684v4,Which model performed the best on the LR metric?,"Our (Joint) + RoBERTa model performed the best on the LR metric, with an accuracy of 92.61.",1907.02684v4-Table6-1.png, Constituent parsing on CTB test set. * represents CTB dependency data splitting.,
1907.05012v2,Which dataset shows the most consistent performance across different values of epsilon?,Gaussian.,1907.05012v2-Figure4-1.png, Loss Ratio vs. w for DC-k-means on 6 datasets,
1907.05012v2,Which dataset shows the highest loss ratio for Q-k-means?,The Celltype dataset shows the highest loss ratio for Q-k-means.,1907.05012v2-Figure3-1.png, Loss Ratio vs. ε for Q-k-means on 6 datasets,
1907.05012v2,Which dataset has the highest number of retrain occurrences?,MNIST,1907.05012v2-Figure2-1.png, Average retrain occurrences during deletion stream for Q-k-means,
1907.05012v2,Which dataset shows the most improvement when using DC-k-means compared to k-means?,The Gaussian dataset shows the most improvement when using DC-k-means compared to k-means.,1907.05012v2-Table3-1.png, Normalized Mutual Information (higher is better),
1907.05012v2,Which clustering algorithm has the lowest loss ratio for the Gaussian dataset?,DC-k-means,1907.05012v2-Table1-1.png, Loss Ratio,
1907.05012v2,Which algorithm performed the best on the Gaussian dataset?,DC-k-means,1907.05012v2-Table2-1.png, Silhouette Coefficients (higher is better),
1907.05012v2,How does the amortized runtime of Q-k-means change as the quantization granularity increases?,The amortized runtime decreases as the quantization granularity increases.,1907.05012v2-Figure5-1.png, Amortized runtime (seconds) for Q-k-means as a function of quantization granularity on Covtype,
1907.05012v2,How does the amortized runtime of DC-k-means vary with the tree width on Covtype?,"The amortized runtime of DC-k-means generally decreases as the tree width increases. However, there is a slight increase in runtime when the tree width is 2^7.",1907.05012v2-Figure6-1.png, Amortized runtime (seconds) for DC-k-means as a function of tree width on Covtype,
1908.02646v1,Which market had the highest average percentage return?,AS,1908.02646v1-Table2-1.png, Performance comparison on Chinese markets.,
1908.02646v1,What is the relationship between the price rising and the winner score?,"The price rising has a positive relationship with the winner score. This means that as the price of a stock rises, the winner score also increases.",1908.02646v1-Figure3-1.png, Influence of history trading features to winner scores.,
1908.02646v1,Which market has the highest CR?,AS,1908.02646v1-Table1-1.png, Performance comparison on U.S. markets.,
1908.02646v1,Which investment strategy performed the best overall from 1990 to 2015?,"The AS strategy performed the best overall, as it consistently outperformed the other strategies and the market throughout the entire period.",1908.02646v1-Figure2-1.png, The Cumulative Wealth in U.S. markets.,
1908.02646v1,What is the role of the CAAN in the AlphaStock model?,The CAAN takes the outputs of the LSTM-HA modules and generates a portfolio.,1908.02646v1-Figure1-1.png, The framework of the AlphaStock model.,
1908.04598v2,What are the challenges that state-of-the-art visual localization methods face when dealing with indoor environments?,"Indoor environments often have severe occlusion and weak textures, which can make it difficult for visual localization methods to accurately identify the camera pose.",1908.04598v2-Figure1-1.png," Using further modalities for indoor visual localization. Given a set of camera pose estimates for a query image (a, g), we seek to identify the most accurate estimate. (b, h) Due to severe occlusion and weak textures, a stateof-the-art method [72] fails to identify the correct camera pose. To overcome those difficulties, we use several modalities along with visual appearance: (top) surface normals and (bottom) semantics. (c, i) Our approach verifies the estimated pose by comparing the semantics and surface normals extracted from the query (d, j) and database (f, l).",
1908.04598v2,What is the role of the feature extraction network F in the Trainable Pose Verification model?,"The feature extraction network F is responsible for extracting dense descriptors from the input images. These descriptors are then used to compute the descriptor similarity map SD, which is ultimately used to generate the score s of the Trainable Pose Verification model.",1908.04598v2-Figure3-1.png, Network architecture for Trainable Pose Verification. Input images are passed through a feature extraction network F to obtain dense descriptors f . These are then combined by computing the descriptor similarity map SD. Finally a score regression CNN R produces the score s of the trainable pose verification model.,
1908.04598v2,Which method performs the best at localizing queries within a distance threshold of 1 meter?,DensePNV+S with scan graph.,1908.04598v2-Figure4-1.png, The impact of geometric and semantic information on the pose verification stage. We validate the performance of the proposed methods that consider additional geometric and semantic information on the InLoc dataset [72]. Each curve shows the percentage of the queries localized within varying distance thresholds (x–axis) and a fixed rotational error of at most 10 degrees.,
1908.04598v2,Which method performed the best on the InLoc dataset with and without the scan-graph?,DensePV + S performed the best on the InLoc dataset both with and without the scan-graph.,1908.04598v2-Table1-1.png, The impact of using the scan-graph for pose verification evaluated on the InLoc dataset [72]. We report the percentage of queries localized within given positional and rotational error bounds.,
1908.04598v2,Which of the following images shows a typical failure case of DensePV with the scan-graph?,(e),1908.04598v2-Figure5-1.png," Typical failure cases of view synthesis using the scan-graph. Top: Synthetic images obtained during DensePV with the scan-graph, affected by (a) misalignment of the 3D scans to the floor plan, (b) sparsity of the 3D scans, and (c) intensity changes. Bottom: A typical failure case of DensePV with the scan-graph: (d) query image, (e) re-rendered query, (f) error map computed with RootSIFT.",
1908.04598v2,What is the relationship between the panoramic scan in (a) and the database image in (b)?,The panoramic scan in (a) and the database image in (b) are linked based on visual overlap. This is indicated by the blue line connecting them in (d).,1908.04598v2-Figure2-1.png," Image-scan-graph for the InLoc dataset [72]. (a) Example RGB-D panoramic scan. (b) Neighboring database image. (c) 3D points of the RGB-D panoramic scan projected onto the view of the database image. (d) Red dots show where RGB-D panoramic scans are captured. Blue lines indicate links between panoramic scans and database images, established based on visual overlap.",
1908.06387v1,"What is the difference between the ""Sem. Classes"" image and the ""100 Clusters"" image?","The ""Sem. Classes"" image uses a small set of human-defined semantic classes to segment the image, while the ""100 Clusters"" image uses a neural network to automatically discover a large set of fine-grained clusters.",1908.06387v1-Figure1-1.png," Rather than using a small set of human-defined semantic classes, we train a neural network that automatically discovers a large set of fine-grained clusters. We experimentally show that using a larger number of clusters improves localization performance.",
1908.06387v1,Which class is most likely to be split into multiple clusters when the number of clusters is increased?,Road.,1908.06387v1-Figure3-1.png," Visualization of contingency tables between Cityscapes classes and cluster indices for different number of clusters. The clusters were trained on the CMU Correspondence Dataset using a model pre-trained on semantic segmentation. The colormap goes from dark blue (lowest value) to yellow (highest value). The data used is the 500 images from the Cityscapes validation set. Many of the classes common in the test images such as road, building and vegetation are split into several clusters.",
1908.06387v1,Which method performs best in the Suburban setting according to the table?,"PFSL (FGSM, 200 clusters, trained on CMU)",1908.06387v1-Table3-1.png, Comparison to state-of-the-art methods on the Extended CMU Seasons dataset. Best results for single-shot image localization and sequential localization are marked separately.,
1908.06387v1,Which training configuration achieved the highest localization performance on the RobotCar Seasons dataset in the all-night setting?,"The training configuration with 200 clusters, trained on CS+V+E data, with the correspondence loss active, and with segmentation pretraining achieved the highest localization performance on the RobotCar Seasons dataset in the all-night setting, with a recall of 93.5% at 0.25 meters, 99.7% at 0.5 meters, and 99.7% at 5 meters.",1908.06387v1-Table2-1.png," Localization performance for the SSMC method with different segmentation networks on the Extended CMU Seasons dataset and the RobotCar dataset. The first column marks entries from this paper, for the entry marked with * clustering was not repeated during training. Column two indicates the number of clusters (or classes) output by the network. Note that for entries marked with 19 and 66 use the semantic classes of Cityscapes and Vistas respectively and were trained with the method presented in [42]. Column three details what datasets were used during training: CS (Cityscapes), V (Vistas), E (Extra i.e. CMU for CMU results and RobotCar for RobotCar results), O (Other extra i.e. RobotCar for CMU results and CMU for Robotcar Results). The fourth column indicates, with a X, if the correspondence loss was active during training while column five specifies the pretraining of the network (Seg for segmentation pretraining and Class for classification pretraining).",
1908.06387v1,How does the performance of P3P RANSAC compare to that of FGSN with different numbers of clusters on the Extended CMU dataset?,P3P RANSAC performs worse than all of the FGSN models on the Extended CMU dataset.,1908.06387v1-Figure4-1.png," Inlier count and inlier ratio on the Extended CMU dataset (above) and the RobotCar dataset (below) using SSMC. FGSNs with varying amount of clusters are evaluated against two baselines. For the for the ”19 classes” [42], the Cityscapes classes are used for match consistency, while for the ”P3P RANSAC” no filtering is done. Ideal curves are flat for a small number of inliers / inlier ratio and the quickly grow for a larger number of inliers / inlier ratio.",
1908.06387v1,What is the purpose of using k-means clustering in the label creation process?,K-means clustering is used to group similar features together and assign them to the same label. This helps to create a set of discrete labels that can be used to train the FGSN.,1908.06387v1-Figure2-1.png," Illustration of the training procedure of an FGSN. To create the training data, features are extracted from all reference images from the correspondence dataset. The features are then clustered using k-means clustering and the assignments are used as labels for the images. In addition to having dense labels for the reference images, we also use the 2D-2D correspondences during training to encourage consistency across weather conditions and seasons as well as varying viewpoints.",
1908.06387v1,"What is the effect of increasing the number of clusters on the normalized mutual information (NMI) for the ""Seg"" network on the Cityscapes dataset?","Increasing the number of clusters generally increases the NMI for the ""Seg"" network on the Cityscapes dataset.",1908.06387v1-Table1-1.png," Measuring the semantic information contained in our clusters. Using models trained on the CMU or RobotCar Correspondence data, we measure the normalized mutual information (in %) between our clusters and the 19 Cityscapes classes on the Cityscapes (CS) and the WildDash (WD) validation sets. “Seg” networks are pre-trained on semantic segmentation and “Class” networks on classification.",
1908.06605v2,Which category of clothing has the most instances in the dataset?,Tops,1908.06605v2-Table1-1.png, Detailed statistics of our dataset. # Attr. / # Val.: the total number of attributes / attribute values.,
1908.06605v2,What is the average number of input pairs in the dataset?,7.5,1908.06605v2-Table2-1.png, General statistics of our dataset. We counted the size of vocabulary after removing brand names.,
1908.06605v2,Which attribute of the dress is most frequently mentioned by the models?,The figure-flattering design.,1908.06605v2-Figure5-1.png, Generated advertising texts from different models. Attribute values are colored in red. Repeated expressions are underlined.,
1908.06605v2,What are the four groups of attributes that PHVM uses to generate a description of the skirt?,"The four groups of attributes are:
* Group 1: Material and Length (Cotton, Skirt)
* Group 2: Style and Pattern (Fresh, Plaid)
* Group 3: Design and Waist (Figure Flattering, High-rise)
* Group 4: Style and Element (Youthful, Irregular)",1908.06605v2-Figure1-1.png," Generation process of PHVM. After encoding a list of input attribute-value pairs, PHVM first conducts planning by generating a sequence of groups, each of which is a subset of input items. Each sentence is then realized conditioned on the corresponding group and its previous generated sentences.",
1908.06605v2,Which model has the highest BLEU score?,Checklist.,1908.06605v2-Table3-1.png," Automatic evaluation for advertising text generation. We applied bootstrap resampling (Koehn, 2004) for significance test. Scores that are significantly worse than the best results (in bold) are marked with ** for p-value < 0.01.",
1908.06605v2,Which dataset has the highest average length and distinct-4 score?,Ours,1908.06605v2-Table6-1.png," Statistics of E2E, WebNLG, WEATHERGOV (WG), WIKIBIO (WB), ROTOWIRE (RW) and our dataset. We computed distinct-4 (see section 5.3) on 3,000 randomly sampled advertising texts for our dataset and on samples with comparable number of words for E2E, WebNLG, WB and RW, respectively.",
1908.06605v2,What is the role of the global latent variable zp in the PHVM architecture?,"The global latent variable zp controls the planning process in the PHVM architecture. It is used by the plan decoder to generate a sequence of groups, where each group represents a subset of input items that specifies the content of the sentence to be generated.",1908.06605v2-Figure2-1.png, Architecture of PHVM. The model controls planning with a global latent variable zp. The plan decoder conducts planning by generating a sequence of groups g = g1g2...gT where gt is a subset of input items and specifies the content of sentence st to be generated. The sentence decoder controls the realization of st with a local latent variable zst ; dependencies among zst are explicitly modeled to better capture inter-sentence coherence.,
1908.06605v2,What are the different types of sleeves that are listed in the table?,"The different types of sleeves listed in the table are Lantern Sleeve, Flare Sleeve, Batwing Sleeve.",1908.06605v2-Figure4-1.png," Samples of attributes and attribute values for tops, dress / skirt, and pants.",
1908.06605v2,Which model performed best in terms of coherence according to the human evaluation?,PHVM,1908.06605v2-Table4-1.png," Manual pair-wise evaluation for advertising text generation. We conducted Sign Test for significance test. Scores marked with * mean p-value < 0.05 and ** for p-value < 0.01. κ denotes Fleiss’ kappa, all indicating moderate agreement.",
1908.06605v2,What is the relationship between the number of input pairs and the average number of distinct planning results?,The average number of distinct planning results increases as the number of input pairs increases.,1908.06605v2-Figure3-1.png, Average number of distinct planning results (left) / average score of generation quality (right) when the number of input pairs varies.,
1908.06605v2,"Which model generated the longest recipes, on average?","PHVM generated the longest recipes, with an average length of 70.92 words.",1908.06605v2-Table5-1.png," Automatic evaluation for recipe text generation. Checklist was trained with its own source code. We also re-printed results from (Kiddon et al., 2016) (i.e., Checklist §). We applied bootstrap resampling (Koehn, 2004) for significance test. Scores that are significantly worse than the best results (in bold) are marked with * for p-value < 0.05 or ** for p-value < 0.01.",
1908.06605v2,Which recipe is the most likely to be correct?,The Truth recipe is the most likely to be correct.,1908.06605v2-Figure6-1.png, Generated recipes from different models.,
1908.06989v1,What is the purpose of the joint embedding space?,The joint embedding space allows for the retrieval of CAD models that are similar to a given scan or CAD query.,1908.06989v1-Figure4-1.png," Our CAD model retrieval results, visualizing the top 4 retrieved models using our joint embedding space for various scan and CAD queries. Our feature space learns to mix together scan and CAD objects in a semantically meaningful fashion.",
1908.06989v1,Which method achieved the highest instance-level accuracy for k=50?,Ours.,1908.06989v1-Table7-1.png," Evaluation of the joint scan-CAD embedding space. We compare our learned scan-CAD feature space to those constructed from features computed through both handcrafted and learned shape descriptors. We evaluate the confusion between scan and CAD, where 0.5 reflects a perfect confusion.",
1908.06989v1,Which method has the highest retrieval accuracy for chairs?,Ours.,1908.06989v1-Table8-1.png, Evaluation of CAD model retrieval by top-1 retrieval accuracy on the test split of our Scan-CAD Object Similarity benchmark.,
1908.06989v1,Which method achieved the best instance average score on the Scan-CAD Object Similarity benchmark?,Ours (w/o end-to-end),1908.06989v1-Table9-1.png, Evaluation of CAD model retrieval by ranking quality on the test split of our Scan-CAD Object Similarity benchmark.,
1908.06989v1,Which method performs better in terms of retrieval accuracy on the Scan-CAD Object Similarity benchmark?,"""Ours with rotations"" performs better than ""Ref Method"" in terms of retrieval accuracy on the Scan-CAD Object Similarity benchmark.",1908.06989v1-Table6-1.png, Evaluation of CAD model retrieval by retrieval accuracy on our Scan-CAD Object Similarity benchmark.,
1908.06989v1,Which of the methods shown in the figure retrieves the most accurate CAD models for the guitar scan query?,Our approach retrieves the most accurate CAD model for the guitar scan query.,1908.06989v1-Figure5-1.png," CAD model retrieval results (top-1) for various scan queries (from left to right: piano, table, guitar, trash bin, bed, lamp, dresser). Our approach to a joint embedding of scan and CAD can retrieve similar models at a finer-grained level than state-of-the-art handcrafted (FPFH [28], SHOT [30]) and learned (PointNet [26], 3DCNN [27]) 3D object descriptors.",
1908.06989v1,Which method performs the best according to the table?,"The method labeled ""Ours"" performs the best according to the table.",1908.06989v1-Table3-1.png, Evaluation of CAD model retrieval by Top-1 and Top-5 using category-based evaluation of retrieval accuracy.,
1908.06989v1,Which method achieves the highest IoU score?,"The method that achieves the highest IoU score is ""Segmentation(Ours) + Completion(Ours)"".",1908.06989v1-Table5-1.png," Evaluation (IoU) of our segmentation and completion to SGPN [31] and 3D-EPN [10], respectively.",
1908.06989v1, What is the purpose of the first hourglass module in the network architecture?, The purpose of the first hourglass module is to segment the scan object from its background clutter. ,1908.06989v1-Figure2-1.png," Our network architecture to construct a joint embedding between scan and CAD object geometry. The architecture is designed in a stacked hourglass fashion, with a series of hourglass encoder-decoders to transform a scan input to a more CAD-like representation, before mapping the features into an embedding space with a triplet loss. The first hourglass (blue) segments a scan object from its background clutter, the second hourglass (green) predicts the complete geometry for the segmented object, from which the final feature encoding is computed (yellow); CAD object features are computed with the same final encoder. Note that layers are denoted with parameters c× (k, s, p) with number of output channels c, kernel size k, stride s, and padding p. Lighter colored layers denote residual blocks, darker colored layers denote a convolutional layer.",
1908.06989v1,Which method performed the best on average across all classes?,Ours (w/o end-to-end),1908.06989v1-Table4-1.png, Ranking quality of CAD model retrieval on the test split of our Scan-CAD Object Similarity benchmark.,
1908.06989v1,Which method is most successful in learning an embedding space where scan and CAD objects mix together but remain semantically structured?,Our approach.,1908.06989v1-Figure6-1.png," Comparison of latent spaces visualized by t-SNE. Filled triangles represent scan objects, circles represent CAD models. While FPFH, SHOT, and PointNet result in almost entirely disjoint clusters, 3DCNN is able to co-locate the classes of both domains next to each other, but does not confuse them. Our approach learns an embedding space where scan and CAD objects mix together but remain semantically structured.",
1908.06989v1,How many CAD models are proposed to the user in this annotation interface?,Six,1908.06989v1-Figure3-1.png, Annotation interface for obtaining ranked similarity of CAD models to a scan query. A user selects and ranks up to 3 CAD models from a pool of 6 proposed models.,
1908.06989v1,Which method achieved the highest confusion between scan and CAD on average?,Ours,1908.06989v1-Table1-1.png," Evaluation of the joint scan-CAD embedding space. We compare our learned scan-CAD feature space to those constructed from features computed through both handcrafted and learned shape descriptors. We evaluate the confusion between scan and CAD, where 0.5 reflects a perfect confusion.",
1908.06989v1,"Which method achieves the highest average accuracy for retrieving instances of the ""display"" object?",Ours (w/o end-to-end),1908.06989v1-Table2-1.png, Top-1 retrieval accuracy for CAD model retrieval on the test split of our Scan-CAD Object Similarity benchmark.,
1908.07442v5,Which model performed the best on the Sarcos dataset?,TabNet-L,1908.07442v5-Table4-1.png, Performance on Sarcos dataset. Three TabNet models of different sizes are considered.,
1908.07442v5,Which model performed the best on the Poker Hand induction dataset?,The Rule-based model.,1908.07442v5-Table3-1.png, Performance for Poker Hand induction dataset.,
1908.07442v5,Which model performed best on Syn5 and Syn6?,TabNet,1908.07442v5-Table1-1.png," Mean and std. of test area under the receiving operating characteristic curve (AUC) on 6 synthetic datasets from (Chen et al. 2018), for TabNet vs. other feature selection-based DNN models: No sel.: using all features without any feature selection, Global: using only globally-salient features, Tree Ensembles (Geurts, Ernst, and Wehenkel 2006), Lasso-regularized model, L2X (Chen et al. 2018) and INVASE (Yoon, Jordon, and van der Schaar 2019). Bold numbers denote the best for each dataset.",
1908.07442v5,Which model performed the best on the Forest Cover Type dataset?,TabNet,1908.07442v5-Table2-1.png, Performance for Forest Cover Type dataset.,
1908.07442v5,What is the effect of increasing the number of decision steps on the accuracy of the TabNet encoder model?,Increasing the number of decision steps first decreases the accuracy of the TabNet encoder model and then increases it.,1908.07442v5-Table11-1.png, Ablation studies for the TabNet encoder model for the forest cover type dataset.,
1908.07442v5,What is the role of feature selection in TabNet?,"Feature selection in TabNet allows the model to focus on the most important features for the task at hand, which improves interpretability and learning.",1908.07442v5-Figure1-1.png," TabNet’s sparse feature selection exemplified for Adult Census Income prediction (Dua and Graff 2017). Sparse feature selection enables interpretability and better learning as the capacity is used for the most salient features. TabNet employs multiple decision blocks that focus on processing a subset of input features for reasoning. Two decision blocks shown as examples process features that are related to professional occupation and investments, respectively, in order to predict the income level.",
1908.07442v5,How does the unsupervised pre-training help the supervised fine-tuning task?,"The unsupervised pre-training helps the supervised fine-tuning task by learning representations of the data that capture the interdependencies between the feature columns. This allows the encoder model to learn more robust and informative representations of the data, which can then be used to improve the performance of the supervised learning task.",1908.07442v5-Figure2-1.png," Self-supervised tabular learning. Real-world tabular datasets have interdependent feature columns, e.g., the education level can be guessed from the occupation, or the gender can be guessed from the relationship. Unsupervised representation learning by masked self-supervised learning results in an improved encoder model for the supervised learning task.",
1908.07442v5,Does pre-training help improve the accuracy of the Tabnet-M model on the Higgs dataset?,Yes.,1908.07442v5-Table7-1.png," Mean and std. of accuracy (over 15 runs) on Higgs with Tabnet-M model, varying the size of the training dataset for supervised fine-tuning.",
1908.07442v5,Which model performed the best on the Higgs Boson dataset in terms of accuracy?,The Sparse evolutionary MLP model performed the best with an accuracy of 78.47%.,1908.07442v5-Table5-1.png, Performance on Higgs Boson dataset. Two TabNet models are denoted with -S and -M.,
1908.07442v5,Which model performed the best on the Rossmann Store Sales dataset?,TabNet,1908.07442v5-Table6-1.png, Performance for Rossmann Store Sales dataset.,
1908.07442v5,What happens to the decision boundary as C1 and C2 get larger?,The decision boundary gets sharper.,1908.07442v5-Figure3-1.png," Illustration of DT-like classification using conventional DNN blocks (left) and the corresponding decision manifold (right). Relevant features are selected by using multiplicative sparse masks on inputs. The selected features are linearly transformed, and after a bias addition (to represent boundaries) ReLU performs region selection by zeroing the regions. Aggregation of multiple regions is based on addition. As C1 and C2 get larger, the decision boundary gets sharper.",
1908.07442v5,What features are used by the model for the Syn2 dataset?,X3-X6,1908.07442v5-Figure5-1.png," Feature importance masks M[i] (that indicate feature selection at ith step) and the aggregate feature importance mask Magg showing the global instance-wise feature selection, on Syn2 and Syn4 (Chen et al. 2018). Brighter colors show a higher value. E.g. for Syn2, only X3-X6 are used.",
1908.07442v5,What is the relationship between age and the other features in the dataset?,"The T-SNE plot shows that age is strongly correlated with the other features in the dataset. This is because the points on the plot are clustered together based on their similarity. The points that are closer together are more similar, while the points that are further apart are less similar. Since the points are colored by age, we can see that the points that are closer together tend to have similar ages. This suggests that age is a good predictor of the other features in the dataset.",1908.07442v5-Figure6-1.png, First two dimensions of the T-SNE of the decision manifold for Adult and the impact of the top feature ‘Age’.,
1908.07442v5,What is the effect of pre-training on the test accuracy of a machine learning model?,Pre-training can significantly improve the test accuracy of a machine learning model.,1908.07442v5-Figure7-1.png, Training curves on Higgs dataset with 10k samples.,
1908.07442v5," 

How does the TabNet encoder select features for each decision step? "," 

The TabNet encoder selects features for each decision step using a feature selection mask. The feature selection mask is generated by the attentive transformer block. ",1908.07442v5-Figure4-1.png," (a) TabNet encoder, composed of a feature transformer, an attentive transformer and feature masking. A split block divides the processed representation to be used by the attentive transformer of the subsequent step as well as for the overall output. For each step, the feature selection mask provides interpretable information about the model’s functionality, and the masks can be aggregated to obtain global feature important attribution. (b) TabNet decoder, composed of a feature transformer block at each step. (c) A feature transformer block example – 4-layer network is shown, where 2 are shared across all decision steps and 2 are decision step-dependent. Each layer is composed of a fully-connected (FC) layer, BN and GLU nonlinearity. (d) An attentive transformer block example – a single layer mapping is modulated with a prior scale information which aggregates how much each feature has been used before the current decision step. sparsemax (Martins and Astudillo 2016) is used for normalization of the coefficients, resulting in sparse selection of the salient features.",
1908.07442v5, Which model performs the best on the Churn dataset?, CatBoost,1908.07442v5-Table8-1.png, Performance on KDD datasets.,
1908.07442v5,Which feature is considered to be the most important by XGBoost for predicting Adult Census Income?,Gender,1908.07442v5-Table9-1.png, Importance ranking of features for Adult Census Income. TabNet yields feature importance rankings consistent with the well-known methods.,
1908.07442v5,What is the effect of pre-training on the test accuracy of a self-supervised tabular learning model?,Pre-training improves the test accuracy of a self-supervised tabular learning model.,1908.07442v5-Table10-1.png," Self-supervised tabular learning results. Mean and std. of accuracy (over 15 runs) on Forest Cover Type, varying the size of the training dataset for supervised fine-tuning.",
1909.00080v2,"Which words in the sentence ""It is just merely very bad"" are most important for determining the sentiment of the sentence?","The words ""bad"" and ""very"" are most important for determining the sentiment of the sentence.",1909.00080v2-Figure6-1.png, Attention weights for some of the sentences from the SST2 dataset,
1909.00080v2,Which model achieved the highest accuracy on the SO dataset?,The SCARNN model achieved the highest accuracy on the SO dataset with a score of 92.4%.,1909.00080v2-Table1-1.png, Accuracy scores in percentage of all models on every dataset,
1909.00080v2,Which model has the most parameters?,C-LSTM,1909.00080v2-Table2-1.png, Number of parameters for each model,
1909.00080v2,Which dataset has the highest accuracy for the 10th max pooling layer?,SO,1909.00080v2-Figure8-1.png," nth Max pooling experiments on RT, IMDB and SO Datasets",
1909.00080v2,What is the effect of window size on accuracy for the three ordering methods?,The accuracy decreases with increasing window size for all three ordering methods.,1909.00080v2-Figure11-1.png, Accuracy (y-axis) percentage on SST2 dataset with varying window size.,
1909.00080v2,What are the most common reasons for misclassified samples in the SST2 dataset?,"The most common reasons for misclassified samples in the SST2 dataset are ""it is also"" and ""rainy days"".",1909.00080v2-Figure9-1.png, Percentage distribution of max pooling outputs for misclassified samples from SST2 dataset,
1909.00080v2,Which ordering method resulted in the highest accuracy?,The Correct Ordering method resulted in the highest accuracy.,1909.00080v2-Figure10-1.png, Accuracy (y-axis) percentage on SO dataset with varying window size.,
1909.00080v2,How does the convolution transformation affect the relationship between words with similar meanings?,The convolution transformation makes words with similar meanings closer together in the embedding space.,1909.00080v2-Figure1-1.png, t-SNE projection of original embeddings and after convolution transformation,
1909.00080v2,Which dataset has the largest number of classes?,TREC,1909.00080v2-Table3-1.png, Summary Statistics of all datasets,
1909.00080v2,What is the difference between the mean and standard deviation of the feature distributions?,"The mean of the feature distributions is relatively stable over time, while the standard deviation is more variable.",1909.00080v2-Figure7-1.png, Statistics of each feature in concatenation layer outputs on the IMDB dataset’s training set.,
1909.00080v2,Which ordering method converges to the highest accuracy?,The correct ordering method converges to the highest accuracy.,1909.00080v2-Figure2-1.png, Accuracy (y-axis) percentage on Rotten Tomatoes dataset with varying window size.,
1909.00080v2,Which dataset achieves higher accuracy with 4th Max Pooling?,TREC,1909.00080v2-Figure3-1.png, Accuracy (y-axis) percentage on TREC and SST2 datasets with varying n for the nth Max pooling.,
1909.00080v2,What is the purpose of the attention mechanism in the SCARN architecture?,The attention mechanism is used to select the most relevant information from the input sequence.,1909.00080v2-Figure4-1.png, SCARN Architecture,
1909.00080v2,What is the purpose of the convolution layer in the concat-SCARN architecture?,The convolution layer is used to extract features from the input data.,1909.00080v2-Figure5-1.png, concat-SCARN architecture,
1909.00175v1, What is the role of the CRF layer in the model architecture?, The CRF layer is responsible for capturing the dependencies between the predicted labels. ,1909.00175v1-Figure1-1.png, Model architecture,
1909.00175v1,Which system performs best in terms of F1 score for homographic image location?,Our - BPA-p system.,1909.00175v1-Table1-1.png," Comparison results on two benchmark datasets. (P.: Precision, R.: Recall, F1: F1 score.)",
1909.02177v4,Why does sentence S2 have a lower matching score than sentence S1?,"Sentence S2 has a lower matching score than sentence S1 because it uses the verb ""created"" instead of ""founded"".",1909.02177v4-Figure1-1.png, Current rule-based methods mostly rely on ex-,
1909.02177v4,What is the difference between Bootstrapping and Data Programming?,Bootstrapping extracts rules from a corpus and then uses those rules to find matches in the corpus. Data Programming uses a model to denoise hard-matched rules before using them to find matches in the corpus.,1909.02177v4-Figure2-1.png, Comparison between previous work and the pro-,
1909.02177v4,"Why are the attention weights for the word ""chairman"" in S1 and S2 different?","The attention weights for the word ""chairman"" are different in S1 and S2 because the context of the word is different in each sentence. In S1, ""chairman"" is used in the context of a former employee, while in S2, ""chairman"" is used in the context of someone who founded an organization.",1909.02177v4-Figure9-1.png, Output visualization of SRM. Left: attention weights of words and the soft matching scores between a rule and three,
1909.02177v4,What is the relationship between the number of annotations and the F1 score?,The F1 score increases as the number of annotations increases.,1909.02177v4-Figure10-1.png, Study on label efficiency. Average number of rules,
1909.02177v4,How does the performance of the three methods compare when the number of rules is 147?,"At 147 rules, NERO performs the best, followed by LSTM + ATT(Sa + P) and then Mean-Teacher.",1909.02177v4-Figure6-1.png, Performance w.r.t. different number of rules and,
1909.02177v4,How does the Soft Rules Matcher (SRM) generate pseudo-labels?,The SRM generates pseudo-labels by using the rules to identify the entities in the sentence and then classifying the relations between them.,1909.02177v4-Figure3-1.png," Overview of the Nero framework. Each unmatched sentence is first annotated by the soft rule matcher (SRM) to generate pseudo labels, and then fed into the relation classifier (RC) to update the model parameters. The whole framework is",
1909.02177v4,Which training objective achieved the highest F1 score?,"The training objective ""L (ours)"" achieved the highest F1 score of 51.3.",1909.02177v4-Table4-1.png, Ablation Study of Different Training Objectives on,
1909.02177v4,Which model achieved the highest F1 score in the ablation study?,Word-level attention,1909.02177v4-Table5-1.png, Ablation study of different soft-matching models,
1909.02177v4,How does changing the value of β affect the F1 score on TACRED?,The F1 score decreases as the value of β increases.,1909.02177v4-Figure8-1.png," Sensitivity analysis of τ , α , β , andγ onTACRED.We",
1909.02177v4,Which method achieved the highest F1 score on the TACRED dataset?,BERT-base (frozen),1909.02177v4-Table3-1.png, Performance on predicting unseen relations. Nero,
1909.02177v4,Which semi-supervised model performs the best when only 10% of the raw corpus is used?,Self-Training,1909.02177v4-Figure7-1.png, Performance of different semi-supervised models,
1909.02177v4, What is the role of the dimension weighting in the SRM architecture?," The dimension weighting helps the SRM to focus on the most important dimensions of the input data. This is done by assigning different weights to different dimensions, which allows the SRM to learn more complex relationships between the input and output data.",1909.02177v4-Figure4-1.png, Detailed architecture of the soft rule matcher (SRM).,
1909.06720v2, What is the relationship between the target regression distribution and the performance of the iterative RPN? , The iterative RPN shows limitations in improving the performance of the RPN because it fails in learning the stage-2 distribution. ,1909.06720v2-Figure1-1.png," Iterative RPN shows limitations in improving RPN performance. (a) The target regression distribution to be learned at stage 1 and 2. The stage 2 distribution represents the error after the stage 1 distribution is learned. (b) Iterative RPN fails in learning stage-2 distribution as the average recall (AR) improvement is marginal compared to the of RPN. (c) In Iterative RPN, the anchor at stage 2, which is regressed in stage 1, breaks the alignment rule in detection.",
1909.06720v2,Which method achieved the highest AR1000 score on the COCO 2017 val dataset?,Cascade RPN,1909.06720v2-Table1-1.png, Region proposal results on COCO 2017 val.,
1909.06720v2,Which proposal method has the highest average precision (AP) across all object sizes?,Cascade RPN with Faster R-CNN.,1909.06720v2-Table2-1.png, Detection results on COCO 2017 test-dev,
1909.06720v2,What is the difference between the region proposal results at stage 1 and stage 2 of Cascade RPN?,The region proposal results at stage 2 are more accurate than the region proposal results at stage 1.,1909.06720v2-Figure4-1.png, Examples of region proposal results at stage 1 (first row) and stage 2 (second row) of Cascade RPN.,
1909.06720v2,How does using both center and shape information affect the performance of the system?,Using both center and shape information improves the performance of the system.,1909.06720v2-Table4-1.png, The effects of alignment,
1909.06720v2,Which configuration results in the highest overall improvement in AR?,"The configuration that uses all of the following: alignments, anchor-free and anchor-based metrics, and regression statistics.",1909.06720v2-Table3-1.png," Ablation analysis of Cascade RPN. Here, Align., AFAB, and Stats. denote the use of alignments, anchor-free and anchor-based metrics, and regression statistics, respectively.",
1909.06720v2, What is the difference between the RPN and the iterative RPN architectures? ," The RPN architecture uses a single head network to predict both the classification and the bounding box regression for each anchor. The iterative RPN architecture uses two head networks, one for each task. This allows the network to learn more complex features for each task. ",1909.06720v2-Figure2-1.png," The architectures of different networks. “I”, “H”, “C”, and “A” denote input image, network head, classifier, and anchor regressor, respectively. “Conv”, “DefConv”, “DilConv” and “AdaConv” indicate conventional convolution, deformable convolution [10], dilated convolution [39] and the proposed adaptive convolution layers, respectively.",
1909.06720v2,What are the green boxes in the image?,The green boxes are the region proposals generated by Cascade RPN. These are the areas in the image where the model thinks there might be an object.,1909.06720v2-Figure5-1.png, More examples of region proposal results of Cascade RPN.,
1909.06720v2,How do the sampling locations in a dilated convolution layer differ from those in a regular convolution layer?,"In a dilated convolution layer, the sampling locations are spaced further apart than in a regular convolution layer. This allows the dilated convolution layer to have a larger receptive field, which can be useful for capturing long-range dependencies in the input data.",1909.06720v2-Figure3-1.png, Illustrations of the sampling locations in different convolutional layers with 3× 3 kernel.,
1909.09060v3,What is the relationship between the attention regions and the generated captions?,"The attention regions highlight the parts of the image that are most relevant to the generated caption. For example, in the first image, the attention regions focus on the fire hydrant, which is the subject of the caption.",1909.09060v3-Figure2-1.png," Qualitative examples for the caption generation process of AAT. We show the attention steps taken at each decoding step, with the visualized attention regions, the confidence and the weights of each attention step (confidence/weight is shown below the attention regions for each step).",
1909.09060v3,How does the performance of the Adaptive model compare to the Base and Recurrent models on the METEOR metric after cross-entropy loss training?,The Adaptive model performs slightly better than the Base and Recurrent models on the METEOR metric after cross-entropy loss training.,1909.09060v3-Table1-1.png," Ablative studies of attention time steps. We show the results of different attention models with different attention time steps, which are reported after the cross-entropy loss training stage and the self-critical loss training stage. We obtain the mean score and the standard deviation of a metric for a model by training it for 3 times, each with a different seed for random parameter initialization.",
1909.09060v3,How does the average number of attention steps change as the value of λ increases?,The average number of attention steps increases as the value of λ increases.,1909.09060v3-Table2-1.png, Tradeoff for time cost penalty. We show the average attention time steps as well as the performance with different values of λ. The results are reported by a single model after self-critical training stage.,
1909.09060v3,"Which method performed the best on the MS-COCO ""Karpathy"" test split when using the Cross-Entropy Loss function?","The AAT method performed the best on the MS-COCO ""Karpathy"" test split when using the Cross-Entropy Loss function.",1909.09060v3-Table4-1.png," Single model performance of other state-of-the-art methods as well as ours on the MS-COCO “Karpathy” test split. For our AAT model, we use multi-head attention [25] with the number of attention heads to be 8 and λ =1e-4.",
1909.09060v3,Which attention type and number of heads resulted in the highest BLEU-4 score?,Dot-product attention with 8 heads.,1909.09060v3-Table3-1.png," Ablation study of attention type and number of attention heads. For all models, we set λ =1e-4. The results are reported by a single model after self-critical training stage.",
1909.09060v3,Which attention model is the most efficient in terms of computational cost?,The base attention model.,1909.09060v3-Figure1-1.png," Different attention models. For each decoding step, (a) base takes one attention step; (b) recurrent takes a fixed MR steps; (c) adaptive takes adaptive steps.",
1909.09389v1,How does the performance of FastText and NaiveBayes compare in terms of intersection of samples obtained with different values of b?,"FastText shows a clear trend of similar samples being selected regardless of sample size, with the intersection of samples comparable to the highest possible. NaiveBayes, on the other hand, does not show a clear trend, with the queried percentage occasionally dropping significantly and occasionally remaining unaffected when increasing iterations.",1909.09389v1-Table5-1.png," Intersection of samples obtained with different values of b. We see the intersection of samples selected with different number of intersections comparable to highest possible (different seeds) in FastText, far higher compared to chance intersection. This indicates similar samples are selected regardless of sample size. NaiveBayes does not show clear trends but occasionally the queried percentage drops significantly when increasing iterations, occasionally it remains unaffected.",
1909.09389v1,Which query strategy is the most robust to deletion?,DelEnt-DelEnt,1909.09389v1-Table6-1.png," Intersection of query strategies across acquisition functions. We observe that the % intersection among samples in the Ent-LC is comparable to those Ent-Ent. Similarly, the Ent-DelEnt (entropy with deletion) is comparable to both DelEnt-DelLC and DelEnt-DelEnt showing robustness of FastText to query functions (beyond minor variation). DelEnt-DelEnt obtains similar intersections as compared to Ent-Ent, showing the robustness of the acquired samples to deletion.",
1909.09389v1,"Based on the table, which strategy has the highest percentage of intersection between the single and ensemble models?","Ent-Ent has the highest percentage of intersection between the single and ensemble models, with a value of 85.8 ± 0.0.",1909.09389v1-Table7-1.png, Intersection of query strategies across single and ensemble of 5FTZ models. We observe that the % intersection of samples selected by ensembles and single models is comparable to intersection among either. The 5-model committee does not seem to add any additional value over selection by a single model.,
1909.09389v1,Which method achieves the highest accuracy with the smallest amount of data?,Uncertainty - 9 iterations.,1909.09389v1-Figure2-1.png," Active text classification: Comparison with K-Center Coreset, BALD and SVM algorithms. Accuracy is plotted against percentage data sampled. We reach full-train accuracy using 12% of the data, compared to BALD which requires 50% data and perform significantly worse in terms of accuracy. We also outperform K-center greedy Coreset at all sampling percentages without utilizing additional diversity-based augmentation.",
1909.09389v1,Which algorithm is more robust to increase in query size?,FastText is more robust to increase in query size.,1909.09389v1-Figure3-1.png," Accuracy across different number of queries b for FastText and Naive Bayes, with b × K constant. FastText is robust to increase in query size and significantly outperforms random in all cases",
1909.09389v1,Which paper used more datasets and models for active text classification?,Our work used more datasets and models for active text classification than DAL.,1909.09389v1-Table1-1.png," Comparison of active text classification datasets and models (Acc on Trec-QA) used in (Siddhant and Lipton, 2018) and our work. We use significantly larger datasets (two orders larger), perform 20x more experiments, and use more efficient and accurate models.",
1909.09389v1,Which dataset has the lowest average label entropy for 39 query iterations?,AMZP,1909.09389v1-Table15-1.png," Class Bias Experiments: Average Label entropy (mean ± std ) across query iterations, for 39, 19 and 4 query iterations each.",
1909.09389v1,Which query strategy performed the best on the AGN dataset with 19 iterations?,"The FTZ Ent-Ent query strategy performed the best on the AGN dataset with 19 iterations, with a mean score of 75.8 ± 0.3.",1909.09389v1-Table14-1.png, Intersection across query strategies using 19 and 9 iterations (mean± std across runs) and different seeds,
1909.09389v1,Which of the sample selection methods in this study yielded the highest accuracy?,"The sample selection method that yielded the highest accuracy was SV Com%, which achieved an accuracy of 79.0% on the Trec-QA dataset.",1909.09389v1-Table8-1.png, Results of sample selection from previous investigations on small datasets (Trec-QA).,
1909.09389v1,Which model performs the best on the AGN dataset?,DPCNN,1909.09389v1-Table9-1.png, Comparison of accuracies with state-of-the-art approaches (earliest-latest) for text classification (%dataset in brackets). We are competitive with state-of-the-art models while using 5x-40x compressed datasets.,
1909.09389v1,What is the speedup achieved by using the Ours-Small dataset compared to the full dataset for the AGN model?,The speedup achieved by using the Ours-Small dataset compared to the full dataset for the AGN model is approximately 25x.,1909.09389v1-Table10-1.png," ULMFiT: Resulting sample Ŝb compared to reported accuracies in (Howard and Ruder, 2018) (%dataset in brackets). We observe that using our cheaply obtained compressed datasets, we can achieve similar accuracies with 25x-200x speedup (5x less epochs, 5x-40x less data). Transferability to other models is evidence of the generalizability of the subset collected using FTZ to other deep models.",
1909.09389v1,Which method has a more stable label entropy across queries?,FastText.,1909.09389v1-Table2-1.png," Label entropy with a large query size (b = 9 queries). ∩Q denotes averaging across queries of a single run, ∩S denotes the label entropy of the final collected samples, averaged across seeds. Naive Bayes (∩Q) has biased (inefficient) queries while FastText (∩Q) shows stable, high label entropy showing a rich diversity in classes despite the large query size. Overall, the resultant sample (∩S) becomes balanced in both cases.",
1909.09389v1,Which algorithm is more robust to an increase in query size?,FastText is more robust to an increase in query size.,1909.09389v1-Figure1-1.png," Accuracy across different number of queries b for FastText and Naive Bayes, with b × K constant. FastText is robust to increase in query size and significantly outperforms random in all cases. Naive Bayes: (Left) All including b=39 perform worse than random, (Center) All including b=9 eventually perform better than random (Right) b = 39 performs better than random but larger query sizes perform worse than random. Uncertainty sampling with Naive Bayes suffers from sampling size bias.",
1909.09389v1,Which method is the most initialization-independent according to the table?,FastText,1909.09389v1-Table4-1.png," % Intersection of samples obtained with different seeds (ModelD) compared to same seeds (ModelS) and chance intersection for b = 39 queries. We see that FastText is initialization independent (FTZD ≈ FTZS Chance). NaiveBayes shows significant dependency on the initial set sometimes, while other times performs comparable to FastText.",
1909.09389v1,Which dataset has the highest proportion of support vectors intersecting with the actively selected set?,DBP,1909.09389v1-Table3-1.png, Proportion of Support Vectors intersecting with our actively selected set calculated by |SSV ∩Ŝb|,
1909.11448v3,How does the source domain change over time?,The source domain does not change over time.,1909.11448v3-Figure1-1.png, Example of a source domain and a sequence of slowlyvarying target domains.,
1909.11448v3,Which method performs the best when the rotation angle is 150 degrees?,"""class reg + time reg""",1909.11448v3-Figure3-1.png, Performance comparison of different continuous domain adaptation strategies with optimal transport. Plot shows average and minimum and maximum values over 10 runs using the best regularization parameters for each of the methods (tuned using grid search on different samples).,
1909.11448v3,What is the difference between the proposed method and the CGS method in terms of convergence speed?,The proposed method converges faster than the CGS method.,1909.11448v3-Figure2-1.png, Objective value versus time. The step size is set to α = 10.,
1909.13789v2,What does the red dashed line in the figure represent?,The red dashed line represents a path on the image manifold induced by Hamiltonian dynamics.,1909.13789v2-Figure1-1.png," The Hamiltonian manifold hypothesis: natural images lie on a low-dimensional manifold in pixel space, and natural image sequences (such as one produced by watching a two-body system, as shown in red) correspond to movement on the manifold according to Hamiltonian dynamics.",
1909.13789v2,What is the relationship between the Hamiltonian and the generated datasets?,The Hamiltonian is a mathematical function that describes the total energy of a system. The generated datasets are samples from the probability distribution defined by the Hamiltonian.,1909.13789v2-Figure5-1.png," Ground truth Hamiltonians and samples from generated datasets for the ideal pendulum, mass-spring, and two- and three-body systems used to train HGN.",
1909.13789v2,What is the role of the decoder in the Hamiltonian Generative Network?,The decoder reconstructs the image from the position q state variables only.,1909.13789v2-Figure2-1.png," Hamiltonian Generative Network schematic. The encoder takes a stacked sequence of images and infers the posterior over the initial state. The state is rolled out using the learnt Hamiltonian. Note that we depict Euler updates of the state for schematic simplicity, while in practice this is done using a leapfrog integrator. For each unroll step we reconstruct the image from the position q state variables only and calculate the reconstruction error.",
1909.13789v2,Which physical system has the highest average pixel MSE?,The Three-Body system has the highest average pixel MSE.,1909.13789v2-Figure6-1.png," Average pixel MSE for each step of a single train and test unroll on four physical systems. All versions of HGN outperform HNN, which often learned to reconstruct an average image. Note the different scales of the plots comparing HGN to HNN and different versions of HGN.",
1909.13789v2,What does the integrated flow plot show?,The integrated flow plot shows the basins of attraction for the learned potential energy function.,1909.13789v2-Figure9-1.png," Multimodal density learning using Hamiltonian flows. From left to right: KDE estimators of the target and learned densities; learned kinetic energy K(p) and potential energy V (q); single leapfrog step and an integrated flow. The potential energy learned multiple attractors, also clearly visible in the integrated flow plot. The basins of attraction are centred at the modes of the data.",
1909.13789v2," 
Which method, RNVP or NHF, converges to a lower negative ELBO faster?"," 
NHF converges to a lower negative ELBO faster than RNVP.",1909.13789v2-Figure10-1.png," Comparison between RNVP (Dinh et al., 2017) and NHF on a Gaussian Mixture Dataset. The RNVPs use alternating masks with two layers (red) or 3 layers (purple). NHF uses 1, 2 or 3 leapfrog steps (blue, yellow and green respectively).",
1909.13789v2,What is the difference between a symplectic and a non-symplectic integrator?,"A symplectic integrator preserves the volume of a region in phase space over the course of integration, while a non-symplectic integrator does not. This can be seen in the figure, where the symplectic integrator keeps the volume of the blue quadrilaterals constant, while the non-symplectic integrator causes the volume to increase with each integration step.",1909.13789v2-Figure13-1.png," A: example of using a symplectic (leapfrog) and a non-symplectic (Euler) integrators on the Hamiltonian of a harmonic oscillator. The blue quadrilaterals depict a volume in phase space over the course of integration. While the symplectic integrator conserves the volume of this region, but the non-symplectic integrator causes it to increase in volume with each integration step. The symplectic integrator clearly introduces less divergence in the phase space than the non-symplectic alternative over the same integration window. B: an illustration of the leapfrog updates in the phase space, where q is position and p is momentum.",
1909.13789v2,"Which model performed the best on the mass-spring system, and what was its average pixel MSE on the test data?","The HGN (LEAPPFROG) model performed the best on the mass-spring system, with an average pixel MSE of 6.23 ± 2.03 on the test data.",1909.13789v2-Table1-1.png," Average pixel MSE over a 30 step unroll on the train and test data on four physical systems. All values are multiplied by 1e+4. We evaluate two versions of the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019): the original architecture and a convolutional version closely matched to the architecture of HGN. We also compare four versions of our proposed Hamiltonian Generative Network (HGN): the full version, a version trained and tested with an Euler rather than a leapfrog integrator, a deterministic rather than a generative version, and a version of HGN with no extra network between the posterior and the initial state.",
1909.13789v2,What is the trend of the average MSE for pixel reconstructions during training of HNN (conv) for all four systems?,The average MSE for pixel reconstructions decreases with increasing training iterations for all four systems.,1909.13789v2-Figure12-1.png, Average MSE for pixel reconstructions during training of HNN (conv). The x axis indicates the training iteration number 1e+2.,
1909.13789v2, What is the difference between the rollouts of the pendulum and the two-body datasets?," The pendulum dataset shows a single object swinging back and forth, while the two-body dataset shows two objects interacting with each other.",1909.13789v2-Figure8-1.png, Examples of sample rollouts for all four datasets from a trained HGN.,
1909.13789v2,How do the rollouts of HGN and HNN differ in the test sequence?,"The rollouts of HGN show the expected motion of the three bodies in the system, while the rollout of HNN shows a blurry image that does not accurately capture the dynamics of the system.",1909.13789v2-Figure7-1.png," Example of a train and a test sequence from the dataset of a three-body system, its inferred forward, backward, double speed and half speed rollouts in time from HGN, and a forward rollout from HNN. HNN did not learn the dynamics of the system and instead learned to reconstruct an average image.",
1909.13789v2,Which model conserves energy the best on the Pendulum system?,HGN (NO fψ),1909.13789v2-Table2-1.png," Variance of the Hamiltonian on four physical systems over single train and test rollouts shown in Fig. 6. The numbers reported are scaled by a factor of 1e+4. High variance indicates that the energy is not conserved by the learned Hamiltonian throughout the rollout. Many HNN Hamiltonians have collapsed to 0, as indicated by N/A. HGN Hamiltonians are meaningful, and different versions of HGN conserve energy to varying degrees.",
1909.13789v2,What is the role of the Hamiltonian in the NHF model?,"The Hamiltonian is used to define the dynamics of the system, which are then used to generate samples from the desired distribution.",1909.13789v2-Figure4-1.png," A schematic representation of NHF which can perform expressive density modelling by using the learned Hamiltonians as normalising flows. Note that we depict Euler updates of the state for schematic simplicity, while in practice this is done using a leapfrog integrator.",
1909.13789v2,What is the difference between standard normalizing flow and Hamiltonian flows?,"Standard normalizing flow uses a neural network to implement the invertible function fi, while Hamiltonian flows use learned Hamiltonian dynamics to transform the initial density.",1909.13789v2-Figure3-1.png," A: standard normalising flow, where the invertible function fi is implemented by a neural network. B: Hamiltonian flows, where the initial density is transformed using the learned Hamiltonian dynamics. Note that we depict Euler updates of the state for schematic simplicity, while in practice this is done using a leapfrog integrator.",
1909.13789v2,Which system shows the most rapid decrease in MSE during training?,The mass-spring system.,1909.13789v2-Figure11-1.png, Average MSE for pixel reconstructions during training of HGN (leapfrog). The x axis indicates the training iteration number 1e+2.,
1910.02190v2,What are the three types of image processing tasks shown in the figure?,"The three types of image processing tasks shown in the figure are color processing, filtering, and geometry processing.",1910.02190v2-Figure1-1.png," The library implements routines for low level image processing tasks using native PyTorch operators and their custom optimization. The purpose of the library is to be used for large-scale vision projects, data augmentation, or for creating computer vision layers inside of neural network layers that allow for backprogating error through them. The above results are obtained from a given batch of images using data parallelism in the GPU.",
1910.02190v2, Which level of the multi-resolution image pyramid provides the most accurate depth estimation? , Level 6,1910.02190v2-Figure4-1.png," Results of the depth estimation by gradient descent showing the depth map produced by the given set of calibrated camera images over different scales. Each column represents a level of a multi-resolution image pyramid. Row 1 to 3: the source images, where the 2nd row is the reference view; Row 3: the images from row 1 and 3 warped to the reference camera given the depth at that particular scale level. Row 4 & 5: the estimated depth map and the error per pixel compared to the ground truth depth map in the reference camera. The data used for these experiments was extracted from SceneNet RGB-D dataset [37], containing photorealistic indoor image trajectories.",
1910.02190v2,"Which computer vision library supports batch processing, differentiable operations, distributed computing, and multi-dimensional arrays?",Kornia and tensorflow.image.,1910.02190v2-Table1-1.png," Comparison of different computer vision libraries by their main features. Kornia and tensorflow.image are the only frameworks that mostly implement their functionalities in the GPU, using batched data, differentiable and have the ability to be distributed.",
1910.02190v2,What is the purpose of the green circles in the third row of the image?,The green circles in the third row of the image represent matching features between the two images that survived RANSAC geometric verification.,1910.02190v2-Figure5-1.png," Targeted adversarial attack on image matching. From top to bottom: original images, which do not match; images, optimized by gradient descent to have local features that match; the result of the attack: matching features (Hessian detector + SIFT descriptor). Matching features, which survived RANSAC geometric verification",
1910.02190v2,Which library is the fastest for computing Sobel edges on a GPU?,Kornia-gpu,1910.02190v2-Figure2-1.png," Left: Python script showing our image processing API. Notice that the API is transparent to the device, and can be easily combined with other PyTorch components. Right: Results of the benchmark comparing Kornia to other state-of-the-art vision libraries. We measure the elapsed time for computing Sobel edges (lower is better).",
1910.02190v2,What is the purpose of using an image pyramid in image registration?,"An image pyramid is used in image registration to improve the convergence of the optimization algorithm. The algorithm starts by registering the images at the lowest level of the pyramid, where the images are small and the features are easy to match. The registration results from the lower level are then used to initialize the registration at the next higher level. This process is repeated until the images are registered at the original resolution.",1910.02190v2-Figure3-1.png, Results of the image registration by gradient descent. Each of the columns represent a different level of the image pyramid used to optimize the loss function. Row 1: the original source image; Row 2: the original destination image; Row 3: the source image warped to destination at the end of the optimization loop at that specific scale level. Row 4: the photometric error between the warped image using the estimated homography and the warped image using the ground truth homography. The algorithm starts to converge in the lower scales refining the solution as it goes to the upper levels of the pyramid.,
1910.06036v2,What is the baseline prediction for the reference question?,"The baseline prediction is ""What is the coldest temperature in Celsius?"".",1910.06036v2-Figure1-1.png, An example SQuAD question with the baseline’s prediction. The answer (“0.3 ◦C”) is highlighted.,
1910.06036v2,Which model performed the best on the METEOR metric?,"Our model performed the best on the METEOR metric, with a score of 44.35.",1910.06036v2-Table4-1.png," The main experimental results for our model and several baselines. ‘-’ means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L)",
1910.06036v2,What is the percentage of non-stop sentence words that appear in the ground truth question and are within a distance of 0 to 10 from the answer fragment?,72.8%.,1910.06036v2-Table1-1.png," Performance for the average relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L)",
1910.06036v2,What was the significance of Beyonce's album release according to the New York Times?,"The New York Times noted the album's unconventional, unexpected release as significant.",1910.06036v2-Figure4-1.png," Example questions (with answers highlighted) generated by crowd-workers (ground truth questions), the baseline model and our model.",
1910.06036v2,How does the performance of Our Model compare to Hybrid on sentences with more than 20 words?,"Our Model outperforms Hybrid on all metrics (BLEU, MET, and R-L) for sentences with more than 20 words.",1910.06036v2-Table5-1.png, Performance for the average relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question (BLEU is the average over BLEU-1 to BLEU-4). Values in parenthesis are the improvement percentage of Our Model over Hybrid. (a) is based on all sentences while (b) only considers long sentences with more than 20 words.,
1910.06036v2,Which of the following is true about the copy ratio of answer-relevant relations?,The copy ratio of answer-relevant relations is higher than the copy ratio of sentences.,1910.06036v2-Table2-1.png, Comparisons between sentences and answerrelevant relations. Overlapped words are those non-stop tokens co-occurring in the source (sentence/relation) and the target question. Copy ratio means the proportion of source tokens that are used in the question.,
1910.06036v2,What is the relationship between Beyonce's album release and the New York Times?,"The New York Times noted the album's unconventional, unexpected release as significant.",1910.06036v2-Figure2-1.png, Examples for n-ary extractions from sentences using OpenIE. Confidence scores are shown at the beginning of each relation. Answers are highlighted in sentences. Waved relations are selected according to our criteria in Section 2.2.,
1910.06036v2,Who announced the Apollo program to industry representatives in July 1960?,"Hugh L. Dryden, the NASA Deputy Administrator at the time.",1910.06036v2-Figure5-1.png, Example diverse questions (with answers highlighted) generated by our model with different answer-relevant relations.,
1910.06036v2,What is the role of the gated attention mechanism in the model?,The gated attention mechanism is used to control the flow of information between the encoder and decoder.,1910.06036v2-Figure3-1.png, The framework of our proposed model. (Best viewed in color),
1910.06036v2,Which dataset split has more training pairs?,The Zhou Split has more training pairs.,1910.06036v2-Table3-1.png," Dataset statistics on Du Split (Du et al., 2017) and Zhou Split (Zhou et al., 2017).",
1910.13497v1,Which language pair has the most significant correlation between grammatical gender and lexical semantics?,The Russian-Ukrainian language pair.,1910.13497v1-Table1-1.png, The number of inanimate nouns for each gendered–genderless language pair. Bold indicates that our investigation reveals a significant correlation between grammatical gender and lexical semantics for that pair.,
1910.13497v1,Which gendered language has the highest correlation with Italian?,Spanish,1910.13497v1-Figure2-1.png, The correlation between b? ` and b? `′ for each pair of gendered languages ` and `′ (for English).,
1910.13497v1,Which language pair has the strongest positive correlation between grammatical gender and lexical semantics?,The language pair with the strongest positive correlation between grammatical gender and lexical semantics is **ko-tr**.,1910.13497v1-Figure1-1.png, The correlation between grammatical gender and lexical semantics for each of the 90 gendered– genderless language pairs (∗ indicates significance).,
1911.02496v1,Which model performs better with a smaller number of applications used for training?,E.T.-RNN,1911.02496v1-Figure4-1.png, E.T.-RNN has steeper learning curve than LGBM.,
1911.02496v1,What is the relationship between the number of transactions per client and the ROC AUC?,The ROC AUC generally increases as the number of transactions per client increases.,1911.02496v1-Figure5-1.png, Classification quality vs number of transactions,
1911.02496v1,What is the relationship between the number of transactions per client and the classification quality?,The classification quality generally increases with the number of transactions per client.,1911.02496v1-Figure6-1.png, Classification quality for customers grouped by number of transactions,
1911.02496v1,Which transaction was the largest in terms of amount?,"The transaction at the restaurant was the largest, with an amount of 230 EUR.",1911.02496v1-Table1-1.png, Data structure for a single client,
1911.02496v1,What is the purpose of the GRU RNN in the architecture?,"The GRU RNN is used to process the sequence of transactions for a client. Each GRU cell takes as input the previous hidden state and the current transaction, and outputs a new hidden state. The final hidden state is then used to predict the customer score.",1911.02496v1-Figure1-1.png, Final architecture,
1911.02496v1,Which learning rate schedule resulted in the highest Valid ROC-AUC?,gamma = 0.5,1911.02496v1-Table4-1.png, Learning rate schedules,
1911.02496v1,Which loss function has the highest validation ROC-AUC?,BCE Loss,1911.02496v1-Table3-1.png, Loss comparison,
1911.02496v1,Which encoder architecture performed the best in terms of validation ROC-AUC?,GRU 1-layer Bidirectional,1911.02496v1-Table2-1.png, Encoder architecture comparison,
1911.02496v1,Which model achieved the highest ROC AUC score?,E.T.-RNN,1911.02496v1-Table5-1.png, Experiment comparison,
1911.02496v1,Which regularization method performs the best in terms of ROC-AUC?,No regularization.,1911.02496v1-Figure2-1.png, Regularization methods,
1911.02496v1,Which ensemble size resulted in the highest median ROC-AUC?,The ensemble size of 12 resulted in the highest median ROC-AUC.,1911.02496v1-Figure3-1.png, Ensemble quality comparison,
1911.03624v1,Which of the images is the most natural and has the least artifacts?,(d) Our NatSR,1911.03624v1-Figure1-1.png, Super-resolved results (×4) of “0823” in DIV2K validation set [34]. A part of the image is cropped and zoomed for visualization. Our NatSR result is more natural with less artifacts which is perceptually plausible than other algorithms’ results.,
1911.03624v1,What is the role of the residual dense block (RDBlock) in the NatSR network architecture?,The RDBlock is used for short-path connection.,1911.03624v1-Figure6-1.png, Our NatSR network architecture. We adopt fractal residual learning for mid- and long-path skip connection and employ the residual dense block (RDBlock) for short-path connection.,
1911.03624v1,Which method achieves the highest NMD score according to the table?,Both HR and NatSR achieve the highest NMD score of 1.000.,1911.03624v1-Table1-1.png, Results of NMD score.,
1911.03624v1,What is the purpose of the residual connection in the RDBlock?,The residual connection adds the input of the block to the output of the block. This helps to prevent vanishing gradients and allows for deeper networks to be trained.,1911.03624v1-Figure7-1.png, Residual Dense Block (RDBlock) that we employ for our NatSR.,
1911.03624v1,Which model performed the best according to NIQE and NQSR?,NatSR,1911.03624v1-Figure8-1.png," NR-IQA results in the sorted order (left: NIQE [27], and right: NQSR [24]). The best is at the top and the worst is at the bottom.Our NatSR result is highlighted with darker color.",
1911.03624v1,Which method has the highest RGB-PSNR?,NatSR,1911.03624v1-Table2-1.png, Results of RGB-PSNR between LR input and downsampled SR image in LR domain.,
1911.03624v1,What is the relationship between the high-resolution (HR) image and the low-resolution (LR) image in the frequency domain?,"The LR image is a downsampled version of the HR image, which means that it has a lower frequency resolution. This is evident in the figure, where the LR image (b) has a narrower frequency spectrum than the HR image (a).",1911.03624v1-Figure2-1.png, A simple explanation of LR-HR relationship and SISR in the frequency domain.,
1911.03624v1,What is the relationship between the LR image and the HR space?,The LR image is a low-resolution version of an image in the HR space.,1911.03624v1-Figure3-1.png," Our proposed LR-HR model of the natural manifold and its discrimination for SISR. U is the image space, V is the possible HR space, and A,B, and N are three disjoint sets of V . α and σ control the boundary between the manifolds.",
1911.03624v1,Which model achieved the best average PSNR/SSIM values on the Urban100 benchmark?,EDSR.,1911.03624v1-Table3-1.png," FR-IQA results. The average PSNR/SSIM values on benchmarks. Red color indicates the best results, and the blue indicates the second best.",
1911.03624v1,Which image reconstruction method produced the most realistic results?,NatSR (Ours),1911.03624v1-Figure9-1.png, Visualized results on “img031” of Urban100.,
1911.03624v1,How do the DCT coefficients of bicubic up/downsampling kernels change with the size of the kernel?,The DCT coefficients become more concentrated around the low-frequency components as the size of the kernel increases.,1911.03624v1-Figure4-1.png, DCT coefficients of bicubic up/downsampling kernels for the scaling factor of ×4.,
1911.03624v1,What is the purpose of the convolutional layers in the NMD network architecture?,The convolutional layers are used to extract features from the input image.,1911.03624v1-Figure5-1.png, Our NMD network architecture.,
1911.08019v3,Which model has the highest accuracy for M = 50?,AQM (ours),1911.08019v3-Table1-1.png," Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better).",
1911.08019v3,How many dots are there in the Ms Pacman game?,There are 244 dots in the Ms Pacman game.,1911.08019v3-Figure12-1.png, Ms Pacman,
1911.08019v3,How does the player character's position change between the top and bottom rows of the image?,The player character moves from the left side of the screen to the right side of the screen.,1911.08019v3-Figure11-1.png, Pitfall,
1911.08019v3,What is the difference between the top and bottom images?,"The top image shows the game of Pong at the beginning of a point, while the bottom image shows the game at the end of a point.",1911.08019v3-Figure10-1.png, Pong,
1911.08019v3,What is the relationship between the encoder and decoder in the Online Continual Compression problem?,"The encoder compresses the input data into a latent representation, which is then stored in memory. The decoder can then reconstruct the original data from the latent representation.",1911.08019v3-Figure1-1.png," Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and fixed memory constraints",
1911.08019v3,How does codebook freezing affect the streaming MSE and drift MSE?,"Codebook freezing initially increases the streaming MSE, but then it starts to decrease and eventually stabilizes. The drift MSE increases gradually over time.",1911.08019v3-Figure4-1.png," Impact of codebook freezing. Vertical black line indicates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs",
1911.08019v3,What is the effect of ablating the second module of the AQM on the accuracy of the model?,The accuracy decreases from 23.2 ± 1.1 to 20.5 ± 1.3.,1911.08019v3-Table2-1.png, Imagenet offline training evaluation from online continual compression. We see a clear gain over a standard Reservoir sampling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes).,
1911.08019v3,What is the effect of freezing on the streaming MSE for a reconstruction threshold of 5?,Freezing reduces the streaming MSE for a reconstruction threshold of 5.,1911.08019v3-Table5-1.png, Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs.,
1911.08019v3,What is the effect of increasing the recon_th parameter on the streaming MSE and drift MSE?,Increasing the recon_th parameter generally leads to an increase in both streaming MSE and drift MSE.,1911.08019v3-Figure9-1.png, Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.,
1911.08019v3,What is the purpose of the codebook in the Vector Quantization process?,"The codebook is used to store a set of representative vectors, or codewords, that are used to represent the input data. The encoder maps each input vector to the closest codeword in the codebook, and the decoder then uses the codeword to reconstruct the original vector. This process allows for a reduction in the amount of data that needs to be stored, while still preserving the essential information contained in the original data.",1911.08019v3-Figure2-1.png, Illustration of reduced representation drift from Vector Quantization,
1911.08019v3,Which row of the image shows the original images?,The top row.,1911.08019v3-Figure8-1.png," Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12× smaller than their original image.",
1911.08019v3,What is the effect of iterative buffer balancing on the distribution of samples in AQM?,Iterative buffer balancing makes the distribution of samples in AQM more uniform.,1911.08019v3-Figure7-1.png, (left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure,
1911.08019v3,What is the difference between the top and bottom images?,"The top image is a sample decoded from the buffer at the end of training from scratch with a 32x compression rate, while the bottom image is the original lidar data.",1911.08019v3-Figure5-1.png, Top: Sample decoded from the buffer at the end of training from scratch (32x compression rate). Bottom: Original lidar,
1911.08019v3,Which compression method results in the smallest file size while still meeting the SNNRMSE requirement?,AQM + finetune + PNG,1911.08019v3-Table3-1.png, Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm.,
1911.08019v3,What is the difference between the lidar reconstruction and the original image?,"The lidar reconstruction is a 3D representation of the scene, while the original image is a 2D representation. The lidar reconstruction shows the depth of the scene, while the original image does not.",1911.08019v3-Figure14-1.png, Lidar reconstruction (left) vs original (right),
1911.08019v3,Which game shows the biggest difference in performance between the original state and the AQM Recon state?,Ms. Pacman,1911.08019v3-Table4-1.png," Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstructions from AQM after online compression. Acc is averaged for each game over game specific prediction.",
1911.08019v3,What are the differences between the original image and the reconstructed image?,The original image is sharper and has more detail than the reconstructed image. The reconstructed image is also slightly blurry and has some artifacts.,1911.08019v3-Figure6-1.png, Top: original. Bottom: reconstructed from AQM,
1911.08019v3,How does the architecture of Adaptive Quantization Modules ensure that each level learns independently?,The architecture of Adaptive Quantization Modules uses gradient isolation between modules. This means that the gradients from one level do not affect the learning of other levels.,1911.08019v3-Figure3-1.png, Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules,
1911.08019v3,"Which part of the image is the lidar reconstruction, and which part is the original?","The left part of the image is the lidar reconstruction, and the right part is the original.",1911.08019v3-Figure13-1.png, Lidar reconstruction (left) vs original (right),
1911.11758v3,"Which of the following statements best describes the relationship between the ""Real Images"" and the ""Generated Images""?","The ""Generated Images"" are created by mixing and matching the factors of shape, pose, texture, and background from the ""Real Images"".",1911.11758v3-Figure1-1.png," Conditional mix-and-match image generation. Our model, MixNMatch, can disentangle and encode up to four factors—background, object pose, shape, and texture—from real reference images, and can arbitrarily combine them to generate new images. The only supervision used to train our model is bounding box annotations to model background.",
1911.11758v3,What is the effect of varying the shape of the bird?,"The shape of the bird affects the overall appearance of the bird. For example, the seagull in the top left corner of the image has a long, narrow body and wings, while the grebe in the bottom right corner has a short, stout body and wings.",1911.11758v3-Figure4-1.png," Varying a single factor. Real images are indicated with red boxes. For (a-d), the reference images on the left/top provide three/one factors. The center 3x3 images are generations. For example, in (a) the top row yellow bird has an upstanding pose with its head turned to the right, and the resulting images have the same pose.",
1911.11758v3,"What does the letter ""z"" in the caption represent?",Pose.,1911.11758v3-Figure11-1.png," Varying a single factor. Real images are indicated with red boxes. For (a-d), the reference images on the left/top provide three/one factors. The center 5x5 images are generations.",
1911.11758v3,Which of the images in the figure are real photographs and which are generated by the model?,"The first three rows of the figure are real photographs, while the last row shows images generated by the model.",1911.11758v3-Figure5-1.png, sketch2color. First three rows are real reference images. Last row shows generation results of adding background and texture to the sketch images.,
1911.11758v3,What is the relationship between the images in the top row and the images in the bottom row?,"The images in the top row are the input images, and the images in the bottom row are the generated images.",1911.11758v3-Figure6-1.png," cartoon2img. MixNMatch automatically learns part semantics, without supervision; e.g., in the 2nd column, the colors of the texture reference are accurately transferred.",
1911.11758v3,Which of the GANs listed in the table achieved the highest Inception Score for generating images of birds?,MixNMatch,1911.11758v3-Table1-1.png," Image quality & diversity. IS (↑ better) and FID (↓ better). MixNMatch generates diverse, high-quality images that compare favorably to state-of-the-art baselines.",
1911.11758v3,Which method achieves the highest accuracy for classifying dogs?,MixNMatch,1911.11758v3-Table2-1.png, Fine-grained object clustering. Our approach outperforms state-of-the-art clustering methods.,
1911.11758v3," 

What is the purpose of the red boxes in the figure? "," 

The red boxes indicate which images are real and which are generated. ",1911.11758v3-Figure10-1.png," Varying a single factor. Real images are indicated with red boxes. For (a-d), the reference images on the left/top provide three/one factors. The center 5x5 images are generations.",
1911.11758v3,What is the role of the image-code pair discriminators in the MixNMatch architecture?,The image-code pair discriminators are responsible for ensuring that the generated images and their corresponding latent codes are realistic and match the distribution of real image-code pairs.,1911.11758v3-Figure2-1.png," MixNMatch architecture. (a) Four different encoders, one for each factor, take a real image as input to predict the codes. (b) Four different latent codes are sampled and fed into the FineGAN generator to hierarchically generate images. (c) Four image-code pair discriminators optimize the encoders and generator, to match their joint image-code distributions.",
1911.11758v3,What does the figure show?,The figure shows a comparison of the original bird images with the images generated by different methods.,1911.11758v3-Figure8-1.png," Shape & texture disentanglement. Our approach preserves shape, texture better than strong baselines.",
1911.11758v3,Which model has the best performance in terms of texture disentanglement?,"Deforming AE has the best performance in terms of texture disentanglement, with a score of 0.792.",1911.11758v3-Table3-1.png, Shape & texture disentanglement. (Top) Comparisons to baselines. (Bottom) Ablation studies. We report keypoint L2-distance and color histogram χ2-distance for measuring shape and texture disentanglement (↓ better).,
1911.11758v3,How does MixNMatch animate the object in the static image?,MixNMatch combines the pose factor z from a reference video with the other factors in a static image to animate the object.,1911.11758v3-Figure7-1.png," image2gif. MixNMatch can combine the pose factor z from a reference video (top row), with the other factors in a static image (1st column) to animate the object.",
1911.11758v3,Which method achieves the best disentanglement of shape and texture for all keypoints?,MixNMatch,1911.11758v3-Table4-1.png," Shape & texture disentanglement. MixNMatch outperforms strong baselines in terms of both shape or texture disentanglement for all keypoints. (c) is code mode, (f) is feature mode.",
1911.11758v3,What is the relationship between the images in the red boxes and the images in between them?,"The images in the red boxes are real images, while the images in between them are generated by linearly interpolating codes predicted by the encoder. This means that the generated images are a smooth transition between the two real images.",1911.11758v3-Figure9-1.png," Latent code interpolation. Images in the red boxes are real, and intermediate images are generated by linearly interpolating codes predicted by our encoders.",
1911.11758v3," What is the difference between the ""feature mode"" and ""code mode"" generations of MixNMatch? "," The ""feature mode"" generations more accurately preserve the original shape information of the reference images, while the ""code mode"" generations preserve shape information at a more semantic level. ",1911.11758v3-Figure3-1.png," Comparison between code mode & feature mode. Rows 1-3 are real reference images, in which we extract background b, texture c, and shape+pose p & z, respectively. Rows 4-5 are MixNMatch’s feature mode (which accurately preserves original shape information) and code mode (which preserves shape information at a semantic level) generations.",
1911.11758v3,What is the purpose of the red boxes in the figure?,The red boxes indicate which images are real and which are generated.,1911.11758v3-Figure12-1.png," Varying a single factor. Real images are indicated with red boxes. For (a-d), the reference images on the left/top provide three/one factors. The center 5x5 images are generations.",
1911.12199v4,Which method has the highest percentage of instances that are closer to the counterfactual than the factual example on the HELOC dataset when using the Euclidean distance metric?,FOCUS,1911.12199v4-Table1-1.png," Evaluation results for Experiment 1 comparing FOCUS and FT counterfactual examples. Significant improvements and losses over the baseline (FT) are denoted by H and N, respectively (p < 0.05, two-tailed t-test,); ◦ denotes no significant difference; ⊗ denotes settings where the baseline cannot find a counterfactual example for every instance.",
1911.12199v4,Which method performed better on the COMPASS dataset in terms of Mahalanobis distance?,FOCUS performed better than DACE on the COMPASS dataset in terms of Mahalanobis distance.,1911.12199v4-Table2-1.png," Evaluation results for Experiment 2 comparing FOCUS and DACE counterfactual examples in terms of Mahalanobis distance. Significant improvements over the baseline are denoted by H (p < 0.05, two-tailed t-test,). ◦ denotes no significant difference.",
1911.12199v4,Which method converged faster to a stable mean distance?,Wine_DT,1911.12199v4-Figure3-1.png, Mean distance (top) and cumulative % (bottom) of counterfactual examples in each iteration of FOCUS for Manhattan explanations.,
1911.12199v4,What is the difference between the left and right trees?,"The left tree is a decision tree, while the right tree is a differentiable approximation of the decision tree.",1911.12199v4-Figure1-1.png, Left: A decision tree T and node activations for a single instance. Right: a differentiable approximation of the same tree T̃ and activations for the same instance.,
1911.12199v4,Which of the following methods leads to perturbed examples that do not change the prediction of the forest?,The Feature Tweaking (FT) baseline method.,1911.12199v4-Figure2-1.png," An example of how the Feature Tweaking (FT) baseline method (explained in Section 5.1) and our method handle an adaptive boosting ensemble with three trees. Left: decision boundary of the ensemble. Middle: three positive leaves that form the decision boundary, an example instance and the perturbed examples suggested by FT. Right: approximated loss L̃pred and its gradient w.r.t. x̄. The FT perturbed examples do not change the prediction of the forest, whereas the gradient of the differentiable approximation leads toward the true decision boundary.",
1912.02184v1,What is the difference between the adversarial images produced by the ResNet-152 and the model with a sequential top-down attention model?,"The adversarial image produced by the ResNet-152 contains no visible interpretable structure, while the adversarial image produced by the model with a sequential top-down attention model contains a salient and coherent image of a beaver’s head.",1912.02184v1-Figure1-1.png," We augment a modern neural network with a sequential top-down attention model for image classification. The model achieves state-of-the-art adversarial robustness against PGD attacks and the resulting adversarial images are often human interpretable. On the left is a source image (label:wallet) — both an adversarially trained ResNet-152 and our model classify it correctly. In the middle and on the right are adversarial examples produced by a 250 step PGD attack against each model (target class is beaver). Both models fail to defend against the attack and predict the target class as their top-1 output. However, while the attack image for the ResNet contains no visible interpretable structure, the attack image for our model contains a salient and coherent image of a beaver’s head (best viewed zoomed in on screen).",
1912.02184v1,Which model has the lowest attack success rate?,S3TA-16-30,1912.02184v1-Figure5-1.png," Attack success rates for all models presented (lower is better). The main effects observed for top 1 accuracy hold here: more attention steps lower the attack success rate and more PGD steps during training help reduce it even further. S3TA-16-30 clearly has the lowest attack success rates, about 25% lower than DENOISE while nominal accuracy is similar (see 1)",
1912.02184v1,What is the difference between the loss landscapes of S3TA-4 and S3TA-16?,"The loss landscape of S3TA-4 is more linear than that of S3TA-16. This means that there is less gradient obfuscation in S3TA-4, which can make it easier to train models.",1912.02184v1-Figure6-1.png, Loss landscapes for S3TA-4 (left) and S3TA-16 (right). The surfaces are approximately linear meaning there is no significant gradient obfuscation.,
1912.02184v1,What is the difference between the adversarial images generated by PGD to attack the ResNet-152 and S3TA-4 models?,"The adversarial images generated by PGD to attack the ResNet-152 model are not particularly interesting, while the S3TA examples are extremely structured.",1912.02184v1-Figure12-1.png," Adversarial images generated by PGD to attack the ResNet-152 and S3TA-4 model with clear and intrepretable structure. The source label is depicted on the left, the target label on the right. Source images are the leftmost column. We generate examples for 10, 100, 1000 and 10,000 PGD steps. While the ResNet attacks are not particularly interesting, the S3TA examples are extremely structured. Note the salient, global and often realistic structures appearing - the Junco bird, the head of the Iguana, the body of the Indian Cobra or the Water Ouzel, a bird which is often depicted next to water (which is also created). We encourage the reader to view these on screen and zoomed-in.",
1912.02184v1,Which model has the highest attack success rate?,ResNet-152,1912.02184v1-Table3-1.png," Top-1 accuracy under random targeted SPSA attacks (batch size of 4096 and 100 iterations). SPSA is a gradient free method which provides evidence whether gradients are obfuscated. As can be seen all models perform similarly, considering they all defend better here than the corresponding gradient based attack (making the actual reported number less informative).",
1912.02184v1,Which model performed the best under a 1000-step PGD attack?,S3TA-16-30.,1912.02184v1-Table1-1.png," Full results for all models on random targeted PGD attacks with the ImageNet test set, with different number of attack steps. Bottom three rows are models trained with 30 PGD steps, the rest were trained with 10 PGD steps.",
1912.02184v1,Which model has the best Top-1 Accuracy?,Squeeze and excite,1912.02184v1-Figure7-1.png," Results of our model on the “Natural Adversarial Examples” dataset. Our model achieves better top-1 accuracy, and better AURRA (see text for details) than both a ResNet-152 and DENOISE. Squeeze and excite outperforms our model in all measures.",
1912.02184v1,Which model has the highest Top-1 accuracy under untargeted attacks at 200 PGD steps?,LLR,1912.02184v1-Table2-1.png," Top-1 accuracy under untargeted attacks at 200 PGD steps. As can be seen, our model is very competitive with existing methods though not optimized for this particular attack method.",
1912.02184v1,How do the adversarial images generated by PGD to attack the ResNet-152 and S3TA-4 model change as the number of PGD steps increases?,The adversarial images become more visually distorted as the number of PGD steps increases.,1912.02184v1-Figure11-1.png," Adversarial images generated by PGD to attack the ResNet-152 and S3TA-4 model with no apparent visual structure. The source label is depicted on the left, the target label on the right. Source images are the leftmost column. We generate examples for 10, 100, 1000 and 10,000 PGD steps. These are cases where the perturbation does not have clear structure, as for many of the images. Some local structure may be interpretable - tiles for the washbasin for example, but there’s little coherent global structure. Images are best viewed on screen and zoomed in.",
1912.02184v1,How does the model use the spatial basis to generate the attention map?,The spatial basis is concatenated to the keys and values tensors before the attention map is generated. This allows the model to learn to attend to specific spatial locations in the input image.,1912.02184v1-Figure2-1.png," A general view of the sequential top-down attention model. The input image is passed through a ResNet to produce a keys and a values tensor. We concatenate a fixed, predefined spatial basis to both. A query vector is decoded from an LSTM state and an inner product between it and the keys vector is calculated at each spatial location. A softmax is then applied to produce an attention map. This attention map is point-wise multiplied with the values tensor, and the result is summed spatially to produce an answer vector. This answer vector is the input to the LSTM at this time step. The LSTM output is then decoded into class logits to produce the output of the classifier. More than one query vector can be produced at each time step (resulting in a matching number of answer vectors).",
1912.02184v1,Which model is the most robust to extra strong PGD attacks with 5000 and 10000 steps?,S3TA-16-30,1912.02184v1-Table4-1.png, Results for extra strong PGD attacks with 5000 and 10000 steps. The general picture remains as S3TA-16-30 is significanly more robust than all other models. S3TA-16 trained with 10 PGD steps is more accurate than a ResNet-152 trained with 30 PGD steps even under these powerful attacks.,
1912.02184v1,Which model is the most robust to PGD attacks with multiple restarts?,S3TA-16-30.,1912.02184v1-Table5-1.png," PGD attack with multiple restarts. We test the various method under attacks starting at multiple random initialization points. We test with 100 and 1000 PGD attack steps, initialized at 1, 10 and 100 random starting points. We count the attack successful even if only one of the adversarial examples caused the model to output the target class. We only count an image to be correctly classified if it was correctly classified across all adversarial images. As can be see, all models are quite robust here, though again, S3TA-16-30 outperforms them all.",
1912.02184v1,Which model is the most robust to random targeted PGD attacks with Adam optimizer?,S3TA-16-30,1912.02184v1-Table6-1.png," Random targeted PGD attack with Adam optimizer. We measure the robustness of some models under an random targeted attack optimized with the Adam optimizer (rather than iterative FGSM). The attack uses 250 iterations, with learning rate 0.1 for the first 100 iterations, 0.01 for next 100 and 0.001 for the last 50. This has be shown [] to be a stronger attack the iterative FGSM. As can S3TA-16-30 is significantly better at defending here compared to the other models.",
1912.02184v1,What is the main difference between the perturbations generated to attack the ResNet model and the S3TA model?,"The perturbations generated to attack the ResNet model are mostly local, comprising at best of disconnected visible features related to the target class. On the other hand, the examples generated to attack S3TA contain global, coherent and human interpretable structure.",1912.02184v1-Figure8-1.png," Adversarial images generated by PGD to attack the ResNet-152 and S3TA-4 model. The source label is depicted on the left, the target label on the right. Source images are the leftmost column. We generate examples for 10, 100, 1000 and 10,000 PGD steps. Note how for the examples shown here, the perturbations are quite visible for both models (not surprising in light of the strength of the attack). However, the perturbations generated to attack the ResNet model are mostly local, comprising at best of disconnected visible features related to the target class. On the other hand, the examples generated to attack S3TA contain global, coherent and human interpretable structure. Note the 3D structure and spatial extent of the locomotive (top-left), the coherency of the king snake on the ground (bottom-left), the camel head on the bark of the tree (top-right) and the beagle and the man appearing (bottom-right). These structures appear mostly when there are already existing features in the image which can be used to form the target class, and the model uses them in a global, coherent manner. Images are best viewed on screen and zoomed in.",
1912.02184v1,How do the attention steps of the model change as it progresses through the 16 steps?,"The attention steps of the model initially focus on the main object in the source image, but some of the heads are gradually attracted away towards a group of branches in the background that resemble a bloodhound dog in the attack image.",1912.02184v1-Figure9-1.png," Attacks attract attention away from the main object. Here we see the source image (left) attack image (target class bloodhound) and the 4 attention heads of the model as they unroll through 16 steps. Some of the heads still attend the main object from the source image, however some of the heads are attracted away towards a group of branches in the background. Upon close inspection it does seem that these branches resemble a bloodhound dog in the attack image. Though this structure is not very salient to humans, it is enough to attract the attention away and cause the model to mislabel the image. Best viewed on screen and zoomed in.",
1912.02184v1,What is the role of the query network in the attention mechanism?,The query network takes the answer from the previous LSTM and uses it to generate a query vector. This query vector is then used to compute the attention weights for the different regions of the image.,1912.02184v1-Figure10-1.png, Full model details. Left - high level overview of the recurrent mechanism (for a 4-step model) and the corresponding vision net. Right - a zoomed in detail on one attention step. See text for full details.,
1912.02184v1,Which model has the highest top-1 accuracy on the ImageNet test set after being attacked with 30 PGD steps?,S3TA-16,1912.02184v1-Figure4-1.png," A S3TA-16 model adversarially trained with 30 PGD steps vs. ResNet-152 (30 steps) and DENOISE [56] top 1 accuracy on the ImageNet test set. DENOISE is the current state-of-the-art on ImageNet and as can be seen S3TA-16 performs significantly better than both models, setting a new state-of-the-art.",
1912.02184v1,How does the introduction of the attention model affect the performance of the S3TA models?,"The introduction of the attention model significantly improves the performance of the S3TA models, even with just 2 attention steps.",1912.02184v1-Figure3-1.png," S3TA-2, 4, 8 and 16 vs. ResNet-152 top 1 accuracy on the ImageNet test set.. All models were adversarially trained with 10 PGD steps. Note how the introduction of the attention model significantly improve performance even with 2 attention steps, and that adding more steps (S3TA-16) improves performance further: a S3TA-16 model is more robust at a 1000 attack steps than a ResNet-152 model at a 100 attack steps.",
1912.02613v3, What are the three main components of the proposed framework?," The three main components of the proposed framework are the encoding block, the optional classification block, and the decoding block.",1912.02613v3-Figure1-1.png," The proposed framework, fully detailed in Section 3.2. The blue, red and green blocks correspond to the singer encoder, vocal technique encoder, and the joint decoder, respectively.",
1912.02613v3,Which vocal technique conversion results in the most noticeable change in the spectrogram?,Vocal fry.,1912.02613v3-Figure2-1.png," Examples of singer conversion (a) and vocal technique conversion (b), converted by the model M3. The first column refers to source, and the rest correspond to different targets. Targets that are the same as the sources are faded.",
1912.02613v3,Which model has the highest accuracy after singer conversion?,Model M0 has the highest accuracy after singer conversion with 90.68%.,1912.02613v3-Table1-1.png," The classification accuracy (%) derived by the three attribute classifiers, given different models. * denotes the converted attributes.",
1912.02644v1,What is the effect of fine-tuning on the extrapolated rotations in the autoencoder latent space?,Fine-tuning improves the quality of the extrapolated rotations.,1912.02644v1-Figure4-1.png, Two examples of extrapolated rotations in the autoencoder latent space. Row 1: Extrapolated linear path in latent space prior to fine-tuning. Row 2: Extrapolated linear path in latent space after fine-tuning. Row 3: Extrapolated hyperspherical VAE path Row 4: Extrapolated transport operator path prior to fine-tuning. Row 5: Extrapolated transport operator path after fine-tuning.,
1912.02644v1,Which method is the most robust to rotation?,The Euclidean method is the most robust to rotation.,1912.02644v1-Figure5-1.png, Nearest neighbor classification accuracy of rotated test digits.,
1912.02644v1,Which latent space representation preserves the most information about the original data?,The autoencoder latent space representation preserves the most information about the original data.,1912.02644v1-Figure1-1.png, (a) Ground truth latent embedding. (b) Circle points embedded in the autoencoder latent space. (c) Circle points embedded in the VAE latent space. (d) Circle points embedding in the hyperspherical VAE latent space (e) Circle points embedded in the manifold autoencoder latent space,
1912.02644v1,"What is the difference between the ""Transport Operator"" and the ""Linear"" method in terms of their ability to generate a full gait sequence?","The ""Transport Operator"" method is more accurate than the ""Linear"" method.",1912.02644v1-Figure6-1.png, (a) The effect of applying a learned operator which generates a full gait sequence. (b) Example of one feature from the data space as it progresses during an extrapolated gait sequence. (c) The error in between the estimated gait sequences and the ground truth gait sequences. The estimated sequences are extrapolated from a starting point in the latent space.,
1912.02644v1,What is the effect of applying transport operators trained on gait data in the input space?,"The transport operators can be used to generate new, realistic gait sequences.",1912.02644v1-Figure10-1.png, The effect of applying four example transport operators trained on gait data in the input space.,
1912.02644v1,Which training phase uses the smallest batch size?,"All three training phases use the same batch size of 32, except for the autoencoder training phase, which uses a batch size of 64.",1912.02644v1-Table3-1.png, Training parameters for rotated MNIST experiment,
1912.02644v1,Which training stage has the highest learning rate for the parameter φ?,Autoencoder training,1912.02644v1-Table1-1.png, Training parameters for 2D circle experiment,
1912.02644v1,Which transport operator has the highest magnitude for the gait sequences experiment?,Transport Operator number 6,1912.02644v1-Figure7-1.png, Magnitude of the operators after the transport operator training phase. (a) For 2D circle experiment (b) For the rotated MNIST dataset (c) For gait sequences.,
1912.02644v1, What is the difference between the encoder and decoder networks? ," The encoder network reduces the dimensionality of the input data, while the decoder network increases the dimensionality of the data to reconstruct the original input. ",1912.02644v1-Table2-1.png, Autoencoder network architecture for rotated MNIST experiment,
1912.02644v1," 

How are the weights of the encoder and decoder updated during the fine-tuning phase? "," 

During the fine-tuning phase, the weights of the encoder are updated using the pre-trained weights from the transport operator training phase. The weights of the decoder are updated from random initialization. ",1912.02644v1-Figure2-1.png, Visualization of the three phases of training the manifold autoencoder.,
1912.02644v1,How many layers does the encoder network have?,The encoder network has 4 layers.,1912.02644v1-Table4-1.png, Autoencoder network architecture for walking gait experiment,
1912.02644v1,What is the difference between the learning rate used for the autoencoder and the transport operator?,"The learning rate for the autoencoder is 0.0005, while the learning rate for the transport operator is 0.005.",1912.02644v1-Table5-1.png, Training parameters for walking gait experiment,
1912.02644v1,What are the differences between the gait sequences generated by operators 3 and 6?,"The gait sequence generated by operator 3 has a more pronounced swing phase, with the legs extending further forward and backward. The gait sequence generated by operator 6 is more compact, with the legs staying closer to the body.",1912.02644v1-Figure8-1.png, (a) Gait sequence generated from operator 3. (b) Gait sequence generated from operator 6.,
1912.02644v1,Which pair of transport operators has the most correlated coefficients?,Transport operators 5 and 6.,1912.02644v1-Figure9-1.png, Scatter plots of the inferred coefficients for pairs of transport operators. The coefficients were inferred over the latent representations of pairs of points in the gait sequences. (a) Coefficients for transport operator 3 and transport operator 5. (b) Coefficients for transport operator 3 and transport operator 6. (c) Coefficients for transport operator 5 and transport operator 6.,
1912.02644v1,Why does the manifold offset distance become better able to separate samples from the inner and outer circles as fine-tuning progresses?,"As fine-tuning progresses, the manifold offset distance learns to take into account the underlying manifold structure of the data. This allows it to better distinguish between points that are close together on the manifold, even if they are far apart in the Euclidean space.",1912.02644v1-Figure3-1.png," (a) Orbits of transport operators learned in the neural network latent space. (b) Heat map of the manifold offset distance from the red reference point to every point on the grid. The manifold offset distance is small on the 1D circular manifold on which the reference point resides. (c) AUC curve for classification of points on the same or different circles during fine-tuning process. As fine-tuning progresses, the manifold offset distance becomes better able to separate samples from the inner and outer circles.",
1912.04487v3,What is the role of audio in the proposed video skimming approach?,"Audio serves as an efficient preview of the visual content, providing important dynamic information that complements the appearance information captured by a single image.",1912.04487v3-Figure1-1.png," Our approach learns to use audio as an efficient preview of the accompanying visual content, at two levels. First we replace the costly analysis of video clips with a more efficient processing of image-audio pairs. A single image captures most of the appearance information within the clip, while the audio provides important dynamic information. Then our video skimming module selects the key moments (a subset of image-audio pairs) to perform efficient video-level action recognition.",
1912.04487v3,Which model performs best on the UCF-101 dataset?,The Clip-based model performs best on the UCF-101 dataset.,1912.04487v3-Figure5-1.png," Clip-level action recognition on Kinetics-Sounds, UCF-101, and ActivityNet. We compare the recognition accuracy and the computational cost of our model against a series of baselines. Our IMGAUD2VID approach strikes a favorable balance between accuracy and efficiency.",
1912.04487v3,What is the effect of increasing the subsampling factor on the accuracy of the model?,The accuracy of the model decreases as the subsampling factor increases.,1912.04487v3-Figure6-1.png, Trade-off between efficiency and accuracy when using sparse indexing features or early stopping on ActivityNet. Uniform denotes the UNIFORM baseline in Table 1.,
1912.04487v3,Which sampling method achieves the highest accuracy on the ActivityNet dataset?,"The SCS sampler achieves the highest accuracy on the ActivityNet dataset, with an accuracy of 69.1%.",1912.04487v3-Table1-1.png," Video-level action recognition accuracy (in %) on ActivityNet (# classes: 200) and Mini-Sports1M (# classes: 487). Kinetics-Sounds and UCF-101 consist of only short trimmed videos, so they are not applicable here. Our method consistently outperforms all baseline methods. Ours (sparse) uses only about 1/5 the computation cost of the last four baselines, while achieving large accuracy gains. See Table 2 for more computation cost comparisons.",
1912.04487v3,Which method achieves the highest mean Average Precision (mAP) with the lowest GFLOPs?,Ours (Image-Audio | Image-Audio).,1912.04487v3-Figure7-1.png, Comparisons with other frame selection methods on ActivityNet. We directly quote the numbers reported in AdaFrame [77] and MultiAgent [75] for all the baseline methods and compare the mAP against the average GFLOPs per test video. See text for details.,
1912.04487v3,What are the differences between the frames selected by the method and the uniformly selected frames?,"The frames selected by the method are more indicative of the corresponding action than the uniformly selected frames. For example, in the throwing discus video, the method selects frames that show the athlete in the process of throwing the discus, while the uniformly selected frames show the athlete at various points in the throwing motion. Similarly, in the rafting video, the method selects frames that show the raft going through rapids, while the uniformly selected frames show the raft at various points in the river.",1912.04487v3-Figure8-1.png, Qualitative examples of 5 uniformly selected moments and the first 5 visually useful moments selected by our method for two videos of the actions throwing discus and rafting in ActivityNet. The frames selected by our method are more indicative of the corresponding action.,
1912.04487v3,Which method achieves the highest mAP on ActivityNet?,Our method achieves the highest mAP of 89.9 on ActivityNet.,1912.04487v3-Table2-1.png, ActivityNet comparison to SOTA methods.,
1912.04487v3,What are the inputs to the student model?,The student model takes two inputs: the starting frame of the video clip and the audio spectrogram.,1912.04487v3-Figure2-1.png," IMGAUD2VID distillation framework: The teacher model is a video-clip classifier, and the student model consists of a visual stream that takes the starting frame of the clip as input and an audio stream that takes the audio spectrogram as input. By processing only a single frame and the clip’s audio, we get an estimate of what the expensive video descriptor would be for the full clip.",
1912.04487v3, What is the purpose of the Softmax function in the attention-based frame selection mechanism?," The Softmax function is used to normalize the attention weights, ensuring that they sum to 1. This allows the weighted sum to be interpreted as a probability distribution over the indexing features.",1912.04487v3-Figure4-1.png, Attention-based frame selection mechanism.,
1912.04487v3, How does the IMGAUD-SKIMMING network make predictions about the next moment to focus on in the untrimmed video?, The network uses the hidden state for the current time step to make predictions about the next moment to focus on in the untrimmed video through the querying operation illustrated in Fig. 4.,1912.04487v3-Figure3-1.png," Our IMGAUD-SKIMMING network is an LSTM network that interacts with the sequences of image and audio indexing features to select where to “look at” and “listen to” next. At each time step, it takes the image feature and audio feature for the current time step as well as the previous hidden state and cell output as input, and produces the current hidden state and cell output. The hidden state for the current time step is used to make predictions about the next moment to focus on in the untrimmed video through the querying operation illustrated in Fig. 4. The average-pooled IMGAUD2VID features of all selected time steps is used to make the final prediction of the action in the video.",
1912.07225v1,Why is sentence S2 more coherent with sentence S1 than sentence S4?,"Sentence S2 is more coherent with sentence S1 than sentence S4 because they share the same entity ""dad.""",1912.07225v1-Figure1-1.png," An example of sentence ordering, where the correct order is: S1, S2, S3, S4. S2 is more coherent with S1 than S4, as they share the same entity “dad”.",
1912.07225v1,Which model modification has the greatest impact on the accuracy of the S-Graph model?,Removing 50% of entities has the greatest impact on the accuracy of the S-Graph model.,1912.07225v1-Table3-1.png," Ablation study of our graph structure on arXiv Abstract, where Share Parameters means employing the same parameters to update entity and sentence nodes.",
1912.07225v1,Which model performed the best on the SIND dataset for predicting the first sentence?,"The SE-Graph model performed the best on the SIND dataset for predicting the first sentence, with an accuracy of 78.12%.",1912.07225v1-Table2-1.png, The ratio of correctly predicting first and last sentences on arXiv Abstract and SIND. † indicates previously reported scores.,
1912.07225v1,What is the role of the Sentence Encoder in the ATTOrderNet architecture?,"The Sentence Encoder is responsible for converting the input sentences into a sequence of hidden representations, which are then fed into the Self-Attention layer.",1912.07225v1-Figure2-1.png, The architecture of ATTOrderNet.,
1912.07225v1,What does the green dashed arrow represent in the figure?,The green dashed arrow represents the influence of the sentence context on the generation of the entity graph at time t.,1912.07225v1-Figure4-1.png, GRN encoding for a sentence-entity graph. The original graph structure is only drawn on step t for being concise.,
1912.07225v1,How does the sentence-entity graph in (b) differ from the fully-connected graph in (a)?,"The sentence-entity graph in (b) only connects entities that are related by a syntactic role in a sentence, while the fully-connected graph in (a) connects all entities to each other.",1912.07225v1-Figure3-1.png, Comparison between (a) a fully-connected graph and (b) our sentence-entity graph for the example in Figure 1. An edge label in (b) corresponds to the syntactic role of an entity in a sentence.,
1912.07225v1,What is the relationship between the recurrent step and accuracy?,The accuracy generally increases as the recurrent step increases.,1912.07225v1-Figure5-1.png, Results on the arXiv Abstract validation set regarding the recurrent steps t.,
1912.07225v1,Which model performs best on the NIPS Abstract test set according to the Accuracy metric?,The SE-Graph model performs best on the NIPS Abstract test set with an Accuracy of 57.27%.,1912.07225v1-Table1-1.png," Main results on the sentence ordering task, where #pm shows the number of parameters, † indicates previously reported scores and * means significant at p < 0.01 over the F-Graph on each test set. V-LSTM+PtrNet stands for Varient-LSTM+PtrNet. We conduct 1,000 bootstrap tests [Koehn, 2004] to measure the significance in metric score differences.",
2001.03653v1,What is the relationship between the divergence of a CNN on a small test set and the divergence on a large test set?,The divergence of a CNN on a small test set is positively correlated with the divergence on a large test set.,2001.03653v1-Figure2-1.png," CNN divergence on a small test set is biased, but correlated, with a large test set.",
2001.03653v1,Which set of data shows the most overfitting?,The training set.,2001.03653v1-Figure3-1.png, CNN divergence reveals overfitting in a large GAN.,
2001.03653v1,Which model performed the best according to the test set CNN divergence metric?,WGAN-GP,2001.03653v1-Table3-1.png, Evaluation of different models on CIFAR-10 by test set CNN divergence. The WGAN-GP attains a lower value of the test divergence than memorization of the training set.,
2001.03653v1,"Which metric is the most sensitive to diversity, and which is the least sensitive?","The CNN Div. metric is the most sensitive to diversity, and the Inception Score is the least sensitive.",2001.03653v1-Table2-1.png," For different evaluation metrics, how many training set images does one need to memorize to score better than a well-trained GAN model? A larger n means the metric is more sensitive to diversity.",
2001.03653v1,Which distribution is preferred by common GAN benchmarks?,"The distribution preferred by common GAN benchmarks is p̂train, which is the red distribution in the figure.",2001.03653v1-Figure1-1.png," Common GAN benchmarks prefer training set memorization (p̂train, red) to a model (q, green) which imperfectly fits the true distribution (p, blue) but covers more of p’s support.",
2001.03653v1,"Based on the table, which evaluation metric assigns better scores to GANs that generalize beyond the training set?",DCNN,2001.03653v1-Table1-1.png," The Inception Score and FID assign better scores to memorization of the training set, but a neural network divergence, DCNN, prefers a GAN which generalizes beyond the training set.",
2001.05876v3,"Which retrieval method performed better, text-retrieval or image-retrieval?",Image-retrieval performed better than text-retrieval.,2001.05876v3-Table3-1.png, Performance of text-retrieval model on MSCOCO Karpathy validation set.,
2001.05876v3,Which model achieved the highest CIDEr score when trained with cross-entropy loss?,Up-Down,2001.05876v3-Table1-1.png," Experiment of our proposed recall mechanism on the MSCOCO Karpathy test split with both cross-entropy loss and CIDEr optimization. We implement our proposed methods: semantic guide (SG), recalled-word slot (RWS) and recalled-word reward (WR) on the baseline model Up-Down. Test results show that our proposed methods have obvious improvement over our baseline. B-1 / B-4 / M / R / C / S refers to BLEU1/ BLEU4 / METEOR / ROUGE-L / CIDEr / SPICE scores.",
2001.05876v3,Which model performed the best in terms of CIDEr score?,stackcap+SG+RWS+WR,2001.05876v3-Table2-1.png, Performance of our proposed methods over other state-of-the-art models after cider optimization training.,
2001.05876v3,How does the plot in the figure help to understand the recalled words?,The plot in the figure shows the weights of the recalled words in the attention mechanism. The words with higher weights are more likely to be used in the generated caption.,2001.05876v3-Figure3-1.png, The left part: recalled words and caption generated by SG+RWS+WR. The right part: visualization of the weights in recalled-word attention,
2001.05876v3,"What is the role of the ""switch"" component in the proposed recall mechanism?","The ""switch"" component combines the word probabilities computed by the semantic guide (SG) and the recalled-word slot (RWS) into a final word probability.",2001.05876v3-Figure2-1.png," An illustration of our proposed recall mechanism. There are 3 parts in this figure: base model, semantic guide (SG), and recalled-word slot (RWS). We incorporate our recall mechanism into the base model Up-Down (Anderson et al. 2018). SG and RWS compute word probability individually, then we employ a soft switch to combine them into a final word probability Pt.",
2001.05876v3,What are the differences between the captions generated by the Up-Down model and the SG+RWS+WR model for the first image?,"The Up-Down model caption states that the plane is sitting on the grass, while the SG+RWS+WR model caption states that the plane is blue and white.",2001.05876v3-Figure4-1.png," Recalled word and caption generation results on MS COCO Karpathy test split and the output sentences are generated by 1) Ground Truth (GT): one ground truth caption, 2)Up-Down model and 3) our SG+RWS+WR model.",
2001.05876v3,Which method achieved the best CIDEr score when K=5?,C,2001.05876v3-Table4-1.png," Experiments on choice of K, the number of captions retrieved for each image. B-1 / B-4 / M / R / C / S refers to BLEU1/ BLEU4 / METEOR / ROUGE-L / CIDEr / SPICE scores. Experiments are conducted on MSCOCO Karpathy validation set.",
2001.05876v3,Which λ value resulted in the highest CIDEr score?,0.5,2001.05876v3-Table5-1.png," Experiments on choice of λ, the trade-off parameter in CIDEr optimization. B-1 / B-4 / M / R / C / S refers to BLEU1/ BLEU4 / METEOR / ROUGE-L / CIDEr / SPICE scores. Experiments are conducted on MSCOCO Karpathy validation set",
2001.05876v3,What is the role of the Recall Unit in the proposed method?,The Recall Unit is a text-retrieval module that retrieves relevant words from a corpus based on the input image. These retrieved words are then used to improve the performance of the captioning model.,2001.05876v3-Figure1-1.png," An overview of our proposed methods. Recall unit is a text-retrieval module. RW-Attention denotes recalledwords attention. RWS represents recalled-words slot. SG is semantic guide. In short, we introduce a recall unit to the traditional captioning model, employing recalled words to boost the performance of captioning.",
2001.09954v1,"Which of the following statements is supported by the figure?

A. The AUC increases with the size of the training data.
B. The AUC increases with the lexical heterogeneity of the expressions used to express a dimension.
C. The AUC is not affected by the size of the training data.
D. The AUC is not affected by the lexical homogeneity of the expressions used to express a dimension.",A. The AUC increases with the size of the training data.,2001.09954v1-Figure4-1.png," AUC increases with the size of the training data (left) and with the lexical homogeneity of the expressions used to express a dimension, estimated with average similarity in the embedding space (right).",
2001.09954v1,Which model performs the best for the dimension of Romance?,The model that performs the best for the dimension of Romance is the LSTM model.,2001.09954v1-Table4-1.png, Performance of differentmodels on each dimension for the Reddit dataset (average AUC over 10-fold cross validation). Top performances are highlighted in bold.,
2001.09954v1,What are the features that are used to measure trust?,"The features used to measure trust are:
- LIWC (posemo, affect)
- VADER (positive)
- Empath (friends, help, trust)
- Style (empathy words)",2001.09954v1-Table5-1.png, Important feature groups per dimension in the Xgboost classifier (Cohen’s d > 0.4),
2001.09954v1,"Which social dimension of relationships is most closely associated with the keywords ""trustworthy, honest, reliable, dependability, loyalty, faith""?",Trust.,2001.09954v1-Table1-1.png," The ten social dimensions of relationships studied by decades of research in the social sciences. The keywords are the most popular terms used by people to describe those dimensions, according to Deri at al. [40]’s survey.",
2001.09954v1,What relationship types are easier to predict than others?,Knowledge and conflict relationship types are easier to predict than others.,2001.09954v1-Figure5-1.png, Left: AUC of LSTM models trained on the Reddit data and tested on the other datasets. Right: growth of AUC in the classification of Twitter relationships as the number of messages exchanged between the two users increases.,
2001.09954v1,Which of the social dimensions shows the most significant decline after the first financial concerns are raised?,Status,2001.09954v1-Figure6-1.png," How the presence of five social dimensions in Enron employees emails changes over time, compared to a sentiment analysis baseline. Status giving, knowledge transfer, and the power-based exchanges plummet after the first financial concerns. After massive layoffs, the remaining employees give support to each other.",
2001.09954v1,Which of the ten dimensions has the strongest association with income?,Knowledge,2001.09954v1-Table7-1.png," Linear regressions that predict real-world outcomes (education, income, suicide rate) at US-State level from the presence of the 10 dimensions in the conversations among Reddit users residing in those States. Population density is added as a control variable. Absolute R2ad j increments of the full models over the density-only models are reported in parenthesis.",
2001.09954v1,"Which social dimension is most predictive of education, income, and suicide rate in the US?","Knowledge is most predictive of education and income, while support is most predictive of suicide rate.",2001.09954v1-Figure7-1.png," Linear relationships between each US-State outcome variable (education, income, suicide rate) and its most predictive social dimension (min-max normalized). Plots are annotated with a few representative US States.",
2001.09954v1, How does the Tinghy game represent the relationship between two people? ," The Tinghy game represents the relationship between two people by allowing the player to select blocks that represent different dimensions of their relationship, such as similarity, trust, romance, and conflict. These blocks are then added to a ""friendship wall"" that visually depicts the relationship.",2001.09954v1-Figure1-1.png," Anonymized screenshot of the Tinghy game. The player (bottom profile picture) is presented with 10 Twitter friends, one at the time (top profile picture) and is asked to describe their relationship by picking 1 to 3 dimensions from themenu on the left. By doing so, new blocks are added to the “friendship wall” in the middle. The dimensions are explained to the player with short text snippets.",
2001.09954v1,What type of social expression is conveyed in the highlighted sentence?,The highlighted sentence conveys a combination of social support and similarity.,2001.09954v1-Figure2-1.png, Example of the crowdsourcing task. The highlighted sentence conveys a combination of social support and similarity.,
2001.09954v1,Which feature family has the most features?,Linguistic lexicons.,2001.09954v1-Table2-1.png, Interpretable linguistic features for classification,
2001.09954v1,"Which movie quote has the highest confidence score for the ""knowledge"" dimension?","""Only a fully trained Jedi Knight, with The Force as his ally, will conquer Vader and his Emperor. If you end your training now, if you choose the quick and easy path, as Vader did, you will become an agent of evil."" - Ben Kenobi, Star Wars Ep.5",2001.09954v1-Table6-1.png," The social dimensions in movie scripts. The three quotes with highest confidence score for each dimension are reported. For each quote, on the right, we report the histogram of the classifier confidence scores for all dimensions, and a horizontal line that marks a level of confidence of 0.5.",
2001.09954v1,Which data set has the highest percentage of messages labeled with one dimension?,Reddit,2001.09954v1-Table3-1.png, Fraction of messages labeled with n numbers of dimensions from the annotators,
2001.09954v1,"Which dataset has the highest number of occurrences of the label ""conflict""?",Twitter.,2001.09954v1-Figure3-1.png, Distributions of labels across datasets.,
2002.08037v3,Which method appears to learn the fastest in each plot?,"In plot (a), PTF-A3C appears to learn the fastest. In plot (b), PTF-PPO appears to learn the fastest. In plot (c), Deep-CAPS appears to learn the fastest.",2002.08037v3-Figure4-1.png, Average discounted rewards of various methods when learning task g′ on grid world W .,
2002.08037v3,Which method has the highest average discounted reward?,PTF-A3C,2002.08037v3-Figure3-1.png, Average discounted rewards of various methods when learning task g on grid world W .,
2002.08037v3,What is the learning rate for the policy network?,3e-4,2002.08037v3-Table4-1.png, PTF Hyperparameters.,
2002.08037v3,What is the learning rate used in the PPO algorithm?,The learning rate used in the PPO algorithm is 3e-4.,2002.08037v3-Table3-1.png, PPO Hyperparameters.,
2002.08037v3,What is the learning rate used in the CAPS algorithm?,The learning rate used in the CAPS algorithm is 3e-4.,2002.08037v3-Table1-1.png, CAPS Hyperparameters.,
2002.08037v3,What is the role of the discount factor in the A3C algorithm?,The discount factor controls how much weight is given to future rewards in the agent's decision-making process.,2002.08037v3-Table2-1.png, A3C Hyperparameters.,
2002.08037v3,Which of the methods is the most effective in terms of average discounted rewards?,PTF-A3C is the most effective method in terms of average discounted rewards.,2002.08037v3-Figure5-1.png, Average discounted rewards of various methods when learning task g′ on grid world W ′.,
2002.08037v3,Which algorithm performed better in the grid world W with imperfect source policies?,Deep-CAPS,2002.08037v3-Figure6-1.png, The performance of PTF-A3C and deep-CAPS on grid world W with imperfect source policies.,
2002.08037v3,What are the two evaluation environments with continuous control?,Pinball and Reacher.,2002.08037v3-Figure7-1.png, Two evaluation environments with continuous control.,
2002.08037v3,What are the three main modules of the policy transfer framework?,"The three main modules of the policy transfer framework are the agent module, the option module, and the policy transfer function (PTF).",2002.08037v3-Figure1-1.png, An illustration of the policy transfer framework.,
2002.08037v3,Which method has the highest average discounted reward?,A3C,2002.08037v3-Figure8-1.png, Average discounted rewards of various methods when learning g on pinball.,
2002.08037v3,"How does the weighting factor f(βo, t) affect the performance of the PTF-A3C algorithm?","The weighting factor f(βo, t) improves the performance of the PTF-A3C algorithm.",2002.08037v3-Figure11-1.png," The influence of weighting factor f(βo, t).",
2002.08037v3,Which of the methods compared in the figure has the highest average return after 2 million training episodes?,PTF-A3C,2002.08037v3-Figure9-1.png, Average discounted rewards of various methods when learning g′ on pinball.,
2002.08037v3,How does the performance of PTF-A3C compare to the performance of A3C?,PTF-A3C performs slightly better than A3C.,2002.08037v3-Figure10-1.png, The performance of PTF on Reacher.,
2002.08037v3,Which algorithm resulted in the highest switch frequency?,PTF-PPO,2002.08037v3-Figure12-1.png, The switch frequency of options.,
2002.09286v1,What is the role of the RNN in the proposed model?,The RNN is used to predict the mask.,2002.09286v1-Figure4-1.png, This block diagram shows the full pipeline for the proposed model. Inside the solid black box are the operations which are trained when using a fixed transform. The dashed boxes contain the additional operations trained in our setup.,
2002.09286v1,How does the connectivity structure of the DFT differ from that of the FFT?,"The DFT has a dense connectivity structure, meaning that each input is connected to every output. In contrast, the FFT has a sparser connectivity structure, meaning that each input is only connected to a few outputs.",2002.09286v1-Figure2-1.png, On the left we show the connectivity pattern enforced by a DFT matrix. It is a dense linear layer since the DFT matrix application can be represented by a single dense matrix multiply. On the right we show the sparser connectivity pattern and bit-reversal enforced by the DIT-FFT algorithm. The FFT connectivity structure makes all matrices in the FFT factorization trainable except for the bit-reversal permutation matrix.,
2002.09286v1,How does the learned window for the analysis window compare to the initialization?,"The learned window for the analysis window is more complex than the initialization. The initialization is a smooth curve, while the learned window has a lot of high-frequency variation.",2002.09286v1-Figure3-1.png," The left plot contains the analysis window. It has fairly regular high frequency patterns. The right plot shows the synthesis window. Interestingly, it is two peaked. Both windows have changed considerably from their intializations.",
2002.09286v1,What is the purpose of the linear layers in the masking network?,The linear layers are used to transform the input data into a form that is more suitable for the GRU layer.,2002.09286v1-Figure5-1.png," The masking network is composed of three layers: two linear layers, and a unidirectional gated recurrent unit layer. This network is causal for real-time speech enhancement. The real and imaginary components of the transformed input are stacked before being fed through the network. Similarly, the output is interpreted as the real mask stacked on top of the imaginary mask.",
2002.09286v1,Which front-end setup performed the best in terms of the perceptual metric C_sig?,The front-end setup with a trainable window and a non-trainable FFT performed the best in terms of the perceptual metric C_sig.,2002.09286v1-Table1-1.png," Comparison of speech enhancement performance on the VCTK test set using several perceptual metrics. We compare performance between several front-end setups. For the trainable window/FFT attributes we use Xwhen this attribute was trainable and× when it was not. In this results table we include the loss as defined in the training section. For the loss, lower is better. Finally, the best score for each metric is displayed in bold.",
2002.09286v1,What is the relationship between the matrices $W_m$ and $W_{m-1}$?,The matrix $W_{m-1}$ is obtained from the matrix $W_m$ by removing the first row and column.,2002.09286v1-Figure1-1.png," The DIT-FFT algorithm computes the Fourier transform as a series of sparse matrix multiplies. This figure demonstrates the structure involved in these matrices. The solid diagonal lines indicate the positions of the non-zero elements in the matrices, and the grey squares represent 2× 2 DFT matrices.",
2002.09758v3,What are the two subquestions that the model decomposes the original question into?,SQ1: What profession does H. L. Mencken have? and SQ2: Who was Albert Camus?,2002.09758v3-Figure1-1.png," Overview: Using unsupervised learning, we decompose a multi-hop question into single-hop subquestions, whose predicted answers are given to a recomposition model to predict the final answer.",
2002.09758v3,How does the use of decompositions affect the QA F1 score for different types of HOTPOTQA questions?,The use of decompositions generally improves the QA F1 score for all four types of HOTPOTQA questions.,2002.09758v3-Table2-1.png," Left: Decompositions improve QA F1 for all 4 HOTPOTQA types. Right (Ablation): QA model F1 when trained with various sub-answers: the sentence of the predicted sub-answer, predicted sub-answer span, or random entity from the context. We also train models with (X) or without (7) sub-questions/sub-answers.",
2002.09758v3,Which model performs best when the single-hop QA model answers with the ground truth “supporting fact” sentences?,The Multi-hop Dev model.,2002.09758v3-Figure3-1.png, Multi-hop QA is better when the single-hop QA model answers with the ground truth “supporting fact” sentences. We plot mean and std. over 5 QA runs.,
2002.09758v3,Which development set has the largest difference in performance between the model that uses ONUS decompositions and the one that does not?,The Out-of-Domain Dev set.,2002.09758v3-Figure7-1.png," Performance difference between a QA model that does vs. does not use ONUS decompositions, stratified by whether the gold final answer is in a subanswer sentence. We find a larger gain when the subanswer sentence contains the gold, final answer.",
2002.09758v3,What is the purpose of the ONUS model?,The ONUS model aims to learn to map multi-hop questions to decompositions of sub-questions.,2002.09758v3-Figure2-1.png," One-to-N Unsupervised Sequence transduction (ONUS): Step 1: We create a corpus of pseudodecompositions D by finding candidate sub-questions from a simple question corpus S which are similar to a multi-hop question in Q. Step 2: We learn to map multi-hop questions to decompositions using Q and D as training data, via either standard sequence-to-sequence learning (Seq2Seq) or unsupervised sequence-to-sequence learning (for ONUS).",
2002.09758v3,Which decomposition method resulted in questions that were more similar to the multi-hop questions in terms of length?,DecompRC,2002.09758v3-Table4-1.png," Analysis of sub-questions produced by our method vs. the supervised+heuristic method of Min et al. (2019b). Left-to-right: Negative Log-Likelihood according to GPT2 (lower is better), % classified as Well-Formed, Edit Distance between decomposition and multi-hop question, and token-wise Length Ratio between decomposition and multi-hop question.",
2002.09758v3,Are both Coldplay and Pierre Bouvier from the same country?,No,2002.09758v3-Table3-1.png," Example sub-questions generated by our model, along with predicted sub-answer sentences (answer span underlined) and final predicted answer.",
2002.09758v3,"How does the addition of UDs and 1hop-Train affect the QA F1 score of the downstream, recomposition model?","The addition of UDs and 1hop-Train increases the QA F1 score of the downstream, recomposition model.",2002.09758v3-Figure6-1.png," QA F1 of the downstream, recomposition model, with and without unsupervised decompositions (UDs), when varying the amount of training data. We also assess the impact of removing single-hop training data (SQUAD 2.0 and HOTPOTQA“easy” questions).",
2002.09758v3,What is the effect of using decompositions on the performance of the multi-hop QA model?,"The use of decompositions generally improves the performance of the multi-hop QA model, as evidenced by the higher F1 scores achieved by the model with decompositions compared to the model without decompositions.",2002.09758v3-Figure5-1.png," How multi-hop QA accuracy varies over the course of decomposition model training, for one training run of ONUS on FastText pseudo-decompositions. Our unsupervised stopping criterion selects the epoch 3 checkpoint, which performs roughly as well as the best checkpoint (epoch 5).",
2002.09758v3,How does the performance of the recomposition model vary with the confidence of the sub-answers?,"The performance of the recomposition model is higher when the single-hop QA model places high probability on its sub-answer. This is shown in the right panel of the figure, where the F1 score of the recomposition model is higher for sub-answer confidence ranks of 80-100% than for lower confidence ranks.",2002.09758v3-Figure4-1.png, Left: We decode decompositions with beam search and use nth-ranked hypothesis as a question decomposition. We plot the F1 of a recomposition model trained to use the nth-ranked decomposition. Right: Multi-hop QA is better when the single-hop QA model places high probability on its sub-answer.,
2002.09758v3,Which model benefits the most from decomposition?,ROBERTA_LARGE,2002.09758v3-Table5-1.png, Better models gain more from decomposition.,
2002.09758v3,Which decomposition method generated the most sub-questions?,PseudoD + Random,2002.09758v3-Table8-1.png," Various decomposition methods for the question “What is the name of the singer who’s song was released as the lead single from the album “Confessions,” and that had popular song stuck behind for 8 consecutive weeks?” Here, the Variable-Length ONUS model decomposed the question into three subquestions rather than two.",
2002.09758v3,Which decomposition method produces the most coherent sub-questions?,"This is subjective, but some might argue that DecompRC produces the most coherent sub-questions, as they are both directly related to the original question and are clear and concise.",2002.09758v3-Table9-1.png, Various decomposition methods for: “Are both Coldplay and Pierre Bouvier from the same country?”,
2002.09758v3,What is the difference between the DecompRC and PseudoD + Random decomposition methods?,"DecompRC decomposes the question into two sub-questions, while PseudoD + Random decomposes the question into two different sub-questions.",2002.09758v3-Table10-1.png," Various decomposition methods for: “Who is older, Annie Morton or Terry Richardson?”",
2002.09758v3,Which decomposition method and pseudo-decomposition retrieval method combination resulted in the highest QA F1 score on the Dev set?,The combination of SAE decomposition method and HGN pseudo-decomposition retrieval method resulted in the highest QA F1 score of 82.2 on the Dev set.,2002.09758v3-Table6-1.png, QA F1 scores for all combinations of learning methods and pseudo-decomposition retrieval methods that we tried.,
2002.09758v3,Which decomposition method achieves the highest F1 score on the HOTPOTQA Dev F1 OOD set?,ONUS with FastText achieves the highest F1 score on the HOTPOTQA Dev F1 OOD set.,2002.09758v3-Table1-1.png," Unsupervised decompositions significantly improve F1 on HOTPOTQA over the baseline and single-hop QA model used to answer sub-questions (“1hop”). On all dev sets and the test set, we achieve similar F1 to methods that use supporting fact supervision (†). (*) We test supervised/heuristic decompositions from Min et al. (2019b). (‡) Scores are approximate due to mismatched Wikipedia dumps.",
2002.09758v3,What is the main difference between the FEVER 1.0 and FEVER 2.0 datasets?,The FEVER 1.0 dataset contains simple questions that can be answered by looking up a single fact in a knowledge base. The FEVER 2.0 dataset contains more complex questions that require reasoning over multiple facts.,2002.09758v3-Table7-1.png," Zero-shot Unsupervised Decompositions of questions or claims from other datasets using our ONUS model trained on HOTPOTQA and Common Crawl questions (without further, dataset-specific fine-tuning).",
2003.06594v1,What is the purpose of the orange boxes in the image?,"The orange boxes represent traffic actors, such as vehicles or pedestrians.",2003.06594v1-Figure1-1.png," Neural motion massage passing is proposed to capture directed interactions among traffic actors, which can serve as a plugin module to guide motion prediction.",
2003.06594v1,Which method has the lowest ADE/FDE for pedestrians?,JMP-NMMMP,2003.06594v1-Table3-1.png," Quantitative comparison on NuScenes dataset. Error metrics reported are ADE/FDE in meters. Given previous 5 (0.5s), predicting the future 30 (3s).",
2003.06594v1,Which method performed the best on the HOTEL dataset?,PMP-NMMP,2003.06594v1-Table1-1.png," Quantitative results on ETH-UCY dataset. We present ADE/FDE in meters. Given previous 8 (3.2s), predicting future 12 (4.8s).",
2003.06594v1,Which method has the lowest ADE and FDE?,PMP-NMP,2003.06594v1-Table2-1.png, Quantitative results on Stanford Drone dataset. ADE and FDE are reported in pixels.,
2003.06594v1,How can you tell the difference between the SDV and the other vehicles in the image?,"The SDV is represented by a red box, while the other vehicles are represented by yellow boxes.",2003.06594v1-Figure9-1.png," Rasterized scene images. The red box is the SDV, the yellow boxes are the vehicles, and the green dots are the pedestrians. As time goes back, the brightness of the boxes darkens, resulting in the fading tails.",
2003.06594v1,What does the close proximity of the red and blue dots in the first column of the figure indicate?,The close proximity of the red and blue dots in the first column of the figure indicates that the corresponding interactions are similar in the embedding domain.,2003.06594v1-Figure10-1.png," The first column is the t-SNE visualization of interaction embeddings, where each dot represents an interaction and correspond to spatial-domain plots. The close red and blue dots in the first column represent close interaction embeddings in the embedding domain. The right two columns are the corresponding traffic scenes. And the middle column is the corresponding spatial plot of the red dot. The trajectories of the interacted traffic actors are colored in red. Similarly the right column presents spatial plot of the blue dot.",
2003.06594v1,How does the model's prediction of pedestrian trajectory compare to the ground truth and other methods?,The model's prediction of pedestrian trajectory is more accurate than SGAN and closer to the ground truth than both SGAN and GT_Pred.,2003.06594v1-Figure5-1.png, Predicted trajectory comparison in the pedestrian setting. The baseline is SGAN [11].,
2003.06594v1,"Which of the two metrics, ADE or FDE, is more sensitive to the inclusion of the NMMP module?",FDE.,2003.06594v1-Table4-1.png, We evaluate the NMMP module by applying it in multiple trajectory encoding phases in generator (G) and discriminator (D). Xand - means with and without NMMP module respectively. ADE/FDE are reported in meters.,
2003.06594v1,How does the performance of NMMP change as the number of iterations increases?,The performance of NMMP generally decreases as the number of iterations increases.,2003.06594v1-Table5-1.png, The performance as a function of the number of iterations in NMMP (K) on UNIV set.,
2003.06594v1," 

What is the purpose of this figure? "," 

The figure compares the predicted trajectory of a moving object using different methods. The green dashed line shows the ground truth trajectory, the red dashed line shows the trajectory predicted by the proposed method, and the blue dashed line shows the trajectory predicted by the baseline method.",2003.06594v1-Figure6-1.png, Predicted trajectory comparison in the joint setting. The baseline is ALEXNET [8].,
2003.06594v1,What is the effect of using a double decoder on the UNIV set?,"Using a double decoder generally leads to better results, as indicated by the lower ADE/FDE values.",2003.06594v1-Table6-1.png, The exploration of the decoder design on the UNIV set. ADE/FDE are reported in meters.,
2003.06594v1, What is the role of the LSTM module in the NMMP architecture?, The LSTM module is responsible for learning the trajectory embedding for each actor.,2003.06594v1-Figure2-1.png," The NMMP module explicitly learns trajectory embedding for each actor through LSTM, and interacted actor embedding and in-between interaction embedding through message passing.",
2003.06594v1,Which metric has a higher error normalization ratio when the pedestrian amount is around 20?,ADE,2003.06594v1-Figure8-1.png, The ADE and FDE normalization ratio over pedestrian amounts in the scene. The x-axis is the total pedestrian amount in the scene. The y-axis is the ADE (Red) and FDE (Blue) normalization ratio compared with SGAN [11].,
2003.06594v1,What is the relationship between the interaction embeddings and the trajectory pairs?,Close interaction embeddings in the embedding domain lead to similar trajectory relations between the corresponding actor pairs in the spatial domain.,2003.06594v1-Figure7-1.png," Close interaction embeddings in the embedding domain lead to similar trajectory relations between the corresponding actor pairs in the spatial domain. The first column is the visualization of interaction embeddings, where each dot represents an interaction and corresponds to one spatial-domain plot in the right columns.",
2003.06594v1,What is the purpose of the discriminator in the PMP-NMMP system?,The discriminator is responsible for classifying the complete trajectories as real or fake.,2003.06594v1-Figure3-1.png, The pedestrian motion prediction system based on NMMP (PMP-NMMP) consists of a generator and a discriminator. The generator predicts future trajectories from two branches: the individual branch considers the individual trajectory embedding and the interactive branch considers the interacted actor embedding. The discriminator classifies the complete trajectories to be real or fake.,
2003.06594v1,What is the difference between the individual branch and the interactive branch of the JMP-NMMP system?,"The individual branch considers the individual behavior of each actor, while the interactive branch considers the interactions among actors.",2003.06594v1-Figure4-1.png," The joint pedestrian and vehicle motion prediction system based on NMMP (JMP-NMMP) predicts the future trajectories from two branches: the individual branch considers the individual behavior of each actor, and the interactive branch considers the interactions among actors.",
2004.01398v1,Which of the blocks in the figure includes a residual connection?,"Blocks a, b, d, and e.",2004.01398v1-Figure4-1.png, The altered blocks of different baselines based on standard ResNet block [15].,
2004.01398v1,Which method achieves the highest Top-1 accuracy on Something-Something?,TEA achieves the highest Top-1 accuracy on Something-Something.,2004.01398v1-Table1-1.png, Comparison results on Something-Something.,
2004.01398v1,What is the accuracy of the STM model?,The accuracy of the STM model is 77.5%.,2004.01398v1-Table9-1.png,Table 9 in [22].,
2004.01398v1,Which method performs best on the Something Something V2 dataset according to the table?,TEA.,2004.01398v1-Table8-1.png, Comparison results on Something Something V2.,
2004.01398v1,Which method achieves the highest Top-1 accuracy on the test set?,"TEA (Ours) with 16 × 1 × 10 frames, crops, and clips.",2004.01398v1-Table2-1.png, Comparison results of TEA with other state-of-the-art methods on Something-Something V1.,
2004.01398v1,Which stage of the ResNet50 backbone has the highest Top-1 accuracy on the Something Something V1 dataset?,conv5,2004.01398v1-Table7-1.png, Comparison results on Something Something V1.,
2004.01398v1,Which method has the highest Val Top-1 accuracy?,TEA,2004.01398v1-Table6-1.png, Comparison results on Something-Something V1.,
2004.01398v1,What is the role of the ME and MTA modules in the proposed method?,The ME and MTA modules are inserted into each ResNet block to form the TEA block. The TEA block is used to extract features from the video frames.,2004.01398v1-Figure1-1.png," The framework of the proposed method for action recognition. The sparse sampling strategy [44] is adopted to sample T frames from videos. The 2D ResNet [15] is utilized as the backbone, and the ME and MTA modules are inserted into each ResNet block to form the TEA block. The simple temporal pooling is applied to average action predictions for the entire video.",
2004.01398v1,What is the purpose of the Motion Excitation (ME) module?,The ME module aims to capture the temporal dynamics of the input video by calculating the difference between consecutive frames.,2004.01398v1-Figure2-1.png, The implementations of the motion excitation (ME) module (left panel) and multiple temporal aggregation (MTA) module (right panel).,
2004.01398v1,Which method achieved the highest MCA on UCF101?,TEA achieved the highest MCA on UCF101.,2004.01398v1-Table4-1.png, Comparison results on HMDB51 and UCF101.,
2004.01398v1,Which method achieves the highest Top-1 accuracy on the Kinetics400 validation set?,NL SlowFast,2004.01398v1-Table3-1.png, Comparison results of TEA with other state-of-the-art methods on Kinetics400 validation set.,
2004.01398v1,Which method achieved the highest Top-1 accuracy on the Something-Something dataset?,(2+1)D ResNet-Shift (Ours),2004.01398v1-Table5-1.png," Comparison results on Something-Something. All the models are trained with 8 input frames, and the one clip-one crop protocol is utilized for inference.",
2004.01398v1,How does the TEA block differ from the ResNet block?,The TEA block replaces the 3x3 convolution layer in the ResNet block with an MTA module and adds an ME module after the first 1x1 convolution layer.,2004.01398v1-Figure3-1.png, The motion excitation (ME) module is placed after the first 1×1 convolution layer. The multiple temporal aggregation (MTA) module is utilized to replace the 3×3 convolution layer.,
2004.08055v1,What is the effect of using a global structure module on the performance of the model?,"The global structure module generally improves the performance of the model, as can be seen by the higher M-Accu and M-IoU values for the models that use it.",2004.08055v1-Table1-1.png," Semi-supervised experiments in LIP dataset. DS denotes the labeled data size. R, G, and L respectively represent the retraining strategy, global structure module, and local consistency module. IoU.I. denotes mean IoU increased, and IoU.D. denotes the difference between the value here and the value of using whole labeled dataset.",
2004.08055v1,What are the two types of errors that are commonly seen in human parsing?,Global structure errors and local consistency errors.,2004.08055v1-Figure1-1.png," The left column shows our self-learning process. The sub-figures (a), (b), (c) respectively denote the partially labeled images, the predicted masks, and the rectified high-quality pseudo-labels. The right columns show the common errors in human parsing, including global structure errors (d) and local consistency errors (e). We propose the retraining strategy with our rectification framework to correct the predicted errors in segmentation network, and retrain the network with labeled data and the generated high-quality pseudo-labeled samples.",
2004.08055v1,Which base algorithm performs the best on the LIP dataset according to the table?,CE2P,2004.08055v1-Table2-1.png," Ablation experiments in LIP dataset for different base algorithms. G, and L respectively represent the GSM, and LCM.",
2004.08055v1, What are the two types of errors that the rectification network is designed to correct?, The two types of errors that the rectification network is designed to correct are global structure errors and local consistency errors.,2004.08055v1-Figure2-1.png, Illustration of our rectification network. We concatenate image and predicted mask as input for the rectification network. Then we perform the global structure and local consistency rectification for the two types of errors. The S-Net denotes the segmentation network.,
2004.08055v1,Which method achieved the highest mIoU on the LIP dataset?,HRNet,2004.08055v1-Table4-1.png, Module effectiveness experiments in LIP dataset,
2004.08055v1,How does the rectification process improve the predicted masks?,The rectification process improves the predicted masks by reducing the errors in the segmentation of body parts. This can be seen in the figure by comparing the predicted masks without rectification to the predicted masks with rectification. The predicted masks with rectification are more accurate and have fewer errors.,2004.08055v1-Figure3-1.png," Visualization of the predicted masks with rectification. The rows correspond to the original images, the predicted masks without rectification (the errors are highlighted), the attention maps for rectification, the predicted masks with rectification, and the ground truth .",
2004.12406v2,Which model and training method combination achieves the best performance on the RTE task?,RoBERTa with masking.,2004.12406v2-Table1-1.png," Dev set task performances (%) of masking and finetuning. Each experiment is repeated four times with different random seeds and we report mean and standard deviation. Numbers below dataset name (second row) are the size of training set. For POS and NER, we report the number of sentences.",
2004.12406v2,What is the difference between the bottom-up and top-down masking setups?,"The bottom-up masking setup masks the first n transformer blocks, while the top-down masking setup masks the last n transformer blocks.",2004.12406v2-Figure2-1.png," The impact of masking different transformer blocks of BERT for MRPC (left), CoLA (middle), and RTE (right). The number of masked blocks is shown on the x-axis; that number is either masked “bottom-up” or “top-down”. More precisely, a bottom-up setup (red) masking 4 blocks means we mask the transformer blocks {0, 1, 2, 3}; a top-down setup (blue) masking 4 blocks means we mask the transformer blocks {8, 9, 10, 11}. WP and WT are always masked.",
2004.12406v2,"Which approach requires more memory to solve an increasing number of tasks, finetuning or masking?",Finetuning requires more memory to solve an increasing number of tasks than masking.,2004.12406v2-Figure3-1.png, The accumulated number of parameters and memory required by finetuning and masking to solve an increasing number of tasks.,
2004.12406v2,"Which method, finetuning or masking, performed better for BERT on the QNLI task?",Finetuning performed better for BERT on the QNLI task.,2004.12406v2-Table4-1.png, The optimal learning rate on different tasks for BERT/RoBERTa/DistilBERT. We perform finetuning/masking on all tasks for 10 epochs with early stopping of 2 epochs.,
2004.12406v2,Which task has the largest number of training examples?,POS,2004.12406v2-Table5-1.png," Number of examples in dev and test per task. For POS and NER, we report the number of words.",
2004.12406v2,Which model and training method achieved the lowest error rate on the POS task?,DistilBERT with ensemble training achieved the lowest error rate on the POS task with 2.29%.,2004.12406v2-Table6-1.png, Error rate (%) on test set of tasks by RoBERTa and DistilBERT. Single: the averaged performance of four models with different random seeds. Ensem.: ensemble of the four models.,
2004.12406v2,"What is the relationship between the accuracy of BERT and RoBERTa on the MRPC dev set and the point on the curves φθ(γ), connecting the two minima found by finetuning (left, γ=0) and masking (right, γ=1)?","The accuracy of both BERT and RoBERTa increases as the point on the curves φθ(γ) moves from left to right, indicating that masking is more effective than finetuning for improving accuracy.",2004.12406v2-Figure7-1.png," The accuracy on MRPC dev set, as a function of the point on the curves φθ(γ), connecting the two minima found by finetuning (left, γ=0) and masking (right, γ=1).",
2004.12406v2,Which model has the most distinct clusters of positive and negative sentiment?,The masked ROBERTA-SST2 model has the most distinct clusters of positive and negative sentiment.,2004.12406v2-Figure4-1.png," t-SNE visualization of the representation of [CLS] computed by the topmost transformer block in pretrained (left), finetuned (top right), and masked (bottom right) BERT/RoBERTa. We use scikit-learn (Pedregosa et al., 2011) and default t-SNE parameters.",
2004.12406v2,How does masking affect the performance of BERT on the SST2 dataset?,Masking improves the performance of BERT on the SST2 dataset by 41.8%.,2004.12406v2-Table3-1.png, Generalization on dev (%) of binary masked and finetuned BERT. Row: training dataset; Column: evaluating dataset. Numbers are improvements against the majority-vote baseline: 50.9 for SST2 and 74.4 for SEM. Results are averaged across four random seeds.,
2004.12406v2,Which method performs better in terms of error rate?,The Masking method performs better than the Fine-tuning method in terms of error rate.,2004.12406v2-Table2-1.png, Error rate (%) on test set and model size comparison. Single: the averaged performance of four models with different random seeds. Ensem.: ensemble of the four models.,
2004.12406v2,Which task results in masks that are more similar across different layers of the transformer?,The task corresponding to the left figure results in masks that are more similar across different layers.,2004.12406v2-Figure5-1.png," Scores s of two sets of masks, trained with two different tasks, of layer WO in transformer blocks 2 (left) and 11 (right) in BERT. A large s means that the two masks are dissimilar.",
2004.12406v2,How does the accuracy of BERT and RoBERTa compare on the MRPC and SST2 datasets?,"BERT and RoBERTa have similar accuracy on the MRPC dataset, but RoBERTa has slightly higher accuracy on the SST2 dataset.",2004.12406v2-Figure6-1.png, Mode connectivity results on MRPC (left) and SST2 (right). Top images: dev set accuracy of an interpolated model between the two minima found by finetuning (γ=0) and masking (γ=1). Bottom images: accuracy of an interpolated model between pretrained (γ=0) and finetuned/masked (γ=1) BERT.,
2004.12406v2,How does the performance of the RTE task compare to the performance of the SST2 task as the initial mask sparsity increases?,The performance of the RTE task decreases more rapidly than the performance of the SST2 task as the initial mask sparsity increases.,2004.12406v2-Figure1-1.png, Dev set performance of masking BERT when selecting different amounts of pretrained parameters.,
2004.12406v2,Which method and sparsity level performs best on the CoLA task?,"Masking (BERT 00-07 + classifier, initial sparsity 5%)",2004.12406v2-Table7-1.png, Numerical value of the layer-wise behavior experiment. We train for 10 epochs with mini-batch size 32. The learning rate is finetuned using the mean results on four different random seeds.,
2004.12406v2,Which model has the smallest memory footprint when fine-tuning?,The MRPC model has the smallest memory footprint when fine-tuning.,2004.12406v2-Table8-1.png, Model size comparison when applying masking and finetuning. Numbers are based on BERT-baseuncased. Note that our masking scheme enables sharing parameters across tasks: tasks with the same number of output dimension can use the same classifier layer.,
2004.14710v1,What is the difference between the Primal Task and the Dual Task in the proposed joint dual learning framework?,"The Primal Task takes an input x and produces an output y, while the Dual Task takes the output y and tries to reconstruct the original input x.",2004.14710v1-Figure1-1.png," Left: The proposed joint dual learning framework, which comprises Primal Cycle and Dual Cycle. The framework is agnostic to learning objectives and the algorithm is detailed in Algorithm 1. Right: In our experiments, the models for NLG and NLU are a GRU unit accompanied with a fully-connected layer.",
2004.14710v1,Which learning scheme achieves the highest ROUGE-L score?,"The learning scheme (f) + RL_end(BLEU+ROUGE, F1) achieves the highest ROUGE-L score of 40.28%.",2004.14710v1-Table1-1.png," The NLU performance reported on micro-F1 and the NLG performance reported on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L of models (%).",
2004.14710v1,What is the difference between the baseline and proposed models?,"The baseline model only outputs the family-friendly and food type attributes of the restaurant, while the proposed model also outputs the area, eatType, priceRange, and name attributes.",2004.14710v1-Table3-1.png," An example of Dual Cycle, where the baseline model is row (a) in Table 1.",
2004.14710v1,What is the difference between the baseline and proposed methods for generating text from images?,"The baseline method generates text that describes the image in detail, while the proposed method generates text that is more concise and focuses on the most important aspects of the image.",2004.14710v1-Table2-1.png," An example of Primal Cycle, where the baseline model is row (a) in Table 1.",
2005.02991v2,Which individuals are both peppers and inside the orange line?,"Individuals 1, 3, 5, 7, 8, 11, 12, and 14 are both peppers and inside the orange line.",2005.02991v2-Figure1-1.png," An example model structure with 14 individuals. Subscripts distinguish individuals with identical features, but are otherwise arbitrary. The pepper predicate is true of individuals inside the orange line, but the positions of individuals are otherwise arbitrary.",
2005.02991v2,Which nodes in the figure represent observed data and which nodes represent latent variables?,"The nodes X, Y, and Z represent observed data, while the nodes e(p), e(q), e(r), and Ta, Y represent latent variables.",2005.02991v2-Figure5-1.png," An example of logical inference, building on Fig. 4. Given an observed semantic dependency graph (here, with three nodes, like Fig. 2, with predicates p, q, r), we would like to know if some predicate is true of some latent individual (here, if a is true of Y ). We can apply the inference network to infer distributions for the pixie nodes, and then apply a semantic function to a pixie node (here, the function for a applied to Y ).",
2005.02991v2,Which model performed the best on the RELPRON development set?,The Pixie Autoencoder & vector addition ensemble model performed the best on the RELPRON development set with a MAP of 0.532.,2005.02991v2-Table1-1.png, Mean Average Precision (MAP) on RELPRON development and test sets.,
2005.02991v2,Which model performed the best on the GS2011 dataset when using separate annotator scores?,Pixie Autoencoder (logical inference in one direction),2005.02991v2-Table2-1.png," Spearman rank correlation on the GS2011 dataset, using separate or averaged annotator scores.",
2005.02991v2,"What is the relationship between the observed predicates p, q, r and the pixie nodes X, Y, Z?","The observed predicates p, q, r are used as input to the network to predict the posterior distribution over the pixie nodes X, Y, Z.",2005.02991v2-Figure4-1.png," Graph-convolutional inference network for Fig. 3. The aim is to predict the posterior distribution over the pixie nodes X , Y , Z, given the observed predicates p, q, r. Each edge indicates the weight matrix used in the graph convolution, as defined in (6). In the bottom row, the input at each node is an embedding for the node’s predicate. The intermediate representations h do not directly correspond to any random variables in Fig. 3. Conversely, the truth-valued random variables in Fig. 3 are not directly represented here.",
2005.08164v2,Which model performed the best on the Automotive dataset for in-segment users according to the HR@10 metric?,The DCGAN model performed the best on the Automotive dataset for in-segment users according to the HR@10 metric.,2005.08164v2-Table3-1.png," Attack performance against NNMF. Best results are marked in bold, and AUSH results are also marked in bold if they are the second best in each category.",
2005.08164v2,Which method performs best in terms of HR@10 on the FilmTrust dataset for in-segment users?,WGAN performs best in terms of HR@10 on the FilmTrust dataset for in-segment users.,2005.08164v2-Table2-1.png," Attack performance against NMF. Best results are marked in bold, and AUSH results are also marked in bold if they are the second best in each category.",
2005.08164v2,Which attack model has the best performance in terms of recall?,The AUSHi_rec attack model has the best performance in terms of recall.,2005.08164v2-Figure2-1.png, Attack detection of injected profiles on ML-100K. Lower value suggests a better attack model.,
2005.08164v2,Which dataset has the highest sparsity?,Automotive,2005.08164v2-Table1-1.png, Statistics of data,
2005.08164v2,Which model performed the best on the ML-100K dataset for all users in terms of HR@10?,WGAN,2005.08164v2-Table4-1.png," Attack performance against U-AutoEncoder. Best results are marked in bold, and AUSH results are also marked in bold if they are the second best in each category.",
2005.08164v2,Which method had the lowest JS distance between injected user profiles and real user profiles?,WGAN,2005.08164v2-Table7-1.png, Two distance measures between injected user profiles and real user profiles in ML-100K.,
2005.08164v2,"Which model performed the best on the Automotive dataset for in-segment users, according to the HR@10 metric?","The Segment model performed the best on the Automotive dataset for in-segment users, according to the HR@10 metric.",2005.08164v2-Table5-1.png," Attack performance against I-AutoEncoder. Best results aremarked in bold, and AUSH results are alsomarked in bold if they are the second best in each category.",
2005.08164v2,Which attack method performs the best according to the table?,Segment.,2005.08164v2-Table6-1.png, Attack performance on I-AutoEncoder using different sampling strategies and losses in ML-100K. Best results are marked in bold.,
2005.10481v1, What is the purpose of the dashed arrows in the figure? , The dashed arrows represent the backpropagation of the loss through the network. ,2005.10481v1-Figure1-1.png, Overview of OWS applied to a 3-layer neural network. Top: slimmable network; bottom: MRF for optimal selection of channel numbers c1 and c2.,
2005.10481v1,Which method has the highest speedup when using TensorRT?,AOWS,2005.10481v1-Table1-1.png," Run-time vs. accuracy comparison for timings obtained with batch size 64. The GPU+TRT column lists the latencies and speedups allowed by TensorRT. AOWS is obtained with Mobilenet-v1/TensorRT optimization with LT = 0.04ms, and a longer training schedule of 480 epochs.",
2005.10481v1,What is the relationship between the measured and predicted latency of the networks in the search space?,There is a strong positive correlation between the measured and predicted latency of the networks in the search space.,2005.10481v1-Figure3-1.png," Measured vs. predicted latency of 200 randomly sampled networks in our search space for the TensorRT latency model, trained using 9500 inference samples.",
2005.10481v1,What is the relationship between the proxy errors and the final errors after full training?,The proxy errors are positively correlated with the final errors. This means that networks with higher proxy errors tend to have higher final errors.,2005.10481v1-Figure4-1.png, Comparison of the slimmable proxy used by greedy validation (left) and our error estimates used in OWS (right). The proxy errors of 13 networks in our search space are compared to the final error after full training of these configurations.,
2005.10481v1,How does the top-1 error rate change as the number of epochs increases?,The top-1 error rate decreases slightly as the number of epochs increases.,2005.10481v1-Table3-1.png," Effect of the number of epochs when training AOWS, for TRT optimization under LT = 0.04ms.",
2005.10481v1,Which method has the lowest top-1 error?,AOWS,2005.10481v1-Table2-1.png, Accuracies and latencies of channel configurations found for TRT optimization with LT = 0.04ms.,
2005.10481v1,How does the channel ratio of the greedy algorithm compare to the OWS and AOWS algorithms for layers 1 through 14?,The greedy algorithm has a higher channel ratio than the OWS and AOWS algorithms for all layers except for layer 12.,2005.10481v1-Figure5-1.png," Channel configurations found, as a ratio with respect to the original channels of MobileNet-v1.",
2005.10481v1,How does increasing the latency target of AOWS affect its top-1 error rate?,Increasing the latency target of AOWS decreases its top-1 error rate.,2005.10481v1-Table5-1.png, Optimizing for CPU latency (@ indicates the latency targets),
2005.10481v1,How does the top-1 error of AutoSlim compare to that of AOWS for a fixed MFLOPs budget?,The top-1 error of AutoSlim is generally lower than that of AOWS for a fixed MFLOPs budget.,2005.10481v1-Table4-1.png, Optimizing for FLOPs,
2005.10481v1,What is the relationship between CPU latency and top-1 error?,"There is a negative relationship between CPU latency and top-1 error. As CPU latency increases, top-1 error decreases.",2005.10481v1-Figure6-1.png," Pareto front of greedy, vs. Pareto front of AOWS optimized for CPU and GPU latency models.",
2005.10481v1,What is the relationship between latency and Top-1 error for AOWS?,The Top-1 error decreases as the latency increases for AOWS.,2005.10481v1-Table6-1.png, Optimizing for GPU latency (@ indicates the latency targets),
2005.10481v1,Which method produced the configuration with the most channels?,The greedy method produced the configuration with the most channels.,2005.10481v1-TableD.2-1.png,"Table D.2: Channel configurations found in the TRT optimization (section 7.1), visualized in fig. 5, and with top-1 errors given in table 2 in the paper. Results are compared to the original Mobilenet-v1 [11] channels.",
2005.10481v1,What is the relationship between the temperature and the min-sum path?,"As the temperature decreases, the sampled configurations approach the min-sum path.",2005.10481v1-Figure2-1.png, Min-sum relaxation at temperatures T = 0.1 (left) and T = 0.001 (right). Red: min-sum path. Colored: marginal sampling probabilities. The sampled configurations approach the min-sum path as T → 0.,
2005.11643v2,"Which of the three bounding boxes (blue, red, or green) best captures the bicycle in the image?",The green bounding box best captures the bicycle in the image.,2005.11643v2-Figure1-1.png, Bicycle detection result for an image of the MSCOCO dataset. Blue box: ground truth; red box: detection result by Faster R-CNN; green box: detection result by context-aware CompositionalNet. Probability maps of three-point detection are to the right. The proposed contextaware CompositionalNet are able to detect the partially occluded object robustly.,
2005.11643v2,What is the relationship between the amount of context occlusion and the amount of object occlusion in the OccludedVehiclesDetection dataset?,"The amount of context occlusion and the amount of object occlusion are positively correlated. This means that as the amount of context occlusion increases, the amount of object occlusion also increases.",2005.11643v2-Figure6-1.png," Example of images in OccludedVehiclesDetection dataset. Each row shows increasing amounts of context occlusion, whereas each column shows increasing amounts of object occlusion.",
2005.11643v2,Which model performed the best on average across all levels of occlusion?,CA-CompNet via BBV with ω=0.,2005.11643v2-Table1-1.png," Detection results on the OccludedVehiclesDetection dataset under different levels of occlusions (BBV as in Bounding Box Voting). All models trained on PASCAL3D+ unoccluded dataset except Faster R-CNN with reg. was trained with CutOut. The results are measured by correct AP(%) @IoU0.5, which means only corrected classified images with IoU > 0.5 of first predicted bounding box are treated as true-positive. Notice with ω = 0.5, context-aware model reduces to a CompositionalNet as proposed in [18].",
2005.11643v2,Which method performs best on the heavily occluded L4 category?,CA-CompNet via BBV ω=0,2005.11643v2-Table2-1.png," Detection results on OccludedCOCO Dataset, measured by AP(%) @IoU0.5. All models are trained on PASCAL3D+ dataset, Faster R-CNN with reg. is trained with CutOut and Faster R-CNN with occ. is trained with images in same dataset but occluded by all levels of occlusion with the same set of occluders.",
2005.11643v2,Which method is the most successful at accurately localizing the object in the image?,The context-aware CompositionalNet with robust bounding box voting is the most successful at accurately localizing the object in the image.,2005.11643v2-Figure2-1.png," Object detection under occlusion with RPNs and proposed robust bounding box voting. Blue box: ground truth; red box: Faster R-CNN (RPN+VGG); yellow box: RPN+CompositionalNet; green box: context-aware CompositionalNet with robust bounding box voting. Note how the RPN-based approaches fail to localize the object, while our proposed approach can accurately localize the object.",
2005.11643v2,Which object detection model performs the best in terms of detecting objects that are partially occluded?,CA-CompositionalNet via BB Voting.,2005.11643v2-Figure8-1.png, Selected examples of detection results on OccludedCOCO Dataset. Blue box: ground truth; green box: proposals of CA-CompositionalNet via BB Voting; yellow box: proposals of CA-CompositionalNet via RPN; red box: proposals of Faster R-CNN.,
2005.11643v2,What is the purpose of the different colored boxes in the images?,The different colored boxes represent different methods of object detection.,2005.11643v2-Figure7-1.png," Selected examples of detection results on the OccludedVehiclesDetection dataset. All of these 6 images are the heaviest occluded images (foreground level 3, background level 3). Blue box: ground truth; green box: proposals of CA-CompositionalNet via BB Voting; yellow box: proposals of CA-CompositionalNet via RPN; red box: proposals of Faster R-CNN.",
2005.11643v2,How does the context affect the detection of the airplane in the image?,"The context can lead to false positives, as seen in the orange box.",2005.11643v2-Figure4-1.png, Influence of context in aeroplane detection under occlusion. Blue box: ground truth; orange box: bounding box by CompositionalNets (ω = 0.5); green box: bounding box by Context-Aware CompositionalNets (ω = 0.2). Probability maps of the object center are on the right. Note how reducing the influence of the context improves the localization response.,
2005.11643v2,Which bounding box is the most accurate?,The green bounding box.,2005.11643v2-Figure3-1.png," Example of robust bounding box voting results. Blue box: ground truth; red box: bounding box by Faster R-CNN; green box: bounding box generated by robustly combining voting results. Our proposed part-based voting mechanism generates probability maps (right) for the object center (cyan point), the top left corner (purple point) and the bottom right corner (yellow point) of the bounding box.",
2005.11643v2,What is the relationship between the images in the top row and the images in the bottom row?,"The images in the top row are the original images, and the images in the bottom row are the segmentation masks for those images. The segmentation masks show which pixels in the image belong to the object of interest (in this case, the airplane, car, and bicycle).",2005.11643v2-Figure5-1.png, Context segmentation results. A standard CompositionalNet learns a joint representation of the image including the context. Our context-aware CompositionalNet will disentangle the representation of the context from that of the object based on the illustrated segmentation masks.,
2005.12116v1,Which model has the highest mean label accuracy on the SNLI Dev set?,NILE-NS:Independent,2005.12116v1-Table5-1.png," Mean and Standard Deviation for label accuracues on SNLI Dev set are reported. NILENS:Independent system has a high standard deviation and relatively lower mean accuracy. This is due to a bad random initialization with seed 219. When seed 219 results are excluded, the mean and standard deviation are 91.41 and 0.20 respectively.",
2005.12116v1,What is the purpose of the Explanation Processor (S)?,The Explanation Processor (S) generates label scores using the evidence present in the explanations generated by the Candidate Explanation Generators (G).,2005.12116v1-Figure1-1.png," Overview of NILE: A Premise and Hypothesis pair is input to label-specific Candidate Explanation Generators G which generate natural language explanations supporting the corresponding label. The generated explanations are then fed to the Explanation Processor S, which generates label scores using the evidence present in these explanations (see Figure 3 for the architectures used in this work). In addition to the explanations, NILE also utilizes the premise and hypothesis pair (See Section 4.4.2 for a discussion on the challenges in building such a system). Please see Section 4 for details.",
2005.12116v1,Which model achieves the highest label accuracy on the SNLI Dev set?,NILE-NS (Aggregate),2005.12116v1-Table1-1.png," Comparison of label and explanation accuracy on the in-domain SNLI evaluation sets. Models are selected using the Dev set label accuracy over 5 runs with different seeds of random initialization. Mean (and standard deviation) over the 5 runs are reported in the Appendix. # indicates the best reported result at https://nlp.stanford.edu/projects/snli/ at the time of writing. Note that SemBERT does not provide natural language explanations and is reported here only for reference. Bold numbers indicate highest among methods that produce explanations. Explanations are evaluated on the first 100 SNLI Test examples. We present reported numbers of ETPA (Camburu et al., 2018) as well as the results with our reproduction of ETPA. ETPA (reproduced) is directly comparable with NILE (Section 4.5). NILE-PH competes with or outperforms ETPA baselines on label accuracy, while NILE-NS and NILE provide significant gains in label accuracy. NILE and NILE-NS are competitive with the best reported results in terms of label accuracies. We report the number of correct explanations, averaged across annotators (B) as well as when all annotators agree on correctness (C). All NILE variants are able to provide more correct explanations than the ETPA baseline. We also report the percentage of correct explanations in the subset of correct label predictions (B/A, C/A). On this metric, NILE variants are comparable with the ETPA baseline. However, the real value of NILE lies in being able to probe the faithfulness of its decisions (Section 5.3). Further, NILE explanations generalize significantly better on out-of-domain examples (See Table 2). Please see Section 5.1 for details.",
2005.12116v1,Which model performs best on the MNLI Dev set in terms of label accuracy?,NILE,2005.12116v1-Table2-1.png," Testing the generalization capability of NILE on the out-of-domain MNLI Dev sets. Training and model selection is done on the SNLI dataset (Section 5.1), and evaluation on the out-of-domain MNLI Dev (matched) and MNLI Dev-mm (mismatched) sets. Label accuracies are reported for both MNLI Dev (matched) and MNLI Dev-mm (mismatched) sets, while explanations are evaluated on the first 100 MNLI Dev set examples. We report the number of correct explanations, averaged across annotators (B) as well as when all annotators agree on correctness (C). All NILE variants provide more correct explanations than the ETPA baseline (B, C). Further, the percentage of correct explanations in the subset of correct label predictions (B/A, C/A) is significantly better for all NILE variants. The results demonstrate that NILE provides a more generalizable framework for producing natural language explanations. Please see Section 5.2 for details.",
2005.12116v1,What is the difference between the two architectures shown in the figure?,Architecture A first predicts the label and then generates an explanation based on the label and input text. Architecture B first generates the explanation and then predicts the label based only on the explanation.,2005.12116v1-Figure2-1.png," Existing alternative architectures.: A. Posthoc generation: Given an input instance, first the label is predicted and then an explanation generated conditioned on the label and the input text. B. ExplainThenPredict (Camburu et al., 2018): Given the input instance, first the desired explanation is generated, and then the label is predicted using only the generated explanation. We argue that neither architecture provides a natural way to test the sensitivity of the model’s predictions to the generated explanation. Please see Section 4.2 for details.",
2005.12116v1,Which model has the largest maximum sequence length?,"NILE-NS with the ""Append"" strategy.",2005.12116v1-Table6-1.png, Hyper-parameters (batch size and maximum sequence length) used for fine-tuning roberta-base,
2005.12116v1,Which model is more sensitive to the shuffling of instance-explanation pairs?,NILE is more sensitive to the shuffling of instance-explanation pairs.,2005.12116v1-Table4-1.png," Probing the sensitivity of the system’s predictions by shuffling instance-explanation pairs. Each instance is attached to a randomly selected explanation of the same form as the original pair. The results demonstrate a much weaker link between NILE-NS’s predictions and associated explanations. On the other hand, NILE behaves more expectedly. Note that the baselines don’t allow a similar mechanism to test their faithfulness, and such testability is a key advantage of NILE. Please see Section 5.3 for details.",
2005.12116v1,"Which model is more sensitive to the input explanations, NILE-NS or NILE?",NILE-NS.,2005.12116v1-Table3-1.png," Estimating the sensitivity of the system’s predictions to input explanations through erasure. During testing, we erase either the instance or the explanations from the input to NILE-NS and NILE. The results seem to indicate that NILE-NS’s predictions are more faithful, in the sense of having a higher sufficiency. However, as demonstrated subsequently, the sensitivity of NILE-NS’s prediction to the input explanations is not as desired. Please see Section 5.3 for details.",
2005.12116v1,How do the different Explanation Processor architectures handle missing explanations?,The Aggregate (Agg) architecture allows handling missing explanations by looking for contradictory evidence.,2005.12116v1-Figure3-1.png," Explanation Processor architectures. A. Independent (Ind) collects evidence for a label symmetrically from the corresponding explanation. B. Aggregate (Agg) allows handling missing explanations by looking for contradictory evidence. C. Append (Apn) allows arbitrary evidence collection for each label. Please see Section 4.4.1 for details. Premise and hypothesis sentences are processed by additionally providing them to each block Fz where z ∈ {Ind, Agg, Apn}. Please see Section 4.4.2 for details.",
2006.02080v2,What is the relationship between the sets P and D?,D is a subset of P.,2006.02080v2-Figure2-1.png," Left: Algorithmic differentiation applied to programs combining smooth functions in a smooth way, the diagram commutes. Right: Algorithmic differentiation in nonsmooth settings, connection with known notion of generalized derivative is much less clear.",
2006.02080v2,Which of the plots shows the application of algorithmic differentiation to a constant function?,The bottom left plot.,2006.02080v2-Figure1-1.png," Top: AD applied to relu and two different implementations of the same function. Bottom: Algorithmic differentiation of a constant function, creation of artificial critical point or arbitrary derivatives at prescribed arguments for the sine function.",
2006.03041v3,Which algorithm has the best sample complexity?,It depends on the specific problem instance.,2006.03041v3-Table1-1.png," Sample complexity of asynchronous Q-learning and its variants to compute an ε-optimal Q-function in the `∞ norm, where we hide all logarithmic factors. With regards to the Markovian trajectory induced by the behavior policy, we denote by tcover, tmix, and µmin the cover time, mixing time, and minimum state-action occupancy probability of the associated stationary distribution, respectively.",
2006.03041v3,What does the figure illustrate?,"The figure illustrates variance-reduced Q-learning, which is a type of reinforcement learning algorithm.",2006.03041v3-Figure1-1.png, A pictorial illustration of variance-reduced Q-learning.,
2006.04219v2,What are the different steps involved in the DELPHI preprocessing workflow?,"The DELPHI preprocessing workflow involves four main steps: (1) **preprocessing**, which includes linear and nonlinear layers, (2) **client generation of random value r_i**, (3) **server generation of secret value s_i**, and (4) **garbling of the client's input C_i**.",2006.04219v2-Figure1-1.png," The bottleneck analysis, working flow and HE parameter selection of DELPHI.",
2006.04219v2,Which scheme has the lowest latency for the 7CNET network?,DARL,2006.04219v2-Table2-1.png," The execution time, communication overhead and inference accuracy comparison.",
2006.04219v2,What are the two main approaches to generating HE parameters in AutoPrivacy?,The two main approaches are network-wise selection and layer-wise selection.,2006.04219v2-Figure2-1.png, AutoPrivacy: (a) HE parameter generation of prior HPPNNs; and (b) HE parameter generation of AutoPrivacy.,
2006.04219v2,How does the number of bits required to represent the quantized values change across layers for each of the three methods?,"For all three methods, the number of bits required generally increases with the layer number. However, there are some fluctuations in the number of bits required for each layer, especially for ResNet-32 and MobileNet-V2.",2006.04219v2-Figure6-1.png," The HE parameter comparison of AutoPrivacy against prior works (net-log(q) and netlog(n) indicate the q and n of DELPHI, net-L-log(q) and net-log(n) mean the q and n of DARL, and layer-log(q) and layer-log(n) denote the q and n of AutoPrivacy).",
2006.04219v2,Which architecture has the highest accuracy?,AutoPrivacy,2006.04219v2-Table3-1.png, The comparison between NASS and AutoPrivacy.,
2006.04219v2,What is the relationship between the kernel size and the number of bits required to represent the network?,The number of bits required to represent the network increases with the kernel size.,2006.04219v2-Figure4-1.png, The ineffectiveness of conventional neural architecture search.,
2006.04219v2,Which HPPNN has the larger ciphertext modulus?,DELPHI,2006.04219v2-Table1-1.png, The HE parameters of prior HPPNNs.,
2006.04219v2,"Based on the plot, how does the latency of HE multiplication change with increasing values of log(n) and log(q)?",The latency of HE multiplication increases with increasing values of log(n) and log(q).,2006.04219v2-Figure3-1.png," The latency comparison of a HE multiplication with varying n and q (normalized to 11-120, where log(n)=11 and log(q)=120).",
2006.04219v2,What is the role of the embedding layer in AutoPrivacy?,The embedding layer converts the state of the environment into a representation that can be used by the actor and critic networks.,2006.04219v2-Figure5-1.png, The working flow of AutoPrivacy.,
2006.05394v2,What is the effect of using spatially modulated convolutions in SSNs on droplet artifacts?,Spatially modulated convolutions in SSNs reduce droplet artifacts.,2006.05394v2-Figure4-1.png, Left: droplet artifacts on the panel when using SPADE. Right: no droplet artifacts after introducing spatially modulated convolutions in SSNs. Images generated with the procedure of [18].,
2006.05394v2,How does the SSN model generate new images?,The SSN model generates new images by resampling the latent blocks of the latent code.,2006.05394v2-Figure6-1.png," Generations of our SSN model and corresponding resamplings. The model was trained on 256 x 256 LSUN churches. The latent code has dimension z ∈ R4×4×512 and the new images were obtained by resampling the latent blocks z(1,1) and z(1,2). We can see that the new images change mostly locally, with elements like towers appearing or disappearing, or trees changing. However, some minor changes are present in other parts of the image in order to keep global consistency, something that inpainting would not be able to do. The quality is comparable to that of StyleGAN2 [18].",
2006.05394v2,What is the relationship between the latent code z and the image?,"The latent code z is a spatial representation of the image, where local changes in z correspond to localized changes in the image.",2006.05394v2-Figure1-1.png," A diagram of Spatially Stochastic Networks. We decompose the latent code z spatially into independent blocks, and regularize the model so that local changes in z correspond to localized changes in the image. We then resample parts in the image by resampling their corresponding z’s.",
2006.05394v2,What is the difference between the top and middle rows of the image?,"The top row shows unmodified generations of the SSN model, while the middle row shows the results of resampling two z's near the top of each person's head.",2006.05394v2-Figure2-1.png," Resampling a person’s hair. The top row consists of unmodified generations of our models, Spatially Stochastic Networks (SSNs), trained on FFHQ [17]. With SSNs, resampling two z’s near the top of each persons head makes spatially localized changes (middle row) while also allowing for minimal necessary changes in other parts of the image (third row), unlike in traditional inpainting.",
2006.05394v2,Which configuration has a better FID score for the FFHQ dataset?,Configuration B (SSNs),2006.05394v2-Table1-1.png," Comparison of StyleGAN2 and SSNs without distortion regularization. Lower scores are better for FID and PPL. Both models attain comparable quality, while SSN allows for block resampling.",
2006.05394v2,How does the distortion parameter (λD) affect the quality-distortion trade-off?,"As the distortion parameter (λD) increases, the distortion also increases, while the quality (measured by FID) decreases. This means that there is a trade-off between quality and distortion, and the choice of λD depends on the desired balance between these two factors.",2006.05394v2-Figure5-1.png, Pareto curve visualizing the trade-off between quality (measured by FID) and distortion for SSNs trained in FFHQ at 256 x 256 resolution. Based on these results we chose to use λD = 100 for the qualitative experiments since it incurred a negligible loss in FID while drastically decreasing distortion.,
2006.05394v2,What is the effect of increasing the low distortion regularization weight λD on the FID and PPL quality metrics?,"Increasing the low distortion regularization weight λD generally leads to a decrease in FID and PPL, indicating an improvement in image quality.",2006.05394v2-Table2-1.png," Ablation for different strengths of the low distortion regularization weight λD. Lower is better for both FID and PPL (quality metrics) and for distortion. The value of λD = 100 achieves a significant reduction in distortion without incurring a significant loss in quality (strictly better in both FID and PPL than the state of the art StyleGAN2 baseline). Surprisingly, the PPL metric decreases as the regularization strength increases.",
2006.05394v2,What is the difference between inpainting and local block resampling (LDBR)?,"Inpainting is a technique that fills in missing parts of an image, while LDBR is a technique that resamples blocks of an image to change specific features. Inpainting is not allowed to make changes to the area specified as conditioning, while LDBR can make changes to both the resampled region and the surrounding area.",2006.05394v2-Figure8-1.png," We include additional experiments to highlight two things: The distinction between inpainting and potential advantages in certain situations, and the workings of SSNs with new block resolutions. In these experiments, we switch from 4× 4 blocks to 8× 8 blocks to showcase a more granular resampling. We resample the blocks constituting to the left eye in a picture three times (zoom to view well). The left image for each pair of images is the original generated image and the right is after resampling. All resamplings occur at the same location. The left-most, single image shows the underlying latent code dimension z ∈ R8×8×512 overlayed onto the original generated image, with the blocks to be resampled highlighted in red. In A we obtain a local resampling: the left eye region is more lit and less shadowy. This is a typical desired case of LDBR. In B, we see change that spans outside the resampled region with glasses appearing across the face. This resampling adheres semantically since it would be out of distribution to have glasses appear only on one half of the face. In C we receive little to no change, which is also in distribution but arguably not the desired use case. The distinction between LDBR and inpainting is very clear in case B. Inpainting by definition is not allowed to make changes to the area specified as conditioning, which includes the right eye. However, for SSNs, the other eye can be changed with added glasses. This example highlights the intrinsic trade-off between having a faithful (and diverse) resampling of the data distribution and low distortion.",
2006.05394v2,How does the generator ensure that each block za mostly affects ya?,The generator is regularized.,2006.05394v2-Figure3-1.png," Spatially Stochastic Networks. Each block za is a vector za ∈ Rnz . If we have nblocks = nw × nh, then z ∈ Rnw×nh×nz . The generator is regularized so that each za affects mostly ya.",
2006.05394v2,"What is the effect of resampling the latent blocks z(1,1) and z(1,2) on the generated images?","Resampling the latent blocks z(1,1) and z(1,2) results in local changes to the generated images, such as changes in hair, eye color, expressions, glasses, and other semantic elements.",2006.05394v2-Figure7-1.png," Generations of our SSN model and corresponding resamplings. The model was trained on 256 x 256 FFHQ. The latent code has dimension z ∈ R4×4×512 and the new images were obtained by resampling the latent blocks z(1,1) and z(1,2). We can see that the new images change mostly locally, with changes corresponding to the hair, eye color, expressions, glasses, and other semantic elements. Some minor changes are present in non-resampled parts of the image in order to keep global consistency, something that inpainting would not be able to do. The quality is comparable to that of StyleGAN2 [18].",
2006.05602v1,Which model achieved the highest accuracy on the music domain of the FDU-MTL dataset?,2ST-UDA,2006.05602v1-Table3-1.png, Results on FDU-MTL dataset. The highest performance in each domain is highlighted. The numbers in brackets represent the improvements relative to the Meta baseline. Results of ’MAN’ are obtained by setting private features to zeros and other results are taken from their original papers.,
2006.05602v1,How does the accuracy of D for private features and shared features compare on Amazon when training under WS-UDA?,"The accuracy of D for private features increases steadily with the number of epochs, while the accuracy for shared features fluctuates and does not show a clear trend.",2006.05602v1-Figure2-1.png, The accuracy of D for private features and shared features on Amazon when training under WS-UDA.,
2006.05602v1,"Which domain has the highest weight assigned to it for sentiment classification of the sentence ""i love this toy ! i ’m 12 years old and i still love to play barbie ’s ! i would reccomend this toy to any mother who has a little girl or boy that loves to play barbie’s""?",The 'toy' domain has the highest weight assigned to it for sentiment classification of the sentence.,2006.05602v1-Figure3-1.png," The weight assigned to each source classifier for sentiment classification from two sentences. The first sentence is ’i love this toy ! i ’m 12 years old and i still love to play barbie ’s ! i would reccomend this toy to any mother who has a little girl or boy that loves to play barbie’s’, which is derived from the ’toy’ domain. The second sentence is ’i must be missing something , i bought this movie ... when i watched it i fell asleep , its boring as hell trust me im a true horror fan and a gore-fiend , ... , the only scary part was the end with this witch , is everyone high but me ? this movie sucked ! boring as hell..’, which is derived from the ’dvd’ domain. The characters ’A-P’ denote different domains from ’MR’ to ’dvd’.",
2006.05602v1,Which method performs best on the DVD target domain?,2ST-UDA,2006.05602v1-Table2-1.png," Results on Amazon review dataset. The highest domain performance is shown in bold. The numbers in brackets represent the improvements relative to the best performance in the above baselines. Except for our model, the rest is taken from (Chen and Cardie 2018).",
2006.05602v1,Which domain has the highest percentage of negative reviews in the unlabeled data?,kitchen,2006.05602v1-Table1-1.png, Statistics of the Amazon reviews dataset including the number of labeled and unlabeled reviews for each domain as well as the proportion of negative samples in the unlabeled data.,
2006.06072v2,Which method performed the best on the Convallaria dataset?,CARE,2006.06072v2-Table1-1.png," Quantitative results. For all experiments, we compare all results in terms of mean Peak Signal-toNoise Ratio (PSNR in dB) and ˘1 standard error over 5 runs. Overall best performance indicated by being underlined, best unsupervised method in bold, and best fully unsupervised method in italic. For many datasets, DIVNOISING is the unsupervised SOTA, typically not being far behind the supervised CARE results.",
2006.06072v2,Which noise model best captures the true noise distribution?,The noise model created via bootstrapping and the noise model co-learned with DIVNOISING.,2006.06072v2-Figure3-1.png," Sensibility of Noise Models. For each predicted signal intensity (x-axis), we show the variance of noisy observations (yaxis). The plot is generated from experiments on the Convallaria dataset. The dashed red line shows the true noise distribution (measured from pairs of noisy and clean calibration data). This true distribution, as well as the noise model created via bootstrapping, and the noise model we co-learned with DIVNOISING, show simple (approximately) linear relationships between signal intensities and noise variance. Such a relationship is known to coincide with the physical reality of Poisson noise (shot noise) (Zhang et al., 2019). The implicitly learned noise model of the vanilla VAE has to independently predict the noise variance for each pixel. Its predictions clearly deviate from the true linear relationship. See Appendix A.13 for results on BioID Face dataset and more details.",
2006.06072v2,What is the difference between the noise models co-learned by the vanilla VAE and DIVNOISING?,The noise model co-learned by DIVNOISING is a much better approximation to the ground truth noise model than the noise model co-learned by the vanilla VAE.,2006.06072v2-Figure29-1.png," Comparison of noise models and variance maps predicted by the vanilla VAE and DIVNOISING. (a) For each predicted signal intensity (x-axis), we show the variance of noisy observations (y-axis). The plot is generated from experiments on the Convallaria dataset. The dashed red line shows the true noise distribution (measured from pairs of noisy and clean calibration data). We compare the noise model co-learned by the vanilla VAE and DIVNOISING with ground truth noise model and a noise model bootstrapped from noisy data alone as described in Prakash et al. (2020). Clearly, the noise model learnt by unsupervised DIVNOISING is a much better approximation to the ground truth noise model compared to the noise models learned/obtained by other methods. (b) We visually compare the denoising results and show how the predicted variance varies across the image. As a consequence of the approximately linear relationship between signal and noise variance, both variance images closely resemble the denoised results. However, the result of the vanilla VAE additionally contains artifacts.",
2006.06072v2,What is the effect of increasing the value of beta (β) on the diversity of DIVNOISING samples?,Increasing the value of β leads to increased structural diversity in DIVNOISING samples.,2006.06072v2-Figure13-1.png," Qualitative analysis of the effect of weighting KL loss term with factor β for DenoiSeg Flywing dataset. (a) We show the DIVNOISING MMSE estimate obtained by averaging 1000 samples for all considered β values (Supplementary Eq. 5). We observe that the reconstruction quality suffers on either increasing β ą 1 or decreasing β ă 1. Best results (with respect to PSNR) are obtained with β “ 1, as demonstrated in Fig. 12a. (b) For each β value, we show three randomly chosen DIVNOISING samples as well as difference images. Increasing β ą 1, allows the DIVNOISING network to generate structurally very diverse denoised solutions, while typically leading to textural smoothing. Decreasing β ă 1 generates DIVNOISING samples with overall much reduced structural diversity, introducing reconstruction artefacts/structures at smaller scales.",
2006.06072v2, What is the purpose of the DivNoising and MMSE methods? , The DivNoising and MMSE methods are used to remove noise from microscopy images. ,2006.06072v2-Figure14-1.png," Additional qualitative results for the DenoiSeg Mouse Buchholz et al. (2020) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,How does the quality of the generated images change as the resolution increases?,"The quality of the generated images decreases as the resolution increases. This can be seen by comparing the generated images at different resolutions. The generated images at 28x28 are quite clear and resemble the real images, while the generated images at 200x200 are much more blurry and difficult to discern.",2006.06072v2-Figure10-1.png," Generating synthetic images with the DIVNOISING VAE for the MNIST LeCun et al. (1998) dataset. DIVNOISING can also be used to generate images by sampling from the unit normal distribution ppzq and then using the decoder to produce an image. Here, we compare generated images and random ground truth images. Our fully convolutional architecture allows us to generate images of different sizes (despite all input images being only of size 28ˆ 28).",
2006.06072v2,"Which method, MMSE or DIVNOISING, preserves the details of the original image better?",DIVNOISING appears to preserve the details of the original image better than MMSE.,2006.06072v2-Figure18-1.png," Additional qualitative results for the FU-PN2V Mouse actin Prakash et al. (2020) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2," 
How is the encoder used during training and prediction/inference in DIVNOISING?"," 
During training, the encoder takes a noisy image as input and outputs a distribution over the latent space. This distribution is then used to sample a latent variable, which is then fed into the decoder to generate a denoised image. The loss function is then used to update the parameters of the encoder and decoder. During prediction/inference, the encoder is used to sample multiple latent variables from the distribution over the latent space, which are then fed into the decoder to generate multiple denoised samples.",2006.06072v2-Figure1-1.png," Training and prediction/inference with DIVNOISING. (top) A DivNoising VAE can be trained fully unsupervised, using only noisy data and a (measured, bootstrapped, or co-learned) pixel noise model pNMpxi|siq (see main text for details). (bottom) After training, the encoder can be used to sample multiple zk „ qφpz|xq, giving rise to diverse denoised samples sk. These samples can further be used to infer consensus point estimates such as a MMSE or a MAP solution.",
2006.06072v2,Which method appears to better preserve the detail in the nuclei of the cells in the image?,DIVNoising,2006.06072v2-Figure17-1.png," Additional qualitative results for the FU-PN2V Mouse nuclei Prakash et al. (2020) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,Why does the DIVNOISING method fail to capture the ample structural diversity present in natural photographic images?,"The DIVNOISING networks used in the study were relatively small, limiting their ability to capture the complexity of natural images.",2006.06072v2-Figure26-1.png," Qualitative results for the BSD68 dataset Roth & Black (2005). Our relatively small DIVNOISING networks fail to capture the ample structural diversity present in natural photographic images thereby exhibiting sub-par performance. However, diversity at adequately small image scales (with respect to the used network’s capabilities) can still be observed, as demonstrated with the different samples and the difference images corresponding to the green and cyan insets. We are confident that future work on DIVNOISING with larger networks and different network architectures/training schedules will expand the capabilities of this method to capture more complex image domains.",
2006.06072v2,Which segmentation method performs best according to the F1 score?,The Consensus (Avg) method performs best according to the F1 score.,2006.06072v2-Figure11-1.png," DIVNOISING enables downstream segmentation. Evaluation of segmentation results (using the F1 score (Van Rijsbergen, 1979), Jaccard score (Jaccard, 1901) and Average Precision (Lin et al., 2014). On the x-axis we plot the number of DIVNOISING samples used. The performance of BIC is only evaluated up to 100 samples because we limited run-time to 30 minutes). Remarkably, Consensus (Avg) using only 30 DIVNOISING segmentation labels, outperforms segmentations obtained from high SNR images.",
2006.06072v2,What is the purpose of the bottom row of images in the figure?,The bottom row of images shows the difference between the input image and the denoised image.,2006.06072v2-Figure22-1.png," Additional qualitative results for the MNIST LeCun et al. (1998) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,Which model more accurately predicts the noise variance in the image?,DivNoising.,2006.06072v2-Figure28-1.png," Comparison of noise models and variance maps predicted by the vanilla VAE and DIVNOISING. (a) For each predicted signal intensity (x-axis), we show the variance of noisy observations (y-axis). The plot is generated from experiments on the BioID Face dataset. The dashed red line shows the true noise distribution (Gaussian noise with σ2 “ 225). The noise model created via bootstrapping, and the noise model we co-learned with DIVNOISING, correctly show (approximately) constant values across all signal intensities. The implicitly learned noise model of the vanilla VAE has to independently predict the noise variance for each pixel. Its predictions clearly deviate from the true constant noise variance. (b) We visually compare the denoising results and show how the predicted variance varies across the image. While the variance predicted by the implicitly co-learned vanilla VAE model varies depending on the image content, the variance predicted by the co-learned DIVNOISING model correctly remains flat.",
2006.06072v2,How does DIVNOISING compare to MMSE in terms of diversity of results?,DIVNOISING produces a more diverse set of results than MMSE.,2006.06072v2-Figure21-1.png," Additional qualitative results for the W2S Zhou et al. (2020) dataset (ch. 2, avg1). Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,How does DIVNOISING compare to MMSE in terms of denoising performance?,DIVNOISING appears to be more effective than MMSE in denoising the KMNIST images.,2006.06072v2-Figure23-1.png," Additional qualitative results for the KMNIST Clanuwat et al. (2018) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,"Which of the two methods, DIVNOISING or MMSE, seems to produce more diverse results?",DIVNOISING seems to produce more diverse results.,2006.06072v2-Figure16-1.png," Additional qualitative results for the FU-PN2V Convallaria Krull et al. (2020); Prakash et al. (2020) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,How does the MMSE estimate compare to the MAP estimate and the ground truth?,"The MMSE estimate is an average of 10,000 samples from the posterior distribution, and it shows faintly overlaid letters as a consequence of ambiguities in the noisy input. The MAP estimate is an approximate maximum a posteriori estimate, and it has most artifacts of the MMSE solution removed. The ground truth is the original, uncorrupted image.",2006.06072v2-Figure4-1.png," Exploring the learned posterior. The MMSE estimate (average of 10k samples) shows faintly overlaid letters as a consequence of ambiguities in noisy input. Among these samples from the posterior, we use mean shift clustering (on smaller crops) to identify diverse and likely points in the posterior. We show 9 such cluster centers in no particular order. We also obtain an approximate MAP estimate (see Supplementary Material), which has most artifacts of the MMSE solution removed.",
2006.06072v2,What is the effect of DIVNOISING on the segmentation of the images?,DIVNOISING improves the segmentation of the images.,2006.06072v2-Figure5-1.png," DIVNOISING enables downstream segmentation. We show input images (upper row) and results of a fixed (untrained) segmentation pipeline (lower row). Cells that were segmented incorrectly (merged or split) are indicated in magenta. While segmentations of the noisy raw data are of very poor quality, sampled DIVNOISING results give rise to much better and diverse solutions (cols. 2-5). We then use two label fusion methods to find consensus segmentations (col. 6), which are even outperforming segmentation results on high SNR (GT) images. Quantitative results are presented in Appendix Fig. 11.",
2006.06072v2,What is the relationship between Z and X?,"Z influences S, which in turn influences X.",2006.06072v2-Figure27-1.png, Graphical model of the data generation process.,
2006.06072v2,How does DIVNOISING compare to the MMSE method for denoising text images?,DIVNOISING appears to produce sharper and more legible text than the MMSE method.,2006.06072v2-Figure24-1.png," Additional qualitative results for the eBook Marsh (2004) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,How does the DIVNOISING VAE perform in capturing structures at different scales?,The DIVNOISING VAE is only able to realistically capture small local structures.,2006.06072v2-Figure8-1.png," Generating synthetic images with the DIVNOISING VAE for the FU-PN2V Convallaria dataset Krull et al. (2020); Prakash et al. (2020). DIVNOISING can also be used to generate images by sampling from the unit normal distribution ppzq and then using the decoder to produce an image. Here, we compare generated images and randomly cropped real images. We show images of different resolutions to see how well the VAE captures structures at different scales. The VAE we use for denoising is only able to realistically capture small local structures. Note that the network we use is quite shallow (see Appendix Fig. 6).",
2006.06072v2,What is the purpose of the reparameterization trick in the network architecture?,"The reparameterization trick is used to improve the training of the network by making the sampling process differentiable. This allows the gradients to flow through the sampling process, which helps to optimize the network parameters.",2006.06072v2-Figure7-1.png, The fully convolutional architecture used for depth 3 networks. We show the depth 3 DIVNOISING network architecture used for DenoiSeg Flywing and eBook datasets. These networks count about 700k parameters and have a GPU memory footprint of approximately 5GB on a NVIDIA TITAN Xp.,
2006.06072v2,What is the purpose of the DIVNOISING technique?,The DIVNOISING technique is used to remove noise from images.,2006.06072v2-Figure20-1.png," Additional qualitative results for the W2S Zhou et al. (2020) dataset (ch. 1, avg1). Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,Which dataset has the shortest sampling time?,Mouse Nuc.,2006.06072v2-Table3-1.png, Sampling times with DIVNOISING. Average time (˘ SD) needed to sample 1000 denoised images from a trained DIVNOISING network (evaluated over all test images of the respective dataset).,
2006.06072v2,Which method performed the best on the Convallaria dataset in terms of PSNR?,DivN.all,2006.06072v2-Table2-1.png," Comparison of Self2Self with DivNoising. We train Self2Self (S2S) on a random single image per dataset and compare it with DIVNOISING trained on the same single image (DivN.1) and DIVNOISING trained with the full dataset (DivN.all). All methods are tested on the selected single image. Overall best method is indicated in bold. For all datasets, DIVNOISING leads to best performance while being orders of magnitude faster and needing significantly less GPU memory.",
2006.06072v2,What is the relationship between the input and the ground truth images?,The input images are noisy versions of the ground truth images.,2006.06072v2-Figure15-1.png," Additional qualitative results for the DenoiSeg Flywing Buchholz et al. (2020) dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,How does the diversity of DIVNOISING samples change with increasing values of β?,The diversity of DIVNOISING samples increases with increasing values of β.,2006.06072v2-Figure12-1.png," Analyzing the denoising quality and diversity of DIVNOISING samples with different factors for the DenoiSeg Flywing dataset. (a) The heatmap shows how the quality (PSNR in db) of DIVNOISING MMSE estimate changes with averaging increasingly larger number of samples (numbers shown for 1 run). Unsurprisingly, the more samples are averaged, the better the results get. We also investigate the effect of weighting the KL loss term with a factor β (Supplementary Eq. 5) on the quality of reconstruction. We observe that the usual VAE setup with β “ 1 gives the best results in terms of reconstruction quality. Increasing β ą 1 leads to higher diversity at the expense of poor reconstruction (see Appendix Fig. 13.) (b) We quantify the denoising diversity achieved with different β values in terms of standard deviation PSNR (see Appendix section A.7 for details on the metric). We report the average standard deviation of PSNRs over all test images for different values of β and observe that the higher β values increase the diversity. (c) We also investigate the effect of noise on the diversity of denoised DIVNOISING samples by adding pixel wise independent zero mean Gaussian noise of standard deviations 30, 50 and 70. The higher the noise, the more ambiguous the noisy input images are, thus leading to higher diversity.",
2006.06072v2,What does the image show?,The image shows the results of using DIVNOISING to denoise noisy images of a face.,2006.06072v2-Figure25-1.png," Additional qualitative results for the BioID Face noa dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,What is the difference between the DIVNoising and MMSE results?,The DIVNoising results are more diverse than the MMSE results.,2006.06072v2-Figure19-1.png," Additional qualitative results for the W2S Zhou et al. (2020) dataset (ch. 0, avg1). Here, we show qualitative results for two cropped regions (green and cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity of DIVNOISING results.",
2006.06072v2,How well does the DIVNOISING VAE capture structures at different scales?,"The DIVNOISING VAE captures larger structures better than smaller, high-frequency structures.",2006.06072v2-Figure9-1.png," Generating synthetic images with the DIVNOISING VAE for the DenoiSeg Flywing Buchholz et al. (2020) dataset. DIVNOISING can also be used to generate images by sampling from the unit normal distribution ppzq and then using the decoder to produce an image. Here, we compare generated images and randomly cropped real images. We show images of different resolutions to see how well the VAE captures structures at different scales. Note that the network (see Appendix Fig. 7) we use is a bit deeper compared to Supplementary Fig. 8. This VAE captures larger structures a little better but struggles to produce crisp high frequency structures. This is likely a consequence of the increased depth of the used network.",
2006.06072v2,Which method produces the most diverse results?,DIVNOISING.,2006.06072v2-Figure2-1.png," Qualitative denoising results. We compare two DIVNOISING samples, the MMSE estimate (derived by averaging 1000 sampled images), and results by the supervised CARE baseline. The diversity between individual samples is visualized in the column of difference images. (See Appendix A.9 for additional images of DIVNOISING results.)",
2006.06072v2,How does the number of channels change throughout the network?,"The number of channels starts at 1, increases to 32, then 64, and then decreases back to 1.",2006.06072v2-Figure6-1.png," The fully convolutional architecture used for depth 2 networks. We show the depth 2 DIVNOISING network architecture used for FU-PN2V Convallaria, FU-PN2V Mouse nuclei, FUPN2V Mouse actin, all W2S channels and DenoiSeg Mouse datasets. These networks count about 200k parameters and have a GPU memory footprint of approximately 1.8GB on a NVIDIA TITAN Xp.",
2006.10680v2, Which algorithm has the lowest variance in gradients across all learning rates? , REINFORCE LOO,2006.10680v2-Figure1-1.png, Variance of the gradient estimators for the toy problem (Section 5.1). The variance was computed using 5000 Monte Carlo samples.,
2006.10680v2,Which of the four models has the lowest variance of gradients for the nonlinear model?,DisARM,2006.10680v2-Figure2-1.png," Training a Bernoulli VAE on FashionMNIST dataset by maximizing the ELBO. We plot the train ELBO (left column), test 100-sample bound (middle column), and the variance of gradient estimator (right column) for the linear (top row) and nonlinear (bottom row) models. We plot the mean and one standard error based on 5 runs from different random initializations. Results on MNIST and Omniglot were qualitatively similar (Appendix Figure 6).",
2006.10680v2,Which method achieves the highest test 100-sample bound for the linear model on MNIST?,RELAX.,2006.10680v2-Figure6-1.png," Training a Bernoulli VAE by maximizing the ELBO using DisARM (red), RELAX (blue), REINFORCE LOO (orange), and ARM (green). Both MNIST and Omniglot were dynamically binarized. We report the ELBO on training set (left column), the 100-sample bound on test set (middle column) and the variance of gradients (right column) for linear (top row) and nonlinear (bottom row) models. The mean and standard error (shaded area) are estimated given 5 runs from different random initializations.",
2006.10680v2,Which method performs best on the Fashion MNIST task when trained on ELBO?,DisARM.,2006.10680v2-Table1-1.png," Mean variational lower bounds and the standard error of the mean computed based on 5 runs from different random initializations. The best performing method (up to the standard error) for each task is in bold. To provide a computationally fair comparison between VIMCO 2K-samples and DisARM K-pairs, we report the 2K-sample bound for both, even though DisARM optimizes the K-sample bound. Results are for single stochastic layer models unless stated otherwise.",
2006.10680v2,How does the variance of the gradients change as the number of iterations increases?,The variance of the gradients decreases as the number of iterations increases.,2006.10680v2-Figure5-1.png," Comparing gradient estimators for the toy problem (Section 5.1). We plot the trace of the estimated Bernoulli probability σ(φ), the estimated gradients, and the variance of the estimated gradients. The variance is measured based on 5000 Monte-Carlo samples at each iteration.",
2006.10680v2,"Which method performed best on the Fashion MNIST dataset, according to the table?",DisARM,2006.10680v2-Table2-1.png, Results for models trained by maximizing the ELBO. We report the mean and the standard error of the mean for the ELBO on the training set and of the 100-sample bound on the test set. The results we computed based on 5 runs from different random initializations and the standard error of the mean. The best performing method (up to the standard error) for each task is in bold.,
2006.10680v2,Which of the algorithms has the lowest variance of the gradient estimator?,DisARM,2006.10680v2-Figure3-1.png," Training 2/3/4-layer Bernoulli VAE on Omniglot using DisARM, RELAX, REINFORCE LOO, and ARM. We report the ELBO on the training set (left), the 100-sample bound on the test set (middle), and the variance of the gradient estimator (right).",
2006.10680v2,Which method performs the best on the Fashion MNIST dataset?,"The DisARM method performs the best on the Fashion MNIST dataset, with a mean ELBO of -236.78 ± 0.09 on the training set and a mean 100-sample bound of -236.43 ± 0.10 on the test set.",2006.10680v2-Table3-1.png, Results for training Bernoulli VAEs with 2/3/4 stochastic hidden layers. We report the mean and the standard error of the mean for the ELBO on training set and of the 100-sample bound on the test set. The results are computed based on 5 runs with different random initializations. The best performing methods (up to standard error) for each task is in bold.,
2006.10680v2,Which algorithm provides the lowest variance of the gradient estimators for the linear model on the Fashion MNIST dataset?,VIMCO with 30 samples,2006.10680v2-Figure8-1.png," Training a Bernoulli VAE by maximizing the multi-sample variational bound with DisARM and VIMCO. We report the training and test multi-sample bound and the variance of the gradient estimators for the linear (a) and nonlinear (b) models. We evaluate the model on three datasets: MNIST, FashionMNIST and Omniglot, with dynamic binarization.",
2006.10680v2,How does the variance of the gradient estimators for DisARM and VIMCO compare as the number of samples increases?,"The variance of the gradient estimators for both DisARM and VIMCO decreases as the number of samples increases. However, the variance of the gradient estimators for DisARM is consistently lower than that of VIMCO for all numbers of samples.",2006.10680v2-Figure4-1.png, Training a Bernoulli VAE on FashionMNIST by maximizing the multi-sample variational bound with DisARM (solid line) and VIMCO (dashed line). We report the training multi-sample bound and the variance of the gradient estimators for the linear and nonlinear models. Test performance and results on MNIST and Omniglot were qualitatively similar (Appendix Figure 8).,
2006.10680v2,"Which method performed the best on the Dynamic MNIST task, according to the table?",DisARM 10-pairs,2006.10680v2-Table4-1.png," Train and test variational lower bounds for models trained using the multi-sample objective. We report the mean and the standard error of the mean computed based on 5 runs from different random initializations. The best performing method (up to the standard error) for each task is in bold. To provide a computationally fair comparison between VIMCO 2K-samples and DisARM K-pairs, we report the 2K-sample bound for both, even though DisARM optimizes the K-sample bound.",
2006.10680v2,Which method achieves the lowest variance of the gradient estimator on the Dynamic MNIST dataset?,DisARM.,2006.10680v2-Figure7-1.png," Training 2/3/4-layer Bernoulli VAE on MNIST and FashionMNIST using DisARM, RELAX, REINFORCE LOO, and ARM. We report the ELBO on the training set (left), the 100- sample bound on the test set (middle), and the variance of the gradient estimator (right).",
2006.12631v2,Which dataset has the lowest average test set NLL for the TriTPP model?,PUBG,2006.12631v2-Table5-1.png, Average test set NLL with standard deviations. Datasets where all models have a standard deviation below 0.005 are excluded.,
2006.12631v2,Which configuration of TriTPP has the lowest NLL on the Reddit-S dataset?,TriTPP with L = 4 and H = 16,2006.12631v2-Table6-1.png, Test set NLL for different configurations of TriTPP.,
2006.12631v2,How does the training time of TriTPP and RNN compare as the sequence length increases?,"The training time of both TriTPP and RNN increases as the sequence length increases, but TriTPP has a lower training time than RNN for all sequence lengths.",2006.12631v2-Figure7-1.png, Scalability analysis. Standard devs. are below 1ms.,
2006.12631v2,Which transformation has the most significant impact on the overall Jacobian of the combined transformation F?,The transformation with the most significant impact on the overall Jacobian is the transformation C.,2006.12631v2-Figure12-1.png, Jacobians of the component transformations of TriTPP. We obtain the Jacobian of the combined transformation F = C ◦ Φ2 ◦ B4 ◦ B3 ◦ B2 ◦ B1 ◦ Φ1 ◦D ◦ Λ by multiplying the Jacobians of each transform (right to left).,
2006.12631v2,How do the lengths of the generated sequences compare to the lengths of the true sequences for each model?,The lengths of the generated sequences are generally shorter than the lengths of the true sequences for all three models.,2006.12631v2-Figure13-1.png, Histograms of sequence lengths (true and generated) for Twitter. The difference between the two is quantified using Wasserstein distance (WD) — lower is better.,
2006.12631v2,Which transformation has the largest Jacobian?,Transformation C.,2006.12631v2-Figure11-1.png, Jacobians of the component transformations of the modulated renewal process. We obtain the Jacobian of the combined transformation F = C ◦Φ ◦D ◦Λ by multiplying the Jacobians of each transform (right to left).,
2006.12631v2,What is the relationship between Λ∗(t) and p(t)?,Λ∗(t) is used to compute p(t).,2006.12631v2-Figure1-1.png," Triangular map F (t) = (Λ∗(t1), ...,Λ∗(tN )) is used for computing p(t).",
2006.12631v2,What happens to samples that occur after time T?,They are discarded.,2006.12631v2-Figure2-1.png, Sampling is done by applying F−1 to a sample z from a HPP with unit rate.,
2006.12631v2,What is the difference between the segmentation of server data obtained using the VI approach and MCMC?,The segmentation of server data obtained using the VI approach is smoother than the segmentation obtained using MCMC.,2006.12631v2-Figure15-1.png," Segmentation of server data obtained using our VI approach and MCMC. In both cases, we estimate the posterior p(t, s|o,θ) as well as the MMPP parameters θ.",
2006.12631v2,Which model has the most similar distribution of lengths of true and sampled sequences to the ground truth distribution?,Reddit-C,2006.12631v2-Table7-1.png, Wasserstein distance between the distributions of lengths of true and sampled sequences.,
2006.12631v2,What is the effect of using different random seeds on the marginal posterior trajectories?,The marginal posterior trajectories are slightly different when using different random seeds.,2006.12631v2-Figure14-1.png, Marginal posterior trajectories obtained when using different random seeds.,
2006.12631v2,What is the relationship between the cumulative conditional intensity and the compensator?,The cumulative conditional intensity is equal to the compensator.,2006.12631v2-Table3-1.png, Notation used throughout the paper.,
2006.12631v2,"Which inference method, variational inference (VI) or Markov chain Monte Carlo (MCMC), is more confident in its predictions of the latent trajectory?",VI is more confident in its predictions.,2006.12631v2-Figure8-1.png, Posterior distributions over the latent trajectory of an MMPP learned using our VI approach & MCMC.,
2006.12631v2,Which model achieved the best performance on the Twitter dataset?,TriTPP,2006.12631v2-Table1-1.png," Average test set NLL on synthetic and real-world datasets (lower is better). Best NLL in bold, second best underlined. Results with standard deviations can be found in Appendix F.1.",
2006.12631v2,Which dataset has the highest MMD between the hold-out test set and the generated samples for the TriTPP model?,Yelp2,2006.12631v2-Table2-1.png, MMD between the hold-out test set and the generated samples (lower is better).,
2006.12631v2,What are the different transformations that make up the expressive map F?,"The expressive map F is made up of the following transformations: Jacobian of each transform, 
, , , , , , and .",2006.12631v2-Figure3-1.png, TriTPP defines an expressive map F as a composition of easy-to-invert transformations.,
2006.12631v2,How does the training loss of TriTPP and RNN compare across different datasets?,The training loss of TriTPP is generally lower than that of RNN across all datasets.,2006.12631v2-Figure16-1.png, Training loss convergence for TriTPP and RNN model with different random seeds.,
2006.12631v2,What is the effect of different random seeds on the convergence of the variational inference procedure?,The different random seeds cause the variational inference procedure to converge to slightly different ELBO values.,2006.12631v2-Figure17-1.png, Convergence of our variational inference procedure when using 5 different random seeds.,
2006.12631v2,Which dataset has the longest average sequence length?,Reddit-S,2006.12631v2-Table4-1.png, Statistics for the synthetic & real-world datasets,
2006.12631v2,What does the orange dashed line represent?,The orange dashed line represents the true number of events.,2006.12631v2-Figure10-1.png, The blue area represents the counting measure distance (figure adapted from [60]).,
2006.12631v2,How does the entropy change as a function of iteration?,The entropy initially decreases and then plateaus.,2006.12631v2-Figure4-1.png, Monte Carlo estimate of the entropy. 0 100 200 300 400 Iteration,
2006.12631v2,How does the value of γ affect the estimated entropy?,Increasing the value of γ decreases the estimated entropy.,2006.12631v2-Figure5-1.png, Maximizing the entropy with different values of γ. 0 5 10 15 20 Time,
2006.12631v2,What is the relationship between the state and the observation in the Markov modulated Poisson process?,The state determines the rate of the Poisson process that generates the observation.,2006.12631v2-Figure6-1.png, Markov modulated Poisson process with 2 states.,
2006.12631v2,What is the difference between the hard and relaxed indicator values for the last two timesteps?,"The hard indicator values for the last two timesteps are 0.00 and 0.00, while the relaxed indicator values are 0.01 and 0.00.",2006.12631v2-Figure9-1.png, Examples of values involved in the ELBO computation.,
2006.14567v3,What is the main difference between the original image and the cropped images?,The cropped images are zoomed in on different parts of the original image.,2006.14567v3-Figure21-1.png, Images generated by one of our best LA-ExtraGrad & EMA model (FID of 2.94) trained on SVHN.,
2006.14567v3,How does the performance of the LA–GAN model with a large k value change over time?,The performance of the LA–GAN model with a large k value improves over time.,2006.14567v3-Figure12-1.png, IS (higher is better) of LA–GAN on ImageNet with relatively large k = 10000. The backtracking step is significantly improving the model’s performance every 10000 iterations. This shows how a large k can take advantage of the rotating gradient vector field.,
2006.14567v3,Which model is the most stable according to the figure?,NLA-ExtraGrad,2006.14567v3-Figure13-1.png," LA-ExtraGrad, NLA-ExtraGrad and Extragrad models trained on ImageNet. For LA-Extragrad (k = 5000), the lighter and darker colors represent the fast and slow weights respectively. For NLA-ExtraGrad (ks = 5, kss = 5000), the lighter and darker colors represent the fast and super-slow weights respectively. In our experiments on ImageNet, while LA-ExtraGrad is more stable than ExtraGrad, it still has a tendency to diverge early. Using Alg. 6 we notice a clear improvement in stability.",
2006.14567v3,Which of the four algorithms shown in the figure converges to the optimum the fastest?,LA-ExtraGrad,2006.14567v3-Figure11-1.png," Convergence of GDA, LA-GDA, ExtraGrad, and LA-ExtraGrad on equation QP-2.",
2006.14567v3,What is the difference between the Generator and the Discriminator in a DCGAN?,"The Generator takes a random noise vector as input and outputs a generated image, while the Discriminator takes an image as input and outputs a probability that the image is real or fake.",2006.14567v3-Table5-1.png," DCGAN architectures (Radford et al., 2016) used for experiments on MNIST. We use ker and pad to denote kernel and padding for the (transposed) convolution layers, respectively. With h×w we denote the kernel size. With cin → yout we denote the number of channels of the input and output, for (transposed) convolution layers.",
2006.14567v3,How does the Lookahead–minmax algorithm with GDA achieve convergence in this example?,The backtracking step of the Lookahead–minmax algorithm (lines 10 & 11) allows the otherwise non-converging GDA to converge.,2006.14567v3-Figure1-1.png," Illustration of Lookahead–minmax (Alg.1) with GDA on: minx maxy x · y, with α=0.5. The solution, trajectory {xt, yt}Tt=1, and the lines between (xP , yP) and (xt, yt) are shown with red star, blue line, and dashed green line, resp. The backtracking step of Alg. 1 (lines 10 & 11) allows the otherwise non-converging GDA to converge, see § 4.2.1.",
2006.14567v3,How does the FID of the slow weights compare to the FID of the fast weights in the Lookahead–minmax on 32×32 ImageNet?,The FID of the slow weights is lower than the FID of the fast weights.,2006.14567v3-Figure2-1.png," FID (↓) of Lookahead–minmax on 32×32 ImageNet (§ 5), see Fig. 12 for IS. We observe significant improvements after each LA-step, what empirically confirms the existence of rotations and thus the intuition of the behaviour of LAminmax on games, as well as the relevance of the bilinear game for real world applications–Fig.1.",
2006.14567v3,Which model achieved the best FID score on the CIFAR-10 dataset?,LA-AltGAN,2006.14567v3-Table1-1.png," Comparison of LA–GAN with its respective baselines AltGAN and ExtraGrad (see § 5.1 for naming), using FID (lower is better) and IS (higher is better), and best obtained scores. EMA denotes exponential moving average, see § F. All methods use Adam, see § G for detailed description. Results are averaged over 5 runs. We run each experiment for 500K iterations. See § H and § 5.2 for details on architectures and hyperparameters and for discussion on the results, resp. Our overall best obtained FID scores are 12.19 on CIFAR-10 and 2.823 on SVHN, see § I for samples of these generators. Best scores obtained for each metric and dataset are highlighted in yellow. For each column the best score is in bold along with any score within its standard deviation reach.",
2006.14567v3,Which GAN architecture achieves the best FID score on CIFAR-10?,LA-AltGAN,2006.14567v3-Table2-1.png," Summary of the recently reported best scores on CIFAR-10 and benchmark with LA–GAN, using published results. Note that the architectures are not identical for all the methods–see § 5.2.",
2006.14567v3,What are some of the unrealistic artifacts that are present in the generated images?,"Some of the unrealistic artifacts present in the generated images include the black and white squares in the corners of the images, the unnatural lighting in some of the images, and the blurred edges of some of the objects.",2006.14567v3-Figure16-1.png," Samples from our generator model with the highest IS score. We can clearly see some unrealistic artefacts. We observed that the IS metric does not penalize these artefacts, whereas FID does penalize them.",
2006.14567v3,What does the figure tell us about the stability of the LA-AltGAN model compared to the AltGAN model?,The figure shows that the LA-AltGAN model is more stable than the AltGAN model.,2006.14567v3-Figure18-1.png," Analysis on MNIST at 100K iterations. Fig. 18a & 18b: Largest 20 eigenvalues of the Hessian of the generator and the discriminator. Fig. 18c: Eigenvalues of the Jacobian of JVF, indicating no rotations at the point of convergence of LA–AltGAN (see § 2).",
2006.14567v3,"Which method is more stable on the SVHN dataset, LA-ExtraGrad or ExtraGrad?",LA-ExtraGrad,2006.14567v3-Figure17-1.png," Improved stability of LA–ExtraGrad relative to its ExtraGrad baseline on SVHN and CIFAR-10, over 5 runs. The median and the individual runs are illustrated with ticker solid lines and with transparent lines, respectively. See § I and H for discussion and details on the implementation, resp.",
2006.14567v3,"What are the differences between the AltGAN, LA-AltGAN, and DLA-AltGAN models?","The main differences between the AltGAN, LA-AltGAN, and DLA-AltGAN models are the lookahead parameters k and k'. LA-AltGAN has a lookahead of k = 5, while DLA-AltGAN has a lookahead of k = 5 and k' = 10000.",2006.14567v3-Table11-1.png, Hyperparameters used for our AltGAN experiments on ImageNet.,
2006.14567v3,Which algorithm uses the lookahead parameter k?,LA-ExtraGrad and Unrolled-GAN.,2006.14567v3-Table10-1.png, Hyperparameters that we used for our experiments on CIFAR-10.,
2006.14567v3,Which hyperparameter is set differently for DLA-ExtraGrad compared to ExtraGrad and LA-ExtraGrad?,Lookahead k',2006.14567v3-Table12-1.png, Hyperparameters used for our ExtraGrad experiments on ImageNet.,
2006.14567v3,Which optimizer performs best on CIFAR-10 according to the Fréchet Inception distance metric?,LA-AltGAN-A,2006.14567v3-Table13-1.png," Comparison of the LA-GAN optimizer with its respective baselines AltGAN and ExtraGrad (see § 5.1 for naming), using FID (lower is better) and IS (higher is better). EMA denotes exponential moving average (with fixed β = 0.9999, see § F). With suffix –R and –A we denote that we use RAdam (Liu et al., 2020) and Adam (Kingma & Ba, 2015) optimizer, respectively. Results are averaged over 5 runs. We run each experiment on MNIST for 100K iterations, and for 500K iterations for the rest of the datasets. See § H and § 5.2 for details on architectures and hyperparameters and for discussion on the results, resp.",
2006.14567v3,What is the value of the restart probability for SVRE when the batch size is 64?,The value of the restart probability for SVRE when the batch size is 64 is 0.1.,2006.14567v3-Table4-1.png," List of hyperparameters used in Figure 4. η denotes the learning rate, β1 is defined in equation 6, and α and k in Alg. 1.",
2006.14567v3,Which optimizer performs best for small batch sizes?,Lookahead-Minmax (EMA-LA-GDA),2006.14567v3-Figure7-1.png," Distance to the optimum as a function of the number of passes, for Adam, Extra-Adam, Extragradient, and LA-GAN, all combined with EMA (β = 0.999). For small batch sizes, the high variance of the problem is preventing convergence for all methods but Lookahead-Minmax.",
2006.14567v3,Which implementation of Lookahead-minmax performs better on the SVHN dataset?,The joint Lookahead-minmax implementation performs better on the SVHN dataset.,2006.14567v3-Figure15-1.png," (a): Comparison of the joint Lookahead-minmax implementation (Joint prefix, see Algorithm 1) and the alternating Lookahead-minmax implementation (Alt prefix, see Algorithm 7) on the SVHN dataset. (b): Results obtained with the different methods introduced in §5 as well as an alternating implementation of Lookahead-minmax, on the MNIST dataset. Each curve is obtained averaged over 5 runs. The results of the alternating implementation differ very little from the joint implementation, the curve for Alt-LA-AltGAN matches results in Table 1.",
2006.14567v3,What is the difference between the two figures?,"The left figure shows the results of using LA-AltGAN, while the right figure shows the results of using LA-ExtraGrad.",2006.14567v3-Figure14-1.png," Comparison between different extensions of Lookahead to games on CIFAR-10. We use prefix joint and alt to denote Alg. 1 and Alg. 7, respectively of which the former is the one presented in the main paper. We can see some significant improvements in FID when using the joint implementation, for both LA-AltGAN (left) and LA-ExtraGrad (right).",
2006.14567v3,How does the addition of LA to AltGAN affect the stability of the training process?,The addition of LA to AltGAN appears to improve the stability of the training process.,2006.14567v3-Figure5-1.png," Eigenvalues of v′(θ,ϕ) at 100K iterations on MNIST, see § 5.2.",
2006.14567v3,"How does the performance of LA-AltGAN compare to AltGAN on the SVHN, CIFAR-10, and ImageNet datasets?",LA-AltGAN consistently outperforms AltGAN on all three datasets.,2006.14567v3-Figure6-1.png," Improved stability of LA–AltGAN relative to its baselines on SVHN, CIFAR-10 and ImageNet, over 5 runs. The median and the individual runs are illustrated with ticker solid lines and with transparent lines, respectively. See § 5.2 for discussion.",
2006.14567v3,Which GAN method performed the best in terms of FID score when using the EMA averaging method?,LA-AltGAN,2006.14567v3-Table3-1.png," FID (lower is better) results on MNIST, averaged over 5 runs. Each experiment is trained for 100K iterations. Note that Unrolled–GAN is computationally more expensive: in the order of the ratio 4 : 22–as we used 20 steps of unrolling what gave best results. See 5.2 & H for implementation and discussion, resp.",
2006.14567v3,Which optimization method converges to the optimum the fastest?,Unroll-XY,2006.14567v3-Figure3-1.png," Distance to the optimum of equation SB–G using different full–batch methods, averaged over 5 runs. Unless otherwise specified, the learning rate is η = 0.3. See § 4.2.1.",
2006.14567v3,What type of images are shown in the figure?,"The figure shows a variety of images, including animals, vehicles, and natural scenes.",2006.14567v3-Figure19-1.png," Samples generated by our best performing trained generator on CIFAR-10, using LA-AltGAN and exponential moving average (EMA) on the slow weights. The obtained FID score is 12.193.",
2006.14567v3,Which of the two hyperparameter settings is more sensitive to the value of k?,The hyperparameter setting with η = 0.01 and B = 16 is more sensitive to the value of k.,2006.14567v3-Figure8-1.png," Sensitivity of LA-GDA to the value of the hyperparameter k in Alg. 1 for two combinations of batch sizes and η. The y-axis is the distance to the optimum at 20000 passes. The jolting of the curves is due to the final value being affected by how close it is to the last equation LA step, i.e. lines 10 and 11 of Alg. 1.",
2006.14567v3,What are the hyperparameters that are specific to the LA-AltGAN and LA-ExtraGrad methods?,The hyperparameters that are specific to the LA-AltGAN and LA-ExtraGrad methods are the lookahead k and lookahead α parameters.,2006.14567v3-Table8-1.png, Hyperparameters used on MNIST.,
2006.14567v3,What are the hyperparameters used for the AltGAN model?,"The hyperparameters used for the AltGAN model are: 
* ηG = 0.0002
* ηD = 0.0002
* Adam β1 = 0.0
* Batch-size = 128
* Update ratio r = 5",2006.14567v3-Table9-1.png, Hyperparameters used on SVHN.,
2006.14567v3,What are some of the types of images that the model is able to generate?,"The model is able to generate images of animals, objects, and landscapes.",2006.14567v3-Figure20-1.png," Images generated by our best model trained on 32×32 ImageNet, obtained with LA-AltGAN and EMA of the slow weights, yielding FID of 12.44.",
2006.14567v3,What is the difference between the input to the generator and the input to the discriminator?,"The input to the generator is a 128-dimensional vector sampled from a normal distribution with mean 0 and standard deviation 1, while the input to the discriminator is a 3x32x32 image.",2006.14567v3-Table7-1.png," Deep ResNet architectures used for experiments on ImageNet, SVHN and CIFAR-10, where G– ResBlock and D–ResBlock for the Generator (left) and the Discriminator (right), respectively, are described in Table 6. The models’ parameters are initialized using the Xavier initialization (Glorot & Bengio, 2010). For ImageNet experiments, the generator’s input is of dimension 512 instead of 128.",
2006.14567v3,How do the ResNet blocks in the Generator and Discriminator differ?,"The ResNet blocks in the Generator and Discriminator differ in two ways. First, the Generator's ResNet blocks upsample the input by a factor of 2 using nearest neighbor interpolation, while the Discriminator's ResNet blocks do not. Second, the Discriminator's first ResNet block does not perform the ReLU non-linearity immediately on the input, while all other ResNet blocks in both the Generator and Discriminator do.",2006.14567v3-Table6-1.png," ResNet blocks used for the ResNet architectures (see Table 7), for the Generator (left) and the Discriminator (right). Each ResNet block contains skip connection (bypass), and a sequence of convolutional layers, normalization, and the ReLU non–linearity. The skip connection of the ResNet blocks for the Generator (left) upsamples the input using a factor of 2 (we use the default PyTorch upsampling algorithm–nearest neighbor), whose output is then added to the one obtained from the ResNet block listed above. For clarity we list the layers sequentially, however, note that the bypass layers operate in parallel with the layers denoted as “feedforward” (He et al., 2016). The ResNet block for the Discriminator (right) differs if it is the first block in the network (following the input to the Discriminator), ` = 1, or a subsequent one, ` > 1, so as to avoid performing the ReLU non–linearity immediate on the input.",
2006.14567v3,How does the learning rate η affect the trajectory of the iterates?,The trajectory of the iterates becomes more oscillatory as the learning rate η increases.,2006.14567v3-Figure9-1.png," Illustration of Lookahead-minmax on the bilinear game minx maxy x · y, for different values of the learning rate η ∈ {0.1, 0.4, 1.5}, with fixed k = 5 and α = 0.5. The trajectory of the iterates is depicted with green line, whereas the the interpolated line between (ωt, ω̃t,k), t = 1, . . . , T , k ∈ R with ωt = (θt,ϕt) is shown with dashed red line. The transparent lines depict the level curves of the loss function, and ω? = (0.0). See § D.6 for discussion.",
2007.02503v1,What are the key differences between the concept-based paradigm and the embedding-based paradigm for text-based video retrieval?,"The concept-based paradigm relies on explicit concept matching, where the query is decomposed into individual concepts that are then matched against the video content. In contrast, the embedding-based paradigm projects both the query and the video content into a common embedding space, where the similarity between them can be measured directly.",2007.02503v1-Figure1-1.png, Concept-based paradigm vs. Embedding-based paradigm for text-based video retrieval.,
2007.02503v1,Which method achieves the best performance in terms of R@1 on the MSR-VTT dataset with the data split from [44]?,The DualEncoding method achieves the best performance in terms of R@1 on the MSR-VTT dataset with the data split from [44].,2007.02503v1-Table1-1.png, State-of-the-art performance comparison (%) on MSR-VTT with different dataset splits. Note that TCE uses bidirectional GRU and LSTM for better performance in this experiment based on 1024-D query and video embeddings.,
2007.02503v1,Which method performs the best on LSMDC?,TCE (Visual+Mot.+Aud.),2007.02503v1-Table2-1.png," State-of-the-art performance comparison (%) on LSMDC [33]. Our TCE performs the best with a much lowerdimensional embedding (512-D). The Mot. and Aud. refer to the motion feature and audio feature, respectively.",
2007.02503v1," What are the different steps involved in the text-to-video retrieval process, as shown in the figure?"," 
1. **Text Embedding:** The query text is first processed using a Recurrent Neural Network (RNN) to generate a word embedding. This embedding captures the semantic meaning of each word in the query.

2. **Latent Semantic Tree:** A latent semantic tree is constructed from the word embeddings using a bottom-up approach. This tree represents the hierarchical relationships between the words in the query.

3. **Constituent Node Embedding:** Each node in the latent semantic tree is embedded using a separate RNN. This embedding captures the semantic meaning of the corresponding phrase or sub-query.

4. **Query Embedding:** The constituent node embeddings are then combined using an attention mechanism to generate a single query embedding. This embedding represents the overall semantic meaning of the query.

5. **Video Embedding:** Each video in the database is also embedded using a Convolutional Neural Network (CNN) and an RNN. The CNN extracts features from individual frames of the video, while the RNN captures the temporal relationships between these features.

6. **Text-Video Joint Embedding Learning:** The query embedding and video embeddings are then projected onto a common embedding space. This allows for the comparison of the query with each video in the database.

7. **Text-to-Video Retrieval:** The videos are then ranked based on their similarity to the query embedding. The most similar videos are returned as the retrieval results.",2007.02503v1-Figure2-1.png, An illustration of our tree-augmented cross-modal encoding method for complex-query video retrieval.,
2007.02503v1,Which method performs the best on the MSR-VTT dataset?,"The proposed method, TCE, performs the best.",2007.02503v1-Table3-1.png, Ablation studies on the MSR-VTT dataset using the standard dataset split [44] to investigate the effects of the tree-based query encoder and the temporal-attentive video encoder. The proposed method performs the best.,
2007.02503v1,"Based on the figure, which model performs better for queries with lengths between 19 and 24 words?",DualGRU,2007.02503v1-Figure3-1.png, Performance comparison of DualGRU and our proposed TCE on MSR-VTT. Queries have been grouped in terms of (a) query lengths and (b) query categories.,
2007.02503v1,What is the relationship between the latent semantic trees and the video clips?,"The latent semantic trees are used to represent the queries, and the video clips are the results of the queries. The weights on the nodes of the trees indicate the importance of each word in the query. The video clips with red marks are the correct ones.",2007.02503v1-Figure4-1.png, Four examples obtained by our TCE model on MSR-VTT. The composed latent semantic trees are presented under the corresponding queries. The normalized node weights are also shown. Videos with red marks are the correct ones.,
2008.13719v2,What is the benefit of using different strides in the feature aggregation process?,"Using different strides in the feature aggregation process helps to gather features within different distances, which relieves information loss problems during long-distance propagation.",2008.13719v2-Figure1-1.png," Feature aggregation illustration. (a) Comparison between CNN semantic segmentation and our method (RESA). The segmentation method with ordinary CNN suffers from bad performance due to severe occlusion. (b) Illustration of feature aggregation. Spatial lane feature can be enriched, owing to horizontal and vertical feature aggregation in a layer. Thus, RESA can infer lanes even if they are occluded. We add different strides to gather features within different distances, which relieves information loss problem during long-distance propagation.",
2008.13719v2,"What are the differences between the segmentation results of the three methods (Seg, SCNN, and RESA) on the CULane dataset?","The segmentation results of the three methods (Seg, SCNN, and RESA) on the CULane dataset differ in their accuracy and ability to handle challenging cases. For example, in the first row of the figure, the Seg method fails to segment the lane markings accurately, while the SCNN and RESA methods are able to do so. In the second row of the figure, the Seg and SCNN methods produce noisy segmentation results, while the RESA method produces a cleaner segmentation result.",2008.13719v2-Figure5-1.png," Examples results from CULane dataset with segmentation method, SCNN, and RESA.",
2008.13719v2,Which network architecture has the lowest false positive rate (FP) on the Tusimple dataset?,SCNN,2008.13719v2-Table3-1.png, Comparison with state-of-the-art results on Tusimple dataset. ResNet-18/34 indicates deeplab (Chen et al. 2017) using resnet18 and resnet34 as backbone.,
2008.13719v2,Which model has the best performance on the 'No line' category?,PINet,2008.13719v2-Table2-1.png," Comparison with state-of-the-art results on CULane dataset with IoU threshold = 0.5. For crossroad, only FP are shown. Res50 indicates deeplab (Chen et al. 2017) using resnet50 as backbone.",
2008.13719v2,What is the performance of the baseline model on the CULane dataset with ResNet-34 backbone?,The baseline model achieves an F1 score of 65.1.,2008.13719v2-Table4-1.png, Experiments of the proposed modules on CULane dataset with ResNet-34 backbone. Baseline stands for 8x upsampling directly after backbone.,
2008.13719v2,Which method achieved the highest F1-measure?,ResNet34 + RESA,2008.13719v2-Table7-1.png, The comparison between SCNN and RESA trained using VGG16 and ResNet34 as backbone.,
2008.13719v2,Which method achieved the highest F1-measure?,RESA_DULR,2008.13719v2-Table5-1.png, Effectiveness of feature aggregation of RESA on CULane dataset with ResNet-34 backbone. † means maximum feature aggregation method.,
2008.13719v2,Which method is the fastest on average across all kernel widths?,RESA.,2008.13719v2-Table8-1.png," Runtime of LSTM, SCNN, and RESA. The iteration in RESA is 4.",
2008.13719v2,What is the best performance of the model on the CULane dataset with ResNet-34 backbone?,"The best performance of the model is achieved at iteration 4, with a F1-measure of 74.5.",2008.13719v2-Table6-1.png, The performance of the model by using different iterations on CULane dataset with ResNet-34 backbone.,
2008.13719v2,What is the purpose of the RESA module in the architecture?,The RESA module is used to extract features from the input image. It does this by propagating information in different directions with different strides.,2008.13719v2-Figure2-1.png," Architecture Design. (a) Overall architecture of our model, which is composed by encoder, RESA and decoder. ‘Dk’, ‘Uk’, ‘Lk’, ‘Rk’ denotes “up-to-down”, “down-to-up”, “right-to-left”, and “left-to-right” respectively at k-th iteration in RESA. (b) RESA U module. In this module, information propagates ”down-to-up” with different strides recurrently and simultaneously. (c) RESA R module. In this module, information propagates “left-to-right” with different strides recurrently and simultaneously.",
2008.13719v2," Which of the following statements is true about information passing in RESA?

A. Information can only pass from left to right.
B. Information can only pass from right to left.
C. Information can pass in both directions.
D. Information can only pass within a single block.", C. Information can pass in both directions.,2008.13719v2-Figure3-1.png," Information passing in RESA when s1 = 1 and s2 = 2. X0 can receive information from X0,X1,X2 and X3 only in two iterations.",
2008.13719v2,What is the purpose of the coarse-grained branch in the Bilateral Up-Sampling Decoder?,The coarse-grained branch is used to quickly obtain a rough upsampled feature map while ignoring much detail.,2008.13719v2-Figure4-1.png, Bilateral Up-Sampling Decoder. Decoder upsamples feature map to 2x size. It is composed by coarse grained branch (left) and fine detailed branch (right). Coarse grained branch is used to get a rough up-sampled feature quickly and ignore much detail. Fine detailed branch is used to fine-tune subtle information loss.,
2008.13719v2,Which dataset has the most frames for training?,CULane,2008.13719v2-Table1-1.png, Datasets description.,
2010.02467v1,What are the different interpretations of the retrocardiac opacity seen on the chest X-ray?,"The radiologist interpreted the retrocardiac opacity as concerning for left lower lobe pneumonia, while the Hier-CNN-RNN (Complete) model interpreted it as normal. The Hier-CNN-RNN (Abnormal) and CXR-CVSE (Abnormal) models both considered the possibility of pneumonia but also raised the possibility of other causes, such as atelectasis or prominent vessels.",2010.02467v1-Figure1-1.png, Example of CXR images (frontal and lateral views) and the associated report. Bolded are abnormal findings in the ground-truth and predictions. The CNN-RNN model trained on the complete reports tends to generate normal findings. Both CNN-RNN models generate repetitive sentences.,
2010.02467v1,Which model has the highest BLEU-4 score?,CVSE + mutual exclusivity,2010.02467v1-Table1-1.png," Comparisons of different models’ clinical accuracy and NLG metrics. Accuracy, precision and recall are the macro-average across all 14 diseases.",
2010.02467v1,What is the most likely diagnosis based on the chest radiographs?,Atelectasis,2010.02467v1-Figure2-1.png, Visualization of the attention maps from our method. ‘Real’ and ‘Prediction’ indicates the ground-truth and predicted abnormal findings.,
2010.02467v1,Which model performs best in terms of accuracy?,"The CVSE + mutual exclusiveness model performs best in terms of accuracy, with a macro-average accuracy of 0.863.",2010.02467v1-Table2-1.png," Detailed Accuracy, precision and recall for different models.",
2010.06324v2,Which algorithm performs best in the cartpole domain?,Metal with L(outer) = L(policy) + L(critic),2010.06324v2-Figure1-1.png, MetaL performance using different outer losses (left) and comparison with D4PG and RC-D4PG (right).,
2010.06324v2,Which algorithm has the higher performance in the cartpole domain?,RC-D4PG has the higher performance in the cartpole domain.,2010.06324v2-Figure6-1.png, Performance (left) and overshoot (right) as a function of the best performing Lagrange learning rate for RC-D4PG (α1 = 0.001) and the worst performing initial Lagrange learning rate for MetaL (α1 = 0.005).,
2010.06324v2,Which method performs better when the initial Lagrange multiplier learning rate is 0.003?,MetaL Rpenalized performs better than RC-D4PG Rpenalized when the initial Lagrange multiplier learning rate is 0.003.,2010.06324v2-Figure7-1.png," The performance of MetaL compared to RC-D4PG in terms of initial fixed learning rates. Even though both MetaL and RC-D4PG are sensitive to the initial fixed learning rate, MetaL consistently outperforms RC-D4PG across all of the tested learning rates.",
2010.06324v2,"Which algorithm achieved the highest performance for the max(0, Jc - beta) metric?",The MetalL algorithm.,2010.06324v2-Table1-1.png," Overall performance across domains, safety coefficients and thresholds.",
2010.06324v2,How does penalizing the reward function affect the performance of the different algorithms?,Penalizing the reward function generally leads to a decrease in performance.,2010.06324v2-Figure2-1.png, Performance as a function of safety coefficient.,
2010.06324v2,Which task requires the highest dimensionality in the observation space?,Quadruped: Walk,2010.06324v2-Table4-1.png, Observation and action dimension for each task.,
2010.06324v2,What is the discount factor used in the D4PG algorithm?,0.99,2010.06324v2-Table5-1.png, Hyperparameters for all variants of D4PG.,
2010.06324v2,Which of the following variables is used to define the state of the Cartpole domain?,"The state of the Cartpole domain is defined by the variables x, ẋ, and θ.",2010.06324v2-Table3-1.png, Safety constraints available for each RWRL suite domain; the constraints we use in this paper are indicated by an asterisk (*).,
2010.06324v2,How does the safety coefficient affect the learning progress of MetaL?,"The safety coefficient appears to have a significant impact on the learning progress of MetaL. For the solvable task, with a safety coefficient of 0.3, the agent is able to learn to maximize the return, whereas for the unsolvable task, with a safety coefficient of 0.05, the agent is unable to learn to maximize the return.",2010.06324v2-Figure4-1.png," The learning progress of MetaL for solvable (left) and unsolvable (middle) constraint tasks. In both cases, MetaL attempts to try and maximize the return (right).",
2010.06324v2,Which domain has the highest average reward for MetaL?,Cartpole,2010.06324v2-Figure3-1.png, Performance per domain. MetaL compared to baselines in terms of average reward and penalized reward across the highest safety coefficient and largest thresholds for each domain.,
2010.06324v2,How does the performance of MetaL compare to MeSh across different domains?,MetaL outperforms MeSh in all domains.,2010.06324v2-Figure5-1.png," MetaL and MeSh performance comparison across domains. As seen in the figure, MetaL clearly outperforms MeSh.",
2010.06324v2,Which domain has the lowest threshold values?,The Cartpole domain has the lowest threshold values.,2010.06324v2-Table6-1.png, Constraint thresholds (β) for each domain.,
2010.06324v2,What is the effect of increasing the safety coefficient on the average reward?,Increasing the safety coefficient generally leads to a decrease in the average reward.,2010.06324v2-Table2-1.png," Average reward (and std. dev. over last 100 episodes at convergence), penalized reward and constraint overshoot for MeSh variants on cartpole (single seed, 0.2 and 0.1 safety coefficients).",
2010.06324v2,"Which algorithm performs the best in terms of penalized return across all domains, safety coefficients, and thresholds?",MetaL.,2010.06324v2-Table7-1.png," Performance per domain. MetaL compared to baselines in terms of return, penalized return and constraint overshoot average across the safety coefficients and thresholds for each domain. MetaL is statistically significantly better than RS-0.1 (the best overall reward shaping baseline) in three out of four domains and is better than RS-1.0 in two out of four domains. Overall, MetaL is statistically significantly better than all the baselines across all domains, safety coefficients and thresholds with all p-values less than 1e− 09.",
2010.08865v1,Which dataset has the highest percentage of hateful content?,Twitter.,2010.08865v1-Table1-1.png, Statistics of the three datasets.,
2010.08865v1,Which language model has the most parameters?,BERT-base.,2010.08865v1-Table2-1.png, Parameters Comparison between HABERTORVAFOQF vs. other LMs. “–” indicates not available.,
2010.08865v1,Which model performed the best on Yahoo Finance in terms of AUC?,HABERTOR-VQF with an AUC of 95.72,2010.08865v1-Table3-1.png," Performance of all models that we train on Yahoo train data, test on Yahoo test data and report results on Yahoo News and Yahoo Finance separately. Best baseline is underlined, better results than best baseline are bold.",
2010.08865v1,What is the output of the self-attention block in the BERT architecture?,Context embeddings.,2010.08865v1-Figure4-1.png, General view of the BERT architecture. Uncovering the architecture from left to right.,
2010.08865v1,What are the differences between the real-valued representation and the quaternion representation?,"The real-valued representation uses four separate parameters to compute each output dimension, while the quaternion representation uses only four parameters to compute all four output dimensions.",2010.08865v1-Figure3-1.png," Comparison between linear Euclidean transformation (Left) and linear Quaternion transformation (Right). The Hamilton product in Quaternion space is replaced with an equivalent dot product in real space for an easy reference. Computing each output dimension in real-valued transformation (left) always need 4 new parameters, resulting in 16 degrees of freedom. In contrast, only 4 parameters are used and shared in producing all 4 output dimensions in Quaternion transformation, leading to a better inter-dependency encoding and a 75% of parameter saving.",
2010.08865v1,Which model performs the best on the Twitter dataset?,HABERTOR-VQF performs the best on the Twitter dataset.,2010.08865v1-Table4-1.png," Generalizability of HABERTOR and top baselines. Report AUC, AP, and F1 on each test set.",
2010.08865v1,"Which model performs better on Twitter and Wiki datasets in terms of F1 score, the traditional HABERTOR-VAEQF model or the HABERTOR-VAEQF model with the proposed fine-grained and adaptive noise magnitude?",The HABERTOR-VAEQF model with the proposed fine-grained and adaptive noise magnitude performs better on both Twitter and Wiki datasets in terms of F1 score.,2010.08865v1-Table5-1.png," Comparison of the traditional FGM with a fixed and scalar noise magnitude, compared to the FGM with our proposed fine-grained and adaptive noise magnitude. Better results are in bold.",
2010.08865v1,What are the key differences between traditional fine-tuned BERT and HABERTOR multi-source ensemble heads?,"The key differences between traditional fine-tuned BERT and HABERTOR multi-source ensemble heads are that HABERTOR has two sources of input, News and Finance, and an ensemble of two classification heads, while traditional fine-tuned BERT only has one source of input and one classification head.",2010.08865v1-Figure1-1.png, Architecture comparison of traditional fine-tuned BERT and HABERTOR multi-source ensemble heads.,
2010.08865v1,How does the performance of HABERTOR change as the number of epochs for the pretraining task increases?,The performance of HABERTOR generally improves as the number of epochs for the pretraining task increases.,2010.08865v1-Figure5-1.png, AUC and AP of HABERTOR without regularized adversarial training on Yahoo dataset when varying the number of epochs for the pretraining task.,
2010.08865v1,Which model performs the best on the sentiment classification task?,BERT-base,2010.08865v1-Table6-1.png, Application of our models on the sentiment classification task using Amazon Prime Pantry reviews.,
2010.08865v1,What is the difference in the distribution of noise magnitude between the Twitter and Wiki datasets?,"The Twitter dataset has a more uniform distribution of noise magnitude, while the Wiki dataset has a more peaked distribution with a higher frequency of noise magnitudes around 2.5.",2010.08865v1-Figure2-1.png," Histogram of the learned noise magnitude when performing Language Model transfer learning of HABERTOR on (a) Twitter, and (b) Wiki datasets.",
2010.08865v1,What is the impact of adding more attention heads on the performance of HABERTOR on the Yahoo dataset?,"Adding more attention heads generally improves the performance of HABERTOR on the Yahoo dataset, as evidenced by the higher AUC, AP, and FI scores for models with more heads.",2010.08865v1-Table7-1.png," Ablation study of HABERTOR on Yahoo dataset (i.e. both Yahoo News + Finance, to save space). Default results are in bold. Better results compared to the default one are underlined.",
2010.12306v2,What is the role of labeled data in the SML process?,Labeled data is used to train the classifiers.,2010.12306v2-Figure1-1.png, Social Machine Learning (SML) diagram.,
2010.12306v2,Which agent in the network has the lowest empirical risk?,Agent 1,2010.12306v2-Figure2-1.png," Network classifiers for handwritten digits classification. Leftmost panel: Network topology. Middle panel: Empirical risk evolution for agent 1 (in blue), the rest of the agents (different shades of gray) and for the network average empirical risk (in red). Rightmost panel: Decision variable of agent 1 over the prediction phase, where the dashed line indicates the decision boundary between digits 1 and 0.",
2011.05506v2,Which batch of images has the highest percentage of OOD data?,Batch F,2011.05506v2-Figure1-1.png,"Fig 1: Percentage of images that yield the given score/probability for either SoftMax or EVM. In (a) and (b), we show the percentage for six different batches of 100 images. Reliability is about to determine when the system’s accuracy will be degraded by OOD samples. Can you determine which of the six batches (A–F) are ”normal” and which have increased OOD data? As a hint, three are normal; the others have 3%, 5%, and 20% OOD data. See the text for answers. Open-set classifiers deal with OOD inputs rejecting inputs below a threshold on score/probability. Plot (c) shows the cumulative percentage of images above threshold for knowns and below threshold for unknowns, i.e., the percentage of data correctly handled at that threshold. Assessing a change in reliability, i.e., detecting a shift in the distribution of knowns and unknowns, is a non-trivial problem because of the overlap of scores on knowns and OOD images. This paper formalizes and explores solutions to the open-world reliability problem.",
2011.05506v2,Which policy has the highest total detection percentage?,KL EVM.,2011.05506v2-Figure2-1.png,Fig 2: Performance of proposed policies when the threshold is selected to maximize the sum of on-time + late detection rates validation test with 2% unknown.,
2011.05506v2,Which algorithm has the highest reliability score when the percentage of unknown is 25%?,KL SoftMax,2011.05506v2-Figure5-1.png,Fig 5: Reliability Score of proposed algorithms when the best threshold for each point is selected.,
2011.05506v2,Which policy has the highest total accuracy when the threshold is selected to maximize the total accuracy validation test with 2% unknown?,The Bivariate KL policy has the highest total accuracy when the threshold is selected to maximize the total accuracy validation test with 2% unknown.,2011.05506v2-Figure6-1.png,Fig 6: Performance of proposed policies when the threshold is selected to maximize the total accuracy validation test with 2% unknown. Compare with Fig 2 in main paper.,
2011.05506v2,How does the true detection rate of the three policies change as the percentage of unknown increases?,"The true detection rate of all three policies increases as the percentage of unknown increases. However, the rate of increase is different for each policy.",2011.05506v2-Figure3-1.png,Fig 3: True detection performance with different sliding windows sizes for the proposed policies when the threshold is selected to limit false detection to at most 1%. We show true detection rate with is sum of on-time and late detection.,
2011.05506v2,How does the performance of the Bivariate KL Fusion Algorithm change as the percentage of unknown increases?,"The breadth of good algorithm performance increases, and the peak performance moves slightly to the right (to higher thresholds).",2011.05506v2-Figure4-1.png,"Fig 4: Ablation study on performance of Bivariate KL Fusion Algorithm Section 5 when the threshold changes. As the percentage of unknown increases, the breadth of good algorithm performance increases, and the peak performance of moves slightly to the right (to higher thresholds).",
2011.05506v2,Which policy has the highest true detection percentage when the threshold is selected to have 5% early detection?,Bivariate KL,2011.05506v2-Figure8-1.png,"Fig 8: True detection percentage of proposed policies when the threshold is selected to (a) maximize true detection, (b) have 5% early detection, (c) have 1% early detection, (d) have not early detection on validation test with 2% unknown.",
2011.05506v2,Which policy has the highest total accuracy?,mean SoftMax,2011.05506v2-Figure7-1.png,Fig 7: Performance of proposed policies when the threshold is selected to have less than 1% early detection on validation test with 2% unknown. Compare with Fig 2 in main paper.,
2011.13721v1,How many levels are there in the iterative core extraction with l = 2?,4,2011.13721v1-Figure1-1.png, An iterative core extraction with l = 2,
2012.07619v3,What is the most common type of structure used in subject dictionaries?,Sections and/or paragraphs.,2012.07619v3-Figure4-1.png," Results for the input factor questions. Specific input factor in italics. Answer type in brackets: MC = Multiple Choice, MR = Multiple Response. ** indicates significance (χ2), after Bonferroni correction, with p 0.001. If two options are flagged with **, these options are not significantly different from each other, yet both have been chosen significantly more often than the other options.",
2012.07619v3,"Which feature is the most desired by respondents across all disciplines, and which is the least desired?","The most desired feature is the ability to answer questions, while the least desired feature is the ability to provide more detailed summaries.",2012.07619v3-Figure8-1.png," Results for the future feature questions. Answer type in brackets. MC = Multiple Choice, MR = Multiple Response. ** indicates significance (χ2 or Fisher’s exact test), after Bonferroni correction, with p 0.001.",
2012.07619v3,"What is the most common way in which respondents used the search log, and how helpful did they find it?","The most common way respondents used the search log was to refresh their memory, with 91.7% of respondents selecting this option. They also found it to be the most helpful way to use the search log, with 77% of respondents selecting this option.",2012.07619v3-Figure5-1.png," Results for the purpose factor questions. Specific purpose factor in italics. Answer type in brackets: MC = Multiple Choice, MR = Multiple Response, LS = Likert Scale. ** indicates significance (χ2), after Bonferroni correction, with p 0.001, * with p < 0.05. † indicates noteworthy results where significance was lost after correction for the number of tests. If two options are flagged, these options are not significantly different from each other, yet both were chosen significantly more often than the other options.",
2012.07619v3,What are the different factors that can be considered when summarizing a text?,"The figure shows that there are three main categories of factors to consider when summarizing a text: input factors, purpose factors, and output factors. 

* Input factors describe the characteristics of the text to be summarized, such as its structure, scale, medium, genre, and subject type. 
* Purpose factors describe the context in which the summary will be used, such as whether the situation is tied or floating, who the audience is, and what the intended use of the summary is. 
* Output factors describe the characteristics of the summary itself, such as its material, format, and style.",2012.07619v3-Table3-1.png," Overview of different context factors classes defined by Spärck Jones (1998), with descriptions of the factors within these classes.",
2012.07619v3,What was the most common type of summary format according to the participants' responses?,Abstractive text,2012.07619v3-Figure6-1.png," Results for the output factor questions. Specific output factor in italics. Answer type in brackets: MC = Multiple Choice, MR = Multiple Response, LS = Likert Scale. ** indicates significance (χ2 or Fisher’s exact test), after Bonferroni correction, with p 0.001, * with p < 0.05.",
2012.07619v3,What are the three main factors that should be considered when developing methods for automatic text summarization?,"The three main factors that should be considered are input factors, purpose factors, and output factors.",2012.07619v3-Table2-1.png, Implications for future research directions.,
2012.07619v3,What percentage of participants in the study were Master's students and came from a STEM background?,32.24%,2012.07619v3-Figure2-1.png, Participant details.,
2012.07619v3,What is the purpose of the table?,The table shows the different levels of investigation used in the study.,2012.07619v3-Table1-1.png," Levels of investigation. We did not find significant differences for each, but add all for completeness.",
2012.07619v3, What are the three main stages of the survey procedure?," Introduction, Context Factors, and Closing.",2012.07619v3-Figure3-1.png, Overview of the survey procedure.,
2012.09446v1,Which type of model achieved the highest micro-precision measure?,Human,2012.09446v1-Table1-1.png," Results of the average micro-precision measure, evaluated on the RST-DT corpus. Subscripts identify training sets. Best model in each subset is bold.",
2012.09446v1,Which model is the best performing model based on the table?,HAN(2016),2012.09446v1-Table2-1.png," Five-class sentiment accuracy scores trained and tested on the Yelp’13 dataset, subscripts in model-names indicate dataset for unsupervised training. Best model is bold.",
2012.09446v1,What are the differences between the generated tree and the gold-standard tree?,"The generated tree has different branches and leaves than the gold-standard tree. For example, in the generated tree, the node with the number 19 is a child of the node with the number 17, while in the gold-standard tree, the node with the number 19 is a child of the root node.",2012.09446v1-Figure4-1.png, Our generated tree (left) compared to the gold-standard tree (right) for document wsj 1998,
2012.09446v1, Which nodes are in the left subtree of the root node in the generated tree?," Nodes 1, 2, 3, 4, and 5.",2012.09446v1-Figure2-1.png, Our generated tree (left) compared to the gold-standard tree (right) for document wsj 1395,
2012.09446v1,What is the difference between the generated tree and the gold-standard tree?,"The generated tree and the gold-standard tree are both hierarchical representations of the same document. However, the generated tree has a different structure than the gold-standard tree. For example, in the generated tree, node 6 is a child of node 5, while in the gold-standard tree, node 6 is a child of node 7.",2012.09446v1-Figure3-1.png, Our generated tree (left) compared to the gold-standard tree (right) for document wsj 1198,
2012.09446v1,Which of the following statements best describes the relationship between Similar-1 and Similar-2?,Both documents are similar in that they describe positive aspects of a restaurant.,2012.09446v1-Table3-1.png, Representationally similar/different document-encodings based on the cosine similarity. For more examples of the representational similarity and additional tree structure comparisons see Appendix A and B respectively.,
2012.09446v1,"Which document is most similar to the ""Document"" document?",Similar-3,2012.09446v1-Table4-1.png, Representationally similar/different document-encodings based on the cosine similarity.,
2012.09446v1,Which document is most similar to the original document?,Document Similar-2.,2012.09446v1-Table5-1.png, Representationally similar/different document-encodings based on the cosine similarity.,
2012.09446v1,"Which document is most similar to the document ""Worst experience ever! Salsa was spoiled and my chicken taco was disgusting. Even the little I did eat I got food poisoning. Will never return to this restaurant.""?","The document ""Service was marginal and food was not very good. I remember this place being better, but I guess they changed their Menu around a bit ago. Let's just say I like the old one better. I will pass on this place moving forward."" is most similar to the document ""Worst experience ever! Salsa was spoiled and my chicken taco was disgusting. Even the little I did eat I got food poisoning. Will never return to this restaurant.""",2012.09446v1-Table6-1.png, Representationally similar/different document-encodings based on the cosine similarity.,
2012.09446v1,What is the role of the LSTMCompress modules in the T-AE topology?,The LSTMCompress modules are used to compress the dense encodings of the input spans.,2012.09446v1-Figure1-1.png," T-AE (Tree-AutoEncoder) topology for unsupervised tree inference. Inputs and outputs are dense encodings, Êncx represents reconstruction of spans. ℘ represents the pointer-network, g ∼ G(0, 1) denotes the Gumbel-softmax (in the forwardpass with an additional straight-through computation, not shown here).",
2102.11485v3,Which method performs the best on average across all test sets?,Preferential Labeling-10 (Max Prob.),2102.11485v3-Table2-1.png, The results for SAT solving. “Test-k” indicates a test set where each sample has k variables. “Inference-m” indicates m random labelings during inference.,
2102.11485v3,What is the SAT formula represented by the graph?,The SAT formula represented by the graph is ¬x1 ∧ (x1 ∨ ¬x2).,2102.11485v3-Figure1-1.png," A SAT formula can be represented by a graph, where a node xi is a literal (a variable or its negation) and a node ci is a clause (disjunction of literal nodes). The coresponding SAT formula is the conjunction of clauses, and in this example, it is ¬x1 ∧ (x1 ∨ ¬x2).",
2102.11485v3,"Which of the embedding strategies (static, random, or preferential) is most robust to random labelings during inference?",Preferential,2102.11485v3-Figure4-1.png," Error rate versus the number of random labelings during inference. We compare the embedding strategies for training, and all variants use the labeling with the maximum predicted probability for inference.",
2102.11485v3,What is the relationship between the error rate and the number of random labelings?,The error rate decreases as the number of random labelings increases.,2102.11485v3-Figure5-1.png, Error rate versus the number of random labelings during both training and inference.,
2102.11485v3,How many edges are in the graph C4?,4,2102.11485v3-Figure2-1.png," Graph C4, a circle of length 4. This graph is autoisomorhpic under π : 1 7→ 2, 2 7→ 3, 3 7→ 4, 4 7→ 1.",
2102.11485v3, What is the difference between the training and inference stages of the Preferential Labeling approach?," During training, the GNNs are fed with labeled graphs and their outputs are compared to the ground truth labels. This comparison is used to update the weights of the GNNs. During inference, the GNNs are fed with unlabeled graphs and their outputs are used to predict the labels of the nodes in the graphs.",2102.11485v3-Figure3-1.png, An overview of our Preferential Labeling approach.,
2102.11485v3,"What is the accuracy of the GCN model proposed by Li, Chen, and Koltun in 2018 when using the ""Preferential Labeling-10"" method?",The accuracy of the GCN model is 85.04%.,2102.11485v3-Table1-1.png, The results for MIS solving. “Preferential Labeling10” indicates 10 random labelings in both training and inference.,
2103.00562v1,Which service is responsible for building and testing the code?,CodeBuild,2103.00562v1-Figure2-1.png, The Development Pipeline of CREATe,
2103.00562v1, What are the two main sources of data for CREATE?," The two main sources of data for CREATE are scientific literature mining from PubMed and Hindawi, and author-initiated submissions and CREATE invited submissions.",2103.00562v1-Figure3-1.png, The Architecture of CREATe,
2103.00562v1,What is the likely cause of the patient's death?,Respiratory failure.,2103.00562v1-Figure7-1.png," Example network graph visualization representing a clinical case matching the query: “A patient was admitted to the hospital because of fever and cough”. The start of the graph reflects that the patient presented overlapping symptoms of fever and cough, thus aligning with the temporal and semantic structure of the query. The nodes and edges that follow reflect the evolution of the patient’s case, which ultimately ended in death due to respiratory failure.",
2103.00562v1,What are the different types of entities in the figure?,"The different types of entities in the figure are: Age, Occupation, Clinical_event, Nonbiological_location, Duration, Detailed_description, Sign_symptom, Lab_value, Disease_disorder, Medication.",2103.00562v1-Figure4-1.png," Example of the BRAT annotation interface, showing labeled textual elements linked by temporal and semantic relational structures.",
2103.00562v1,What are the two methods used by CREATe-IR to search for extracted named entities?,The two methods used by CREATe-IR to search for extracted named entities are Neo4j and ElasticSearch.,2103.00562v1-Figure6-1.png, The search workflow of CREATe-IR,
2103.00562v1,"What are the three most common categories of case reports, according to the figure?","Cancer, cardiovascular disease, and musculoskeletal disease.",2103.00562v1-Figure1-1.png," Cardiovascular disease accounts for 20% of all case reports, and is the 2nd largest category of case reports after cancer.",
2103.01519v2,What is the effect of increasing the covariance between features on the Hessian spectrum?,Increasing the covariance between features decreases the magnitude of the eigenvalues and increases the number of small eigenvalues.,2103.01519v2-Figure3-1.png," Impact of feature covariance: Hessian spectrum of single- (left) versus multi-bulk (right) , with p = 800, n = 6 000, logistic model in (2) with w∗ = 0p, w = µ ∼ N (0, Ip/p); for C = diag[1p/2; 2 · 1p/2] (left) and C = diag[1p/2; 4 · 1p/2] (right).",
2103.01519v2,How does the eigengap between the isolated eigenvalue and the bulk of the spectrum change as the norm of the weight vector w increases?,The eigengap increases as the norm of the weight vector w increases.,2103.01519v2-Figure7-1.png," Spike (on left-hand side of bulk) due to response model in Corollary 2 in the absence of data signal : (left) Hessian eigenspectrum for ‖w‖ = 2, with an emphasis on the left isolated eigenvalue λ̂w, (middle) eigengap dist(λw, supp(µ)), and (right) dominant eigenvector alignment, as a function ‖w‖ with w ∝ [−1p/2, 1p/2], w∗ = µ = 0, C = Ip, p = 800 and n = 8 000. Results averaged over 50 runs.",
2103.01519v2,How does the empirical Hessian eigenvalue distribution change as the distribution of the data changes from Gaussian to symmetric Bernoulli to Student-t with 7 degrees of freedom?,"The empirical Hessian eigenvalue distribution appears to be relatively stable across different data distributions. The plots for the Gaussian, symmetric Bernoulli, and Student-t distributions all show a similar decreasing trend, with the majority of eigenvalues concentrated in the lower range.",2103.01519v2-Figure5-1.png," While proved here only in the Gaussian case, the empirical Hessian eigenvalue distribution appears rather “stable” beyond Gaussian distribution: with p = 800, n = 1 200, logistic model in (2) with w∗ = 0p, w = µ ∼ N (0, Ip/p) and C = Ip, for Gaussian (left), symmetric Bernoulli (middle), and Student-t distribution with 7 degrees of freedom (right).",
2103.01519v2,What is the relationship between the empirical distribution of g and the scale factor?,"The empirical distribution of g is closely related to the scale factor. In the top right plot, the green vertical line represents the scale factor, and it aligns well with the peak of the empirical distribution of g. This suggests that the scale factor can be used to estimate the empirical distribution of g.",2103.01519v2-Figure4-1.png," Comparison of Hessian eigenspectra with the Marc̆enko-Pastur law in the setting of Figure 2. (Top): Marc̆enko-Pastur-like Hessian with logistic loss; (left) Hessian eigenvalues and (right) empirical distribution of the gis versus the scaling factor (empirically obtained from the distance between the maximum and minimum Hessian eigenvalues to fit the Marc̆enko-Pastur law). Comparing to Figure 2-(right) and Figure 3, we see that changes in the choice of loss and/or feature statistics (e.g., heterogeneous features) result in very different and non-Marc̆enkoPastur-like Hessian spectral behavior. (Bottom): an example of a very non-Marc̆enko-Pasturlike Hessian with exponential loss (left) and the associated gis (right). Note that the scales of the axes are different in different subfigures.",
2103.01519v2,"Which loss function leads to a bounded Hessian eigenvalue distribution, and which leads to an unbounded one?","The logistic loss function leads to a bounded Hessian eigenvalue distribution, while the exponential loss function leads to an unbounded one.",2103.01519v2-Figure2-1.png," Impact of loss function: bounded (left) versus unbounded (right) Hessian eigenvalue distribution, with p = 800, n = 6 000, logistic model in (2) with µ = 0, C = Ip, w∗ = 0 and w = [−1p/2, 1p/2]/ √ p. Emphasis on the bounded Hessian eigen-support with the logistic loss (left) versus the unbounded support with the exponential loss (right).",
2103.02185v1,How does the proposed Meta-ZSL differ from the conventional Meta-ZSL?,The proposed Meta-ZSL is less biased towards specific classes than the conventional Meta-ZSL.,2103.02185v1-Figure1-1.png," Visualization of model representation θ of the conventional and the proposed meta-ZSL using 1 class for query sets. The conventional meta-ZSL (Finn, Abbeel, and Levine 2017) is biased towards class ’building’, while the proposed aligned task distributions can avoid such local optimality.",
2103.02185v1,Which method performed the best on the AWA2 dataset?,TGMZ-Softmax.,2103.02185v1-Table2-1.png, ZSL average per-class Top-1 accuracy results. Attribute-based methods are shown at the top and generative methods are at the bottom. * denotes meta-ZSL method. ψ denotes using CNN-RNN feature for CUB dataset.,
2103.02185v1,Which method has the highest harmonic mean (H) of seen and unseen classes on the CUB dataset?,TGMZ,2103.02185v1-Table3-1.png," GZSL average per-class Top-1 accuracy results. * denotes meta-ZSL method. U and S represent the accuracy score for seen and unseen classes, respectively. H denotes the harmonic mean of U and S. ψ denotes using CNN-RNN feature for CUB dataset.",
2103.02185v1,Which method performs best on the AWA1&aPY dataset?,TGMZ_Softmax,2103.02185v1-Figure3-1.png, Fusion-ZSL average per-class Top-1 accuracies.,
2103.02185v1,What are the differences between the synthetic feature embedding analysis of ZSML's AWA2 and Our AWA2?,"The synthetic feature embedding analysis of ZSML's AWA2 shows more distinct clusters of animals, while Our AWA2 shows more overlap between the clusters.",2103.02185v1-Figure4-1.png, Synthetic feature embedding analysis.,
2103.02185v1,Which classifier performs the best in terms of accuracy?,SVM_APY,2103.02185v1-Figure5-1.png, Hyper-parameter analysis.,
2103.02185v1,What is the role of the task encoder in the TGMZ architecture?,"The task encoder takes as input a set of training examples and outputs a task embedding, which is used to align the task distributions.",2103.02185v1-Figure2-1.png," Architecture of the proposed TGMZ. a is the attribute vector. Discriminator∗, Generator∗ and Classifier∗ denote the modules that are updated by meta-learner.",
2103.02185v1,Which dataset has the highest number of unseen images?,CUB,2103.02185v1-Table1-1.png, Dataset statistics. # denotes number.,
2103.11624v2,What is the ego car in the image?,The ego car is the green car with a star above it.,2103.11624v2-Figure1-1.png," Examples of multimodal motion prediction in complex driving scenarios. For each moving vehicle near the ego car, three plausible future trajectories are predicted by the proposed model.",
2103.11624v2,Which method performed the best in terms of minADE?,mmTrans.+RTS,2103.11624v2-Table1-1.png," Comparison with state-of-the-art methods on the Argoverse test set (K=6). Here, mmTrans. stands for 6-proposal mmTransformer, while mmTrans.+RTS stands for 36-proposal mmTransformer trained with RTS.",
2103.11624v2,What is the effect of the RTS proposal on the performance of mmTransformer?,The RTS proposal improves the performance of mmTransformer.,2103.11624v2-Table2-1.png," Ablation study on the effectiveness of different components of mmTransformer on the Argoverse validation dataset. As shown in the last two rows of Table 2, same model without RTS shows a poorer performance when other condition remain the same.",
2103.11624v2,What is the difference between the results of the 6-proposal mmTransformer and the 36-proposal mmTransformer+RTS?,"The 6-proposal mmTransformer predicts fewer trajectories than the 36-proposal mmTransformer+RTS. The 36-proposal mmTransformer+RTS also appears to predict trajectories with higher confidence scores, as the trajectories with lower scores have been filtered out.",2103.11624v2-Figure4-1.png," Qualitative comparison between mmTransformer (6 proposals) and mmTransformer+RTS (36 proposals) on four driving scenarios from Argoverse validation set. Green rectangle represents the target vehicle. The red and yellow arrows indicate the groundtruth and predicted trajectories with high confidence scores. For a clear visualization, we filter the trajectories with scores lower than the uniform probability in each scenario. It can be seen that most of plausible areas are covered by our prediction results.",
2103.11624v2,Which partition method achieved a lower minADE?,K-means,2103.11624v2-Table3-1.png, Impact of different partition algorithms on the Argoverse validation dataset.,
2103.11624v2,What are the different types of information that are used by mmTransformer to predict the trajectory of an agent?,"The mmTransformer uses the following types of information to predict the trajectory of an agent:

* Surrounding information: This includes a map of the environment and the history of the target agent's trajectory.
* Nearby agent features: This includes the features of the nearby agents, such as their position, velocity, and acceleration.
* Trajectory proposals: This includes a set of candidate trajectories for the target agent.",2103.11624v2-Figure2-1.png," Overview of mmTransformer: The proposed mmTransformer uses stacked transformers as the backbone, which aggregates the contextual information on the fixed trajectory proposals. Proposal feature decoder further generates the trajectories and corresponding confidence scores by decoding each proposal feature through the trajectory generator and selector, respectively. Besides, the trajectory generator and trajectory selector are two feed-forward network, which have the same structure with the FFN in the transformer [31]. The detailed network architecture can be found in appendix.",
2103.11624v2,What is the relationship between the colored points and the regions in the figure?,"The colored points represent the predicted endpoints of trajectories generated by a specific group of proposals (regional proposals), and each group of regional proposals is associated with a specific region in the figure.",2103.11624v2-Figure5-1.png," Visualization of the multimodal prediction results on Argoverse validation set. We utilize all trajectory proposals of mmTransformer to generate multiple trajectories for each scenario and visualize all the predicted endpoints in the figures (left three columns). For clear illustration, we filter the points with confidences lower than the uniform probability(1/K). The background represents all the predicted results and colored points indicate the prediction results of a specific group of proposals (regional proposals). We observe that the endpoints generated by each group of regional proposals are within the associated region. Miss Rate (MR) matrix of regional proposals is shown on the upper right, where the value in each cell (i, j) represents the MR calculated by proposals assigned to region i and ground truth in region j. The proposals possess high accuracy when the GT is located in their region. For reference, the layout of the regions produced by constrained K-means[32] is shown in the bottom right.",
2103.11624v2,How does the number of regions (M) and the number of proposals in each region (N) affect the MR?,The MR generally decreases as the number of regions and the number of proposals in each region increase.,2103.11624v2-Table4-1.png, Impact of number of region M and the number of proposal in each region N on the Argoverse validation dataset.,
2103.11624v2,How does the region-based training strategy work?,"The region-based training strategy works by first distributing each proposal to one of the M regions. These proposals then learn corresponding proposal features through the mmTransformer. Next, the proposals assigned to the region where the GT endpoints locate are selected. Their trajectories and associated confidence scores are generated, and the losses for them are calculated.",2103.11624v2-Figure3-1.png," Overview of the region-based training strategy. We first distribute each proposal to one of the M regions. These proposals, shown in colored rectangles, learn corresponding proposal feature through the mmTransformer. Then we select the proposals assigned to the region where the GT endpoints locate, generate their trajectories as well as their associated confidence scores, and calculate the losses for them.",
2103.11624v2,"What is the purpose of the ""Add & Normalization"" layer in the transformer module?","The ""Add & Normalization"" layer is used to combine the output of the feed-forward network with the output of the self-attention layer. This helps to improve the performance of the transformer by allowing it to learn more complex relationships between the input and output sequences.",2103.11624v2-Figure6-1.png, The detailed structure of the transformer module.,
2103.11624v2, What is the purpose of the partition procedure? , The partition procedure is used to cluster the data points into different groups.,2103.11624v2-Figure7-1.png, Visualization of the partition procedure.,
2104.02862v2,Which method performs best on the All Search task in terms of Rank-1 accuracy?,"The proposed method performs best on the All Search task in terms of Rank-1 accuracy, achieving 60.02%.",2104.02862v2-Table1-1.png, Performance of the proposed method compared with state-of-the-arts. Note that all methods are measured by CMC and mAP on SYSU-MM01 under single-shot mode.,
2104.02862v2,Which method performs the best on the RegDB dataset under the visible-to-thermal setting?,"The ""ours"" method performs the best on the RegDB dataset under the visible-to-thermal setting.",2104.02862v2-Table2-1.png, Comparison with the state-of-the-arts on RegDB dataset under visible-thermal and thermal-visible settings.,
2104.02862v2,What is the role of the information bottleneck B in the VSD framework?,"The information bottleneck B compresses the encoded representation v of the input data x, while still preserving the information relevant for predicting the output y.",2104.02862v2-Figure1-1.png," Illustration of VSD, in which E and B denote the encoder and the information bottleneck, respectively.",
2104.02862v2,Which method is the fastest and how much faster is it than the slowest method?,"The Re-ID Baseline method is the fastest. It is 1.26 times faster than the slowest method, Conventional IB.",2104.02862v2-Table4-1.png, Computational cost of different methods.,
2104.02862v2,Which embedding space appears to have a clearer separation between the two clusters?,The VCD embedding space appears to have a clearer separation between the two clusters.,2104.02862v2-Figure6-1.png, 2D projection of the joint embedding spaces of zIsh and zVsh obtained by using t-SNE on SYSU-MM01.,
2104.02862v2,What is the effect of increasing the dimension of the representation on the discrepancy between I(v; y) and I(z; y)?,The discrepancy between I(v; y) and I(z; y) decreases as the dimension of the representation increases.,2104.02862v2-Figure7-1.png," Evaluation of the discrepancy between I(v; y) and I(z; y), when the dimension of the representation varies. Note the experiments are conduct on SYSU-MM01 dataset under all-search single-shot mode.",
2104.02862v2, What is the role of the information bottleneck (BS) in the proposed LVCD and LVML methods?," The information bottleneck (BS) compresses the representations generated by the encoder (ES) before they are passed to the decoder. This helps to reduce the redundancy and noise in the representations, which can improve the performance of the downstream tasks.",2104.02862v2-Figure2-1.png," Illustration of LV CD and LVML, where the subscripts are used to denote different views and ES , BS refer to the parameter-shared encoder and information bottleneck.",
2104.02862v2, What are the different types of observations used in the Multi-Modal Re-ID network?," The network uses three types of observations: visual appearance (vsp), soft biometrics (vsh), and viewpoint (vv).",2104.02862v2-Figure3-1.png," Network architecture for Multi-Modal Re-ID. EI/S/V andBI/S/V represent the encoder (ResNet-50) and information bottleneck (multi-layer perceptrons), respectively. v and z denote the observations and representations from encoder and information bottleneck, respectively",
2104.05320v1,What is the difference between the number of mentions and the number of single-antecedent anaphors?,The difference between the number of mentions and the number of single-antecedent anaphors is 41454 in the training/development set and 6776 in the test set.,2104.05320v1-Table1-1.png, Statistics about the corpus used for evaluation.,
2104.05320v1,Which method performs better for single-antecedent anaphors?,"The ""Our model"" method performs better for single-antecedent anaphors.",2104.05320v1-Table4-1.png, LEA evaluation on both single- and splitantecedent anaphors. Impsplit indicates the splitantecedent importance.,
2104.05320v1,How do the systems perform on the Recent-5 task?,"The systems perform best on the JOINT and PRE-TRAINED tasks, with F1 scores of 77.1 and 77.9, respectively.",2104.05320v1-Table3-1.png, Separate evaluation of our systems on the test set. Xsplit are the scores for the split-antecedent anaphors.,
2104.05320v1,"Which model performs better for anaphors with 3 or more antecedents, according to the strict criteria?","Our model performs better for anaphors with 3 or more antecedents, according to the strict criteria.",2104.05320v1-Table6-1.png, Scores for anaphors with different number of antecedents.,
2104.15060v1,What are the different ways in which users can control the DriveGAN simulator?,"Users can control the DriveGAN simulator through various means, including: 

1. **Driving controls:** Users can use a steering wheel and speed controls to navigate the simulated environment, just like driving a real car. This is shown in the bottom left corner of the figure. 

2. **Content control:** Users can manipulate the content of the scene by adding or removing objects like cars, buildings, and trees. This is illustrated on the left side of the figure, where users can click on icons to add or remove objects. 

3. **Weather control:** Users can adjust weather conditions within the simulation, such as changing from sunny to rainy or snowy conditions. This is depicted at the top of the figure, where users can click on different weather icons.",2104.15060v1-Figure1-1.png," We aim to learn a controllable neural simulator that can generate high-fidelity real-world scenes. DriveGAN takes user controls (e.g. steering weel, speed) as input and renders the next screen. It allows users to control different aspects of the scene, such as weather and objects.",
2104.15060v1,"What are the different types of models shown in the figure, and how do their predictions differ?","The figure shows four different types of models: World Model, Stochastic Adversarial Video Prediction, GameGAN, and DriveGAN. The World Model is the ground truth, and the other three models are different types of generative models that are trained to predict future video frames based on the current frame and a sequence of actions. The predictions of the different models differ in terms of their quality and temporal consistency. The World Model produces the most realistic and temporally consistent predictions, while the other models produce predictions that are less realistic and/or temporally consistent.",2104.15060v1-Figure8-1.png, Comparison of baseline models. All models are given the same initial screen and sequence of actions. Our model can produce a high-quality temporally consistent simulation that conforms to the action sequence.,
2104.15060v1,Which model performs best in terms of FVD across all three datasets?,Our model.,2104.15060v1-Table1-1.png, Results on FVD [61]. Lower is better.,
2104.15060v1,Which images are reconstructed by the encoder-decoder model?,The right column of images is reconstructed by the encoder-decoder model.,2104.15060v1-Figure9-1.png," Left: original images, Middle: reconstructed images from VAE, Right: reconstructed images from our encoder-decoder model.",
2104.15060v1,Which model performs best across all criteria and datasets?,GameGAN,2104.15060v1-Figure10-1.png, Human evaluation: Our model outperforms baseline models on both criteria.,
2104.15060v1,Which model performed the best in terms of predicting the BEV lane at a look-ahead distance of 100m?,Ground-Truth,2104.15060v1-Table11-1.png, Mean distance from the BEV lane predictions and the fitted quadratic function in meters.,
2104.15060v1,What is the purpose of the BN layer in the Dsingle architecture?,"The BN layer, or Batch Normalization layer, is used to normalize the activations of the previous layer. This helps to improve the training process by reducing the internal covariate shift.",2104.15060v1-Table9-1.png, Dsingle architecture,
2104.15060v1,What is the role of the Dynamics Engine in DriveGAN?,"The Dynamics Engine takes the content latent code zcontent t, theme latent code ztheme t, and action at as input and outputs the content latent code zcontent t+1 and theme latent code ztheme t+1 at time t+1.",2104.15060v1-Figure2-1.png," DriveGAN takes an image xt and action at as input at time t. With encoder ξ, xt is encoded into disentangled latent codes ztheme",
2104.15060v1,How does swapping zaindep affect the objects in a scene?,Swapping zaindep modifies the objects in a scene while keeping the layout consistent.,2104.15060v1-Figure12-1.png," Swapping zaindep modifies objects in a scene while keeping layout, such as the shape of the road, consistent. Top: right turn, Middle: road for slight left, Bottom: straight road.",
2104.15060v1,What is the difference between the replayed sequences with optimized action and stochastic variable sequences from Video A and Video B?,"The replayed sequences with optimized action and stochastic variable sequences from Video A and Video B differ in the actions that are taken. In the first replayed sequence, the actions are taken from Video A, while in the second replayed sequence, the actions are taken from Video B.",2104.15060v1-Figure13-1.png," We optimize action (aA0..T−1, a B 0..T−1) and stochastic variable sequences (εA0..T−1, ε B 0..T−1) for real videos A and B. Let zA0 be the latent code of A’s initial frame. We show re-played sequences using (zA0 , aA, εA), (zA0 , aB , εA) and (zA0 , aA, εB).",
2104.15060v1,Which model performs best on the Carla benchmark?,Our model.,2104.15060v1-Table2-1.png, Results on Action Prediction. Lower is better.,
2104.15060v1,What happens when a user clicks on a grid cell in ztheme?,The content of the grid cell is changed.,2104.15060v1-Figure11-1.png, Users can randomly sample a vector for a grid cell in ztheme to change the cell’s content. The white figner corresponds to the locations a user clicked to modify.,
2104.15060v1, What is the role of the Dynamics Engine in the figure? ," The Dynamics Engine is responsible for producing the next latent codes, given an action and previous latent codes. ",2104.15060v1-Figure18-1.png," Dynamics Engine produces the next latent codes, given an action and previous latent codes. It disentangles content information into action-dependent and action-independent features with its two separate LSTMs. Dashed lines correspond to temporal connections. Gaussian blocks indicate reparameterization steps.",
2104.15060v1,What is the difference between the images in the left column and the images in the other columns?,"The images in the left column are randomly generated, while the images in the other columns are generated with different themes.",2104.15060v1-Figure4-1.png," Left column shows randomly generated images from different environments. By sampling ztheme, we can change theme information such as weather while keeping the content consistent.",
2104.15060v1,What is the role of the encoder ξ in the pretraining stage?,"The encoder ξ produces two latent variables, zcontent and ztheme, which represent the disentangled content and theme of the input image.",2104.15060v1-Figure3-1.png, Pretraining stage learns the encoder and decoder for images. The encoder ξ produces zcontent and ztheme which comprise the disentangled latent space that the dynamics engine trains on. The gaussian blocks represent reparameterization steps [33].,
2104.15060v1,Which method of frame interpolation produces more realistic results?,Interpolation with differentiable simulation.,2104.15060v1-Figure14-1.png, Frame Interpolation We run differentiable simulation to produce Frame 2 given Frame 1. Top: Linear interpolation in latent space does not account for transition dynamics correctly. Bottom: DriveGAN keeps dynamics consistent with respect to the environment.,
2104.15060v1,How does the Dynamics Engine disentangle content information into action-dependent and action-independent features?,The Dynamics Engine uses two separate LSTMs to disentangle content information into action-dependent and action-independent features. The ConvLSTM receives the previous latent code and the current action as input and outputs the action-dependent features. The LSTM receives the previous latent code as input and outputs the action-independent features.,2104.15060v1-Figure5-1.png," Dynamics Engine produces the next latent codes, given an action and previous latent codes. It disentangles content information into action-dependent and action-independent features with its two separate LSTMs. Dashed lines correspond to temporal connections. Gaussian blocks indicate reparameterization steps.",
2104.15060v1, What are the two latent variables that the encoder ξ produces?, zcontent and ztheme,2104.15060v1-Figure17-1.png, The pretraining stage learns the encoder and decoder for images. The encoder ξ produces zcontent and ztheme which comprise the disentangled latent space that the dynamics engine trains on. The gaussian blocks represent reparameterization steps [33].,
2104.15060v1,What are the effects of adding a tree and a building to the optimized sequence?,Adding a tree to the optimized sequence causes the tree to appear in the generated images. Adding a building to the optimized sequence causes the building to appear in the generated images.,2104.15060v1-Figure6-1.png," Differentiable simulation: We can first optimize for the underlying sequence of inputs that can reproduce a real video. With its controllability, we can replay the same scenario with modified content or scene condition.",
2104.15060v1,What are the different ways you can customize the driving simulation in DriveGAN?,"You can customize the driving simulation in DriveGAN by choosing a theme, style, and layout. You can also choose the content of the simulation, such as buildings, hills, and trees.",2104.15060v1-Figure19-1.png, UI for DriveGAN simulator,
2105.05061v1,What is the effect of imposing an orthogonality constraint on L in the proposed method?,Imposing an orthogonality constraint on L leads to better embeddings.,2105.05061v1-Figure2-1.png, Left: Convergence behaviour of our method on the Cars-196 dataset (Krause et al. 2013). Middle: Ablation study showing sensitivity of our method towards α on Cars-196 (Krause et al. 2013) dataset. Right: tSNE embeddings for the test examples of MNIST (top row) and CIFAR-10 (bottom row). The left column represents the embeddings obtained right after random initialization. The embeddings obtained by our method: without orthogonality constraint on L (middle column) and with orthogonality constraint on L (rightmost column). Orthogonality leads to better embeddings (see Table 4).,
2105.05061v1,Which of the five methods has the best retrieval performance?,Ours,2105.05061v1-Figure3-1.png," Qualitative comparison of retrieval performance against state-of-the-art SSDML approaches on the Cars-196 dataset. For a retrieved image, a red box denotes an incorrect retrieval (different class as the query), and a green box denotes a correct retrieval (same class as the query).",
2105.05061v1,Which method performs the best on the CUB-200 dataset according to the NMI metric?,Ours.,2105.05061v1-Table3-1.png, Comparison of our method against state-of-the-art deep unsupervised methods on fine-grained datasets.,
2105.05061v1,Which method performs the best on the CIFAR-10 dataset in terms of R@8?,Ours,2105.05061v1-Table1-1.png," Comparison against state-of-the-art SSDML approaches on MNIST, Fashion-MNIST and CIFAR-10.",
2105.05061v1,Which method performs the best on the CUB-200 dataset?,Our method.,2105.05061v1-Table2-1.png, Comparison against state-of-the-art SSDML approaches on fine-grained datasets.,
2105.05061v1,What is the effect of using orthogonality on the performance of the method?,The use of orthogonality improves the performance of the method on all metrics and datasets.,2105.05061v1-Table4-1.png," Quantitative comparison of the performance of our method, without (w/o orth) and with orthogonality (w/ orth) on the metric parameters.",
2105.05061v1,Which method performs the best on the CUB dataset?,Our method performs the best on the CUB dataset.,2105.05061v1-Table5-1.png, Comparison of our method against supervised deep metric learning baselines on CUB.,
2105.05061v1,What is the purpose of the push and pull forces in the learned metric space?,The push and pull forces are used to move data points in the learned metric space so that points that are semantically similar are closer together than points that are semantically dissimilar.,2105.05061v1-Figure1-1.png," (Best viewed in color) (a) Illustration of our method. The raw image belongs to the MNIST dataset (LeCun et al. 1998), (b) Triplet mining around an anchor za (shown in black), within its k- neighborhood Nk(za), k= 4. Points in blue (z+ 1 , z + 2 ) are more semantically similar (by virtue of propagated affinities) to the anchor, than the points in red (z−3 , z − 4 ). Hence, they should be pulled closer to the anchor in the learned space, compared to the red ones.",
2106.00840v1,Which dataset has the highest difficulty?,MRQA-NQ,2106.00840v1-Figure4-1.png, Distribution of test examples for each dataset based on the log discrimination (logα) parameter (top) and the difficulty (β) parameter (bottom).,
2106.00840v1,Which task format has the most variability in LEH scores?,Sentence-level multiple-choice.,2106.00840v1-Figure1-1.png," Distribution of test examples according to our proposed locally estimated headroom (LEH) scores (§ 4.1.1), which measure the local slope of the Item Characteristic Curve (ICC) for an example at the ability level corresponding to the best model, and thus reflect the effectiveness of that single example at distinguishing between near-state-of-the-art models. Datasets are grouped by task format: classification (green), sentence-level multiple-choice (blue), paragraph-level multiple-choice (red), and span selection (grey). Within each format, the datasets are sorted by their release date. More details on the datasets are given in Table 1.",
2106.00840v1,How does the discrimination parameter (α) affect the shape of the item characteristic curve (ICC)?,"The discrimination parameter (α) affects the steepness of the ICC. A higher α value results in a steeper curve, indicating that the item is better at discriminating between individuals with different ability levels.",2106.00840v1-Figure2-1.png," An example of item characteristic curves (ICCs) with different values for discrimination (α), difficulty (β), and guessing (γ) parameters. p(θ) is the probability of a correct answer for a given θ. θ measures a model’s ability level (higher is better). α governs the steepness of the function, β determines the θ value at which the curve is the steepest, while γ defines the baseline likelihood that an arbitrarily weak model can guess correctly.",
2106.00840v1,What is the relationship between the difficulty of a question and its log discrimination parameter?,"There is a positive correlation between the difficulty of a question and its log discrimination parameter. This means that questions that are more difficult to answer tend to have a higher log discrimination parameter, which indicates that they are better at discriminating between high-ability and low-ability students.",2106.00840v1-Figure5-1.png, Distributions of log discrimination (logα) versus the difficulty (β) parameters for each dataset..,
2106.00840v1,"Which task is more difficult, MNLI or MC-TACO?",MNLI is more difficult than MC-TACO.,2106.00840v1-Table2-1.png, Hardest and easiest examples along with their estimated difficulty score for MNLI and MC-TACO.,
2106.00840v1,Which dataset has the highest human performance for the Classification task?,COPA,2106.00840v1-Table1-1.png, Datasets grouped by their task format and ordered by release year. Cust. denotes cases when we use our own custom split. Metric: evaluation metric used in this study. RoBERTa: model performance using RoBERTaLarge. Human: human performance.,
2106.00840v1,Which model performs best on the ANLI dataset?,RoBERTa-Large.,2106.00840v1-Figure3-1.png," The best validation performance of ALBERT-XXL-v2, RoBERTaLarge, and the smallest MiniBERTa (RoBERTa-Med-Small-1M-2) on each dataset. The full results table with performance of all models is reported in the Appendix (Table 3)",
2106.00840v1,What is the relationship between guessing and log discrimination for the different datasets?,"The relationship between guessing and log discrimination varies across datasets. For some datasets, there is a clear negative correlation between guessing and log discrimination, indicating that as guessing increases, log discrimination decreases. For other datasets, the relationship is less clear.",2106.00840v1-Figure6-1.png, Plots of the log discrimination (logα) versus the guessing (γ) parameters for each dataset.,
2106.00840v1,Which model performed the best on average across all datasets?,RoBERTa-Base (best),2106.00840v1-Figure7-1.png, Average model accuracy over all datasets vs. ability (θ). The three different hyperparameter configurations of each MiniBERTa are represented by a single color for ease of readability. Best viewed in color.,
2106.02689v3,Which model in the table achieves the highest accuracy?,RegionViT-B+ achieves the highest accuracy with 83.8%.,2106.02689v3-Table3-1.png, Comparisons with recent pyramid-like structure-based ViT models on ImageNet1K. The bold numbers indicate the best number within each section.,
2106.02689v3, What is the difference between the RegionViT-Ti and RegionViT-S models?," The RegionViT-Ti and RegionViT-S models differ in their dimensions and number of encoders at each stage. RegionViT-Ti has dimensions of [64, 128, 256, 512] and uses [2, 2, 8, 2] encoders at each stage, while RegionViT-S has dimensions of [96, 192, 384, 768] and uses the same number of encoders as RegionViT-Ti.",2106.02689v3-Table2-1.png," Model architectures of RegionViT. For all networks, the dimension per head is 32 and the expanding ratio r in FFN is 4. The patch size of local tokens is always 4 while the patch size of regional tokens is 4×M .",
2106.02689v3,How does RegionViT differ from ViT and PVT in terms of patch size and attention mechanism?,"RegionViT uses a pyramid structure like PVT, but it also introduces an efficient regional-to-local (R2L) attention mechanism. This mechanism divides the input image into two groups of tokens: regional tokens of large patch size and local ones of small patch size. The two types of tokens communicate efficiently through R2L attention, which jointly attends to the local tokens in the same region and the associated regional token.",2106.02689v3-Figure1-1.png," Regional-to-Local Attention for Vision Transformers. (a) ViT uses a fixed patch size through the whole network, (b) PVT adopts a pyramid structure to gradually enlarge the patch size in the network. Both ViT and PVT uses all tokens in self-attention, which are computationaland memory-intensive. (c) Our proposed approach combines a pyramid structure with an efficient regional-to-local (R2L) attention mechanism to reduce computation and memory usage. Our approach divides the input image into two groups of tokens, regional tokens of large patch size (red) and local ones of small patch size (black). The two types of tokens communicate efficiently through R2L attention, which jointly attends to the local tokens in the same region and the associated regional token. Note that the numbers denote the patch sizes at each stage of a model.",
2106.02689v3,Which model achieves the highest accuracy on the ChestXRay8 dataset?,RegionViT-M,2106.02689v3-Table4-1.png, Results on IN1K with IN21K and transfer learning.,
2106.02689v3,Which backbone and detector combination achieves the highest average precision (AP) for object detection on the MS COCO val2017 dataset with a 3x schedule?,RegionViT-B+ with Mask R-CNN achieves the highest AP of 48.3.,2106.02689v3-Table5-1.png," Object detection performance on MS COCO val2017 with 1× and 3× schedule. The bold number indicates the best number within the section, and for MaskRCNN, both APb and APm are annotated.",
2106.02689v3,Which method uses regional-to-local attention?,Region ViT.,2106.02689v3-Table1-1.png," Comparison to related works. Most works use non-overlapped windows to group tokens, and then propose the corresponding methods to assure the information exchange among regions.",
2106.02689v3,Which model has the highest accuracy on the K400 dataset?,RegionViT-M,2106.02689v3-Table7-1.png, Performance on action recognition.,
2106.02689v3,What is the role of the R2L transformer encoders in the RegionViT architecture?,The R2L transformer encoders are responsible for exchanging information between the regional and local tokens.,2106.02689v3-Figure2-1.png," Architecture of the proposed RegionViT. Two paths are in our proposed network, including regional tokens and local tokens, and their information is exchanged in the regional-to-local (R2L) transformer encoders. In the end, we average all regional tokens and use it for the classification. The tensor shape here is computed based on that regional tokens take a patch of 282 and local tokens take a patch of 42.",
2106.02689v3,What is the difference in accuracy between using all regional tokens and only the corresponding regional tokens?,The difference in accuracy is 0.1%.,2106.02689v3-Table11-1.png, Keys in LSA.,
2106.02689v3,Does the use of regional tokens improve the performance of Mask R-CNN on the MS COCO dataset?,"Yes, the use of regional tokens improves the performance of Mask R-CNN on the MS COCO dataset.",2106.02689v3-Table8-1.png, Ablation on regional tokens.,
2106.02689v3,Which downsampling approach achieves the highest accuracy on ImageNet (IN1K)?,3x3 depth-wise convolution,2106.02689v3-Table9-1.png, Different downsampling approaches.,
2106.02689v3,What is the impact of weight sharing on the number of parameters and FLOPs?,"Weight sharing reduces the number of parameters from 40.4 to 30.6, but has no impact on the number of FLOPs.",2106.02689v3-Table10-1.png, Weight sharing.,
2106.02689v3,Which of the two models is more efficient in terms of FLOPs per accuracy point?,The Shifted-window model is more efficient.,2106.02689v3-Table14-1.png, Comparison of different attentions.,
2106.02689v3,Does the model perform better when using absolute position information or relative position information?,The model performs better when using relative position information.,2106.02689v3-Table12-1.png, Different position information.,
2106.03518v2,What percentage of cause clauses are located within one position of the emotion clause?,About 87%.,2106.03518v2-Figure1-1.png," The distribution of positions of cause clauses relative to their corresponding emotion clauses in the ECE dataset (Gui et al., 2016). Nearly 87% of cause clauses are located near the emotion clause (About 55% are immediately preceding the emotion clause, 24% are the emotion clauses themselves and over 7% are immediately after the emotion clause).",
2106.03518v2,Which model performs best in terms of Precision?,KAG.,2106.03518v2-Table1-1.png, Results of different models on the ECE dataset. Our model achieves the best Precision and F1 score.,
2106.03518v2,Which model performs best when using absolute position information?,PAE-DGL,2106.03518v2-Figure4-1.png," Emotion cause extraction when using relative, absolute or no clause positional information. Our model demonstrates most stable performance without the relative position information.",
2106.03518v2,What is the purpose of the clause graph update step in the KAG framework?,The clause graph update step is used to update the weights of the edges in the clause graph based on the distance between the candidate clause and the emotion clause along their path.,2106.03518v2-Figure2-1.png," The framework of our proposed KAG. Given an input document consisting of eight clauses (C1 · · ·C8), we first extract knowledge paths from ConceptNet between each candidate clause and the emotion clause (§3.1), e.g., two knowledge paths, p1 and p2, are extracted between C1 and the emotion clause C5. (a) Document Encoding. Clauses are fed into a word-level Bi-LSTM and a clause-level Transformer to obtain the clause representations Ĉi. The document embedding D is generated by Dot-Attention between the emotion embedding ĈE and clause embeddings. (b) Path Representations. The extracted knowledge paths are fed into Bi-LSTM to derive path representations. Multiple paths between a clause pair are aggregated into si based on their attention to the document representation D. (c) Clause Graph Update. A clause graph is built with the clause representations Ĉi used to initialise the graph nodes. The K-Edge weight eiE between a candidate clause Ĉi and the emotion clause ĈE are measured by their distance along their path si. (d) Classification. Node representation hi of a candidate clause Ci is concatenated with the emotion node representation hE , and then fed to a softmax layer to yield the clause classification result ŷi.",
2106.03518v2, What clause has the highest attention weight for Ex. 1?," The clause with the highest attention weight for Ex. 1 is ""emotion.""",2106.03518v2-Figure5-1.png, Attention weights among different graph nodes/clauses on Ex.1 and Ex.2.,
2106.03518v2,Which ECE model is the most robust to adversarial samples generated by the respective discriminator?,PAEDGL,2106.03518v2-Table2-1.png, F1 score and relative drop (marked with ↓) of different ECE models on adversarial samples. The listed four ECE models are attacked by the adversarial samples generated from the respective discriminator. Our model shows the minimal drop rate comparing to other listed ECE models across all sets of adversarial samples.,
2106.03518v2,What is the relationship between the adoption of the advice and the worker's happiness?,The adoption of the advice led to the worker's happiness.,2106.03518v2-Figure3-1.png," A document consisting of 8 clauses in the ECE dataset with extracted knowledge paths from the ConceptNet. Words in red are identified keywords. ‘happiness’ is the emotion label of the emotion clause C5. For better visualization, we only display two extracted knowledge paths between ‘adopt’ and ‘happiness’ in the ConceptNet.",
2106.04480v3,What is the difference between the RAE and RAC methods for reversibility-aware RL?,"RAE encourages reversible behavior via auxiliary rewards, while RAC avoids irreversible behavior by rejecting actions whose estimated reversibility is inferior to a threshold.",2106.04480v3-Figure3-1.png, Our proposed methods for reversibility-aware RL. (a): RAE encourages reversible behavior via auxiliary rewards. (b): RAC avoids irreversible behavior by rejecting actions whose estimated reversibility is inferior to a threshold.,
2106.04480v3,What is the effect of increasing the probability of being pushed by the wind on the score of a random policy in the 2D cliff-walking gridworld?,Increasing the probability of being pushed by the wind generally decreases the score of a random policy in the 2D cliff-walking gridworld.,2106.04480v3-Table1-1.png," Scores for a random policy in the 2D cliff-walking gridworld, where p is the probability of being pushed by the wind. Higher is better.",
2106.04480v3,What is the effect of increasing the probability of being pushed by the wind on the score of the random policy with RAC?,Increasing the probability of being pushed by the wind generally decreases the score of the random policy with RAC.,2106.04480v3-Table2-1.png," Scores for a random policy with RAC in the 2D cliff-walking gridworld, where p is the probability of being pushed by the wind. Higher is better.",
2106.04480v3,How does the threshold β affect the number of irreversible side-effects?,"As the threshold β increases, the number of irreversible side-effects decreases.",2106.04480v3-Figure11-1.png," (a): Reward learning curve for PPO+RAC and several thresholds β (average over 10 random seeds). A threshold of 0 means actions are never rejected, and corresponds to the standard PPO. (b): Number of irreversible side-effects (grass pixels stepped on). For β between 0.2 and 0.4, 0 side-effects are induced during the whole learning.",
2106.04480v3,Is there a path from state 1 to state 3?,Yes.,2106.04480v3-Figure9-1.png," Counterexample for additional property 4. The initial state is sampled uniformly amongst {0, 1, 2}.",
2106.04480v3,What is the role of the joint embedding z in the training procedure for the reversibility estimator used in RAC?,The joint embedding z is used to estimate the degree of reversibility of a state-action pair.,2106.04480v3-Figure10-1.png, The training procedure for the reversibility estimator used in RAC.,
2106.04480v3,How can reversibility be estimated?,Reversibility can be estimated in two ways: from an understanding of physics and from experience.,2106.04480v3-Figure1-1.png," High-level illustration of how reversibility can be estimated. Left: from an understanding of physics. Right: ours, from experience.",
2106.04480v3,What does the color in Figure (b) represent?,The color in Figure (b) represents the online reversibility estimation between two consecutive states (logit scale).,2106.04480v3-Figure4-1.png, (a): Training curves of a PPO+RAE agent in reward-free Cartpole. Blue: episode length. Red: intrinsic reward. A 95% confidence interval over 10 random seeds is shown. (b): The x and y axes are the coordinates of the end of the pole relatively to the cart position. The color denotes the online reversibility estimation between two consecutive states (logit scale). (c): The representation of three random trajectories according to θ (angle of the pole) and dθ dt . Arrows are colored according to the learned reversibility of the transitions they correspond to.,
2106.04480v3," 
Which figure demonstrates the optimal path taken by the agent in the Turf environment?"," 
Figure (d), ""PPO+RAE (500k)"" demonstrates the optimal path taken by the agent.",2106.04480v3-Figure5-1.png," (a): The Turf environment. The agent can walk on grass, but the grass then turns brown. (b): An illustrative trajectory where the agent stepped on grass pixels. (c): State visitation heatmap for PPO. (d): State visitation heatmap for PPO+RAE. It coincides with the stone path (red).",
2106.04480v3,"Which algorithm performs better on the Sokoban task, IMPALA or IMPALA+RAE?",IMPALA+RAE performs better than IMPALA on the Sokoban task.,2106.04480v3-Figure6-1.png," (a): Non-trivial reversibility: pushing the box against the wall can be reversed by pushing it to the left, going around, pushing it down and going back to start. A minimum of 17 moves is required to go back to the starting state. (b): Performances of IMPALA and IMPALA+RAE on 1k levels of Sokoban (5 seeds average). (c): Evolution of the estimated reversibility along one episode.",
2106.04480v3,Which approach leads to faster learning?,PPO,2106.04480v3-Figure8-1.png," PPO and RAC (solid lines) vs PPO (dashed lines). At the cost of slower learning (brown), our approach prevents the agent from producing a single irreversible side-effect (green) during the learning phase. Curves are averaged over 10 runs.",
2106.04480v3,How does the reversibility of a trajectory relate to its score?,The reversibility of a trajectory is positively correlated with its score.,2106.04480v3-Figure7-1.png," (a): Mean score of a random policy augmented with RAC on Cartpole+ for several threshold values, with 95% confidence intervals over 10 random seeds (log scale). (b) and (c): The x and y axes are the coordinates of the end of the pole relatively to the cart position. The color indicates the estimated reversibility values.",
2106.04480v3, What is the purpose of the shuffle operation in the proposed self-supervised procedure for precedence estimation?," The shuffle operation is used to randomly reorder the observations, which helps the model learn to estimate the temporal order of events.",2106.04480v3-Figure2-1.png, The proposed self-supervised procedure for precedence estimation.,
2106.04533v3,Which DeiT backbone has the longest update schedule?,DeiT-Tiny,2106.04533v3-Table1-1.png," Details of training configurations in our experiments, mainly following the settings in [2].",
2106.04533v3,Which of the methods is the most efficient in terms of FLOPs?,DeiT-Tiny,2106.04533v3-Figure2-1.png," Top-1 accuracy (%) over FLOPs (×1010) on ImageNet of our methods, i.e., SViTE, S2ViTE, and SViTE+, compared to DeiT baselines, trained on Imagenet-1K only.",
2106.04533v3,Which model architecture achieved the highest Top-1 accuracy on ImageNet-1K?,DeiT-Base (Dense),2106.04533v3-Table4-1.png, Results of S2ViTE with structured sparsity on ImageNet-1K with DeiT-Tiny/Small/Base. Accuracies (%) within/out of parenthesis are the reproduced/reported [2] performance.,
2106.04533v3,Which model achieves the highest accuracy on ImageNet-1K?,DeiT-Small.,2106.04533v3-Table3-1.png, Results of SViTE-Small on ImageNet-1K. Accuracies (%) within/out of parenthesis are the reproduced/reported [2] performance.,
2106.04533v3,Which model has the highest accuracy on ImageNet-1K?,DeiT-Tiny,2106.04533v3-Table2-1.png, Results of SViTE-Tiny on ImageNet-1K. Accuracies (%) within/out of parenthesis are the reproduced/reported [2] performance.,
2106.04533v3,How does the learnable top-k selection work in this framework?,"The learnable top-k selection works by first scoring each input embedding using a scorer function. Then, the top-k embeddings with the highest scores are selected as the most informative tokens.",2106.04533v3-Figure1-1.png," The overall procedure of our proposed sparse ViT exploration framework. Upper Figure: first training ViT for ∆T iterations, then performing prune-and-grow strategies to explore critical sparse connectivities, repreating until convergence. Bottom Left Figure: enforcing either structured or unstructured sparsity to transformer layers in ViT. Bottom Right Figure: first scoring each input embedding and applying the learnable top-k selection to identify the most informative tokens.",
2106.04533v3,Which model has the most structured sparsity?,S2ViTE-Base with 40% Structured Sparsity.,2106.04533v3-Figure4-1.png," Attention probabilities for DeiT-Base, S2ViTE-Base, and SViTE-Base models with 12 layers (rows) and 12 heads (columns) using visualization tools provided in [94]. Attention maps are averaged over 100 test samples from ImageNet-1K to present head behavior and remove the dependence on the input content. The",
2106.04533v3,What is the purpose of the pruned attention heads in the SViTE+-Small model?,The pruned attention heads help the model to focus on the most important parts of the image.,2106.04533v3-Figure5-1.png, Learned patch selection patterns of SViTE+-Small at 10% data and 50% architecture sparsity levels.,
2106.06624v1,What is the rejection rate for the ACAS Xu dataset when using targeted affinity as a guarantee?,0.195 ± 0.001,2106.06624v1-Table2-1.png, Certification results under affinity robustness on ACAS Xu. VRA is given as the fraction of points that are correctly classified and affinity robust. Results are taken as the average over 10 runs; standard deviations are denoted by±.,
2106.06624v1,Which dataset and guarantee combination resulted in the highest clean accuracy?,EuroSAT with the RT3 guarantee.,2106.06624v1-Table1-1.png, Certification results under various notions of robustness. VRA and clean accuracy numbers are given for the VRA/accuracy metric corresponding to the robustness guarantee the respective model was trained for. Results are taken as the average over 10 runs; standard deviations are denoted by±.,
2106.06624v1,What is the difference between the network outputs in (a) and (b)?,"The network outputs in (a) are top-1 robust but not top-2 robust, while the network outputs in (b) are both top-1 and top-2 robust.",2106.06624v1-Figure1-1.png," (a) Example of network outputs that demonstrate top-k robustness is not a relaxation of standard local robustness. The gray arrows denote bounds on the amount each logit can change within a radius of ε. We see that these bounds are not sufficient for class 2 to surpass class 1; however, the bounds are sufficient for class 3 to surpass class 2. Therefore, the point in this example is top-1 robust, but not top-2 robust. (b) Example of network outputs that are simultaneously top-1 robust and top-2 robust.",
2106.06624v1,"Which of the following classes are included in the RT3 robustness guarantee for the ""highway"" class?","annual crop, pasture, residential buildings, herbaceous vegetation, and permanent crop.",2106.06624v1-Figure3-1.png," Samples of EuroSAT instances labeled “highway” (a) or “river” (b) that are rejected (i.e., cannot be certified) by a standard GloRo Net, but certified by an RT3 GloRo Net. The classes included in the RT3 robustness guarantee are given beneath each image. (c) Two visually similar instances with ground truth label “river” (left) and “highway” (right).",
2106.06624v1,Which model is able to label the points as accurately as the standard model?,The RT2 GloRo Net.,2106.06624v1-Figure2-1.png," (a) An example 2D synthetic dataset containing four classes. (b) Decision boundary of a standard model trained on the synthetic dataset. (c) Decision boundary of a GloRo Net [23] trained to be certifiably robust on the synthetic dataset. (d) Decision boundary of an RT2 GloRo Net (see Section 5) trained to be RT2 robust on the synthetic dataset. We observe that the RT2 GloRo Net can label the points as accurately as the standard model, while the GloRo Net must reject some point on the manifold. The RT2 GloRo Net reports a relaxed robustness guarantee (indicated by orange, purple, and cyan) in the regions where the classes overlap. E.g., in the orange region, the RT2 GloRo Net guarantees that no adversary can change the label to blue or green with a small-norm perturbation.",
2106.06624v1,What does the image show?,The image shows random samples of CIFAR-100 instances that are both correctly classified and certified as topk robust for k>1.,2106.06624v1-Figure6-1.png, Random samples of CIFAR-100 instances that are both correctly classified and certified as topk robust for k>1. The classes included in the RT5 robustness guarantee are given beneath each image.,
2106.06624v1,How many different types of objects are shown in the image?,10,2106.06624v1-Figure4-1.png, Samples of CIFAR-100 instances that are both correctly classified and certified as top-k robust for k>1. The classes included in the RT5 robustness guarantee are given beneath each image.,
2106.06624v1,What are the differences between the predictions made by the RT5 GloRo Net and the superclass Affinity GloRo Net?,"The RT5 GloRo Net did not match a single superclass for the points in the image, while the Affinity GloRo Net was able to successfully certify the point.",2106.06624v1-Figure5-1.png," Comparison of robust prediction sets produced by an RT5 GloRo Net (left) and a superclass Affinity GloRo Net (right). Samples are taken from points on which the RT5 GloRo Net did not match a single superclass, while the Affinity GloRo Net was able to successfully certify the point.",
2106.06624v1,Which dataset and guarantee combination uses the largest batch size?,CIFAR-100 with standard guarantee.,2106.06624v1-Table3-1.png, Hyperparameters for each model used in the evaluation in Section 6.,
2106.07218v2,Which pooling method has a lower per-sample loss value?,Both methods have similar per-sample loss values.,2106.07218v2-FigureA.2-1.png,Figure A.2: A comparison of the per-sample loss values of average pooling (x-axis) and edge preserving smoothing followed by average pooling (y-axis). The colorscale is logarithmic. The two methods have a similar per-sample loss values.,
2106.07218v2,"Which pooling method has a lower loss in most cases, average pooling or edge-preserving smoothing followed by max pooling?",Average pooling.,2106.07218v2-FigureA.3-1.png,Figure A.3: A comparison of the per-sample loss values of average pooling (x-axis) and edge preserving smoothing followed by max pooling (y-axis). The colorscale is logarithmic. Average pooling typically has a lower loss.,
2106.07218v2,Which panel shows the difference map between the solution calculated on the modified elevation map and the non-modified elevation map solution?,Panel (d),2106.07218v2-Figure2-1.png," Gradient flow through PDEs. (a) A modified elevation map with a few pixels of the embankment flattened . (d) Difference map between the solution calculated on the modified elevation map and the nonmodified elevation map solution. (c) Gradient magnitude of the modified elevation map, computed by backpropagating through the PDEs. (b) Zoom-in on the modified pixels of the elevation map (top) and on the gradient magnitude map of the same location (bottom).",
2106.07218v2,What is the relationship between the different loss functions used in the study?,"The different loss functions are positively correlated with each other. This means that as the value of one loss function increases, the value of the other loss functions also tends to increase.",2106.07218v2-FigureA.1-1.png,"Figure A.1: Correlation between loss metrics. A comparison of the per-sample loss values for average pooling with different loss functions. The colorscale is logarithmic. The corresponding Pearson correlations from left to right are 0.78, 0.88, and 0.97",
2106.07218v2,How does the performance of the DNN compare to the average pooling method for different loss functions and percentiles?,The DNN generally outperforms the average pooling method for all loss functions and percentiles. This is evident from the lower error ratios for the DNN in the table.,2106.07218v2-TableA.1-1.png,"Table A.1: Generalization over different elevation maps. A comparison between average pooling and the DNN trained in Section 5.3 for different loss functions and percentiles. Notably, on samples with a large error, the DNN obtains more significant improvement (compared to the baseline).",
2106.07218v2,How do the DNN and baseline elevation maps compare in terms of accuracy?,The DNN elevation map achieves better accuracy than the baseline elevation map.,2106.07218v2-FigureA.9-1.png,"Figure A.9: 20 hours of hydraulic flow over a different geographical region. A comparison of the solution obtained on a coarse grid between the baseline and a DNN trained on samples of 1 hour of hydraulic flow. Water flows in from bottom left and top left edges of the river, and flows out for the right edge of the river. (a) The coarse grid elevation map of the baseline (top) and the DNN (bottom). (b) Difference map between the coarse and fine grid solutions along with the Huber loss. Red indicates pixels more inundated in the coarse grid solution, and blue indicates pixels more inundated in the fine grid solution. The solution calculated on the DNN elevation map achieve better accuracy although the DNN was trained on much shorter time periods and a completely different region from which we tuned the training regime.",
2106.07218v2,"Which method preserves hydraulically significant features better, the average pooling baseline or the DNN?",The DNN preserves hydraulically significant features better than the average pooling baseline.,2106.07218v2-Figure4-1.png, Capturing fine details important to water flow. Two patches taken from different elevation maps. The two larger images are the fine grid patches and the two smaller images on the left of each fine grid image are the coarse grid patches of the average pooling baseline (top) and the DNN (bottom). Note the preservation of hydraulically significant features such as canals and embankments in the DNN as opposed to the baseline.,
2106.07218v2,"Which method, DNN or baseline, achieves better accuracy in terms of the Huber loss?",DNN,2106.07218v2-Figure3-1.png," Generalization over different boundary conditions. Leftmost images are coarse grid elevation maps downsampled using a DNN (bottom) and average pooling baseline (top), where the DNN trained on different boundary conditions and the same elevation map. Each column is a comparison of solutions between the DNN and the baseline elevation maps, for different boundary conditions. Each image on those columns is a difference map between the coarse and fine grid solutions along with the Huber loss. Red indicates pixels more inundated in the coarse grid solution, and blue indicates pixels more inundated in the fine grid solution. The color scale, in meters, appears on the right. Solutions calculated on the DNN elevation map achieve better accuracy compared to the baseline.",
2106.07218v2,What is the effect of the DNN on the coarse grid elevation map?,The DNN expands the narrow water passage in the lower left of the image.,2106.07218v2-FigureA.10-1.png,Figure A.10: Example where a narrow water passage (lower left) is expanded. (a) – Baseline coarse grid elevation map (b) – DNN coarse grid elevation map (c) – The correction of the DNN to the baseline. Colorbars in meters.,
2106.07218v2,Which figure shows the difference between the baseline coarse grid elevation map and the DNN coarse grid elevation map?,Figure (c),2106.07218v2-FigureA.12-1.png,Figure A.12: Example where both river bank is expanded and embankments are preserved next to the river. (a) – Baseline coarse grid elevation map (b) – DNN coarse grid elevation map (c) – The correction of the DNN to the baseline. Colorbars in meters.,
2106.07218v2,Which of the three panels shows the largest difference between the baseline and DNN coarse grid elevation maps?,Panel (c).,2106.07218v2-FigureA.11-1.png,Figure A.11: Example where an embankment height is preserved. (a) – Baseline coarse grid elevation map (b) – DNN coarse grid elevation map (c) – The correction of the DNN to the baseline. Colorbars in meters.,
2106.07218v2,What does the color gradient represent in the figure?,"The color gradient represents the error in the model's prediction of the water surface elevation. Blue represents areas where the model underestimates the water surface elevation, while red represents areas where the model overestimates the water surface elevation.",2106.07218v2-FigureA.6-1.png,Figure A.6: Generalization over different boundary conditions. See Fig. 3 for details.,
2106.07218v2,Which model performs better on unseen data?,DNN,2106.07218v2-FigureA.7-1.png,Figure A.7: Generalization over different boundary conditions. See Fig. 3 for details.,
2106.07218v2,What is the difference between the baseline and DNN models in terms of their ability to generalize over different boundary conditions?,The DNN model generalizes better over different boundary conditions than the baseline model.,2106.07218v2-FigureA.8-1.png,Figure A.8: Generalization over different boundary conditions. See Fig. 3 for details.,
2106.07218v2,What is the role of the downsampling neural network fW in the training setup?,"The downsampling neural network fW is used to generate a coarse grid version of the input elevation map z, which is then used to calculate the coarse grid water height ĥ.",2106.07218v2-Figure1-1.png," Training setup for the downsampling neural network fW . Each data sample consists of a fine grid elevation map z, with boundary and initial conditions c. Two solutions are calculated - the fine grid water height h and the coarse grid water height ĥ. The latter uses a coarse grid version of the input elevation map ẑ. We train the downsampling neural network fW , to minimize the loss measuring the distance between the two solutions (eqs. 3 or 4). This is done by backpropagating the loss gradients through the numerical solver.",
2106.07218v2,Which elevation map produced the DNN with the smallest Huber loss?,The DNN trained on the elevation map in Figure 3.,2106.07218v2-FigureA.4-1.png,Figure A.4: Huber loss per-sample comparison of the baseline (x-axis) and the DNN (y-axis). Each panel is a DNN trained on a different elevation map. The colorscale is logarithmic. the panels correspond to the following figures – (a) Fig. 3 (b) Fig. A.5 (c) Fig. A.6 (d) Fig. A.7 (e) Fig. A.8,
2106.07771v1,How do the results of the proposed approach compare to the results of the FewShotV2V approach?,The proposed approach generates more realistic results than the FewShotV2V approach.,2106.07771v1-Figure3-1.png," Qualitative results on the Youtube-Dancing dataset. The first and fifth column show one reference image from a target person. For each target person, we show short sequences, including the ground-truth driving images, and the images generated by our and FewShotV2V [62] using the pose information from the driving images. As seen here, our approach clearly generates more realistic results.",
2106.07771v1,How well does the proposed method perform in generating realistic images from diverse motion patterns?,"The proposed method performs well in generating realistic images from diverse motion patterns. The figure shows that the generated images are visually similar to the ground-truth images, even for complex poses.",2106.07771v1-Figure2-1.png," Qualitative results on the iPER dataset. The leftmost columns show the reference images. For each example, we show both ground-truth (GT) driving images (odd rows), and the images generated by our method using the pose from the GT images (even rows). Our method can generate realistic images from diverse motion patterns.",
2106.07771v1,How does the number of reference images affect the quality of the generated image?,The quality of the generated image improves as the number of reference images increases.,2106.07771v1-Table4-1.png, Analysis of the use of different numbers of reference images from the target subject for image generation on the iPER dataset. The image quality improves with our method as more reference images are used.,
2106.07771v1,"What is the effect of removing the Multi-Resolution TBN, Skip-Connections, and Random Rotations from the Full method?",Removing any of these components decreases the SSIM and increases the LPIPS.,2106.07771v1-Table3-1.png, Ablation analysis of our proposed architecture on the iPER dataset. Our Full method achieves results that are superior to all other variants.,
2106.07771v1,Which method performed the best in terms of SSIM and LPIPS?,Our method performed the best in terms of SSIM and LPIPS.,2106.07771v1-Table1-1.png, Quantitative results on the iPER [34] dataset. We compare our method with existing works using the SSIM and LPIPS.,
2106.07771v1,Which method performs better on the Youtube-Dancing dataset according to the SSIM metric?,Ours,2106.07771v1-Table2-1.png," Quantitative results on the Youtube-Dancing dataset. We report the SSIM, LPIPS, and FID for both methods.",
2106.07771v1,What is the role of the flow network in the pipeline?,The flow network computes the 3D warping field used to warp the implicit volumetric representations from the canonical pose to the target pose.,2106.07771v1-Figure1-1.png," Overall pipeline. Given N reference images (left), we extract implicit volumetric representations at multiple scales using our 2D-3D encoder. Our flow network (bottom left) computes the 3D warping field used to warp these representations from the canonical pose (the first source image, to which the other source image feature volumes are aligned) to the target pose. Our 3D-2D decoder then synthesizes the subject in the target pose (top right), while transformations such as rotations can be applied to synthesize novel views of the subject in this pose (middle right). Our occupancy decoder (bottom right) improves the bottleneck’s spatial disentanglement and thus allows for better motion retargeting. It computes an occupancy volume used to indicate which regions of the volume are occupied by the subject, which is then decoded into a 2D foreground segmentation mask.",
2106.10394v2,What is the difference between decisions made with and without uncertainty?,Decisions made with uncertainty can reveal more preferences than decisions made without uncertainty.,2106.10394v2-Figure1-1.png," One of our key findings is that decisions made under uncertainty can reveal more preferences than clear decisions. Here we give examples of decisions made with and without uncertainty. (a) In the case without uncertainty, nobody would choose to quarantine the traveler, so we cannot distinguish between different people’s preferences. However, in the case with uncertainty, people might decide differently whether to quarantine the traveler depending on their preferences on the tradeoff between individual freedom and public health. This allows us to identify those preferences by observing decisions. (b) Similarly, observing decisions on whether to convict a person under uncertainty reveals preferences about the tradeoff between convicting innocent people and allowing criminals to go free.",
2106.10394v2, What is the relationship between the state and the observation? , The observation is generated according to the conditional distribution of X | Y. ,2106.10394v2-Figure4-1.png," A graphical depiction of the POMDP formulation of IDT described in Appendix B. A state in {s0, s1} is randomly selected at each timestep and an observation is generated according to the conditional distribution of X | Y . An action (decision) is taken and the agent receives reward equal to the negative of the loss.",
2106.10394v2,In which settings is the surrogate loss not identifiable?,The surrogate loss is not identifiable in the setting of IDT for a suboptimal decision maker.,2106.10394v2-Table1-1.png, An overview of which of our results apply in the setting when the decision maker is minimizing a surrogate loss rather than the true loss.,
2106.10394v2,How does the expected reward change as the posterior probability q(x) increases?,The expected reward decreases as the posterior probability q(x) increases.,2106.10394v2-Figure5-1.png," A graphical depiction of the belief state MDP formulation of IDT. There is a belief state for each posterior probability q(x) = P(Y = 1 | X = x) ∈ [0, 1]. Observing the agent at a belief state gives a constraint on their reward function [2]. Thus, if q(X) has support on [0, 1], i.e. if there is a significant range of uncertainty in the decision problem, then there can be arbitrarily many such constraints, allowing the loss parameter c to be learned to arbitrary precision.",
2106.10394v2,What is the relationship between the observed decisions and the probability that the ground truth decision is 1?,The observed decisions are more likely to be 1 when the probability that the ground truth decision is 1 is higher.,2106.10394v2-Figure2-1.png," A visualization of three settings for inverse decision theory (IDT), which aims to estimate c, the parameter of a decision maker’s loss function, given observed decisions ŷ1, . . . , ŷm ∈ {0, 1}. Here, each decision ŷi is plotted against the probability q(xi) = P(Y = 1 | X = xi) that the ground truth (correct) decision Y is 1 given the decision maker’s observation xi. Lemma 4.1 shows that an optimal decision rule assigns ŷi = 1{q(xi) ≥ c}. (a) For uncertain decision problems, IDT can estimate c as the threshold of posterior probabilities q(xi) where the decision switches from 0 to 1 (Section 4.1). If the distribution of q(X) has probability density at least pc on [c − ǫ, c + ǫ], Theorem 4.2 shows we can learn c to precision ǫ with m ≥ O(1/(pcǫ)) samples. (b) When there is no uncertainty in the decision problem, IDT cannot characterize the loss parameter c because the threshold between positive and negative decisions could be anywhere between 0 and 1 (Section 4.4). (c) A suboptimal human decision maker does not use an optimal decision rule for any loss parameter c, but we can often still estimate their preferences (Sections 4.2 and 4.3).",
2106.10394v2,How does the figure illustrate the lack of MD-smoothness?,The figure shows that shifting either decision rule slightly causes a jump in minimum disagreement with the other hypothesis class from 0 to a positive value.,2106.10394v2-Figure6-1.png," A visualization of the distribution and decision rules used in Lemma D.3 to show that a lack of MD-smoothness can prevent identifiability of the loss parameter c. On the left, the distribution over X = (X1, X2) and Y is shown; X has constant density on unit squares in the first and third quadrants, and P(Y = 1 | X) varies as shown with the heatmap. We consider two decision rules h1 and h2 which are optimal thresholds of X1 and X2, respectively, for loss parameters c1 = 2/5 and c2 = 3/5, respectively. Since c1 6= c2 but P(h1(X) = h2(X)) = 1, it is impossible to identify c reliably. This is because the distribution and decision rules are not MD-smooth, since shifting either decision rule slightly causes a jump in minimum disagreement with the other hypothesis class from 0 to a positive value.",
2106.10394v2,What is the difference between the known suboptimal and unknown suboptimal cases?,"In the known suboptimal case, the decision maker knows which hypothesis class H they are choosing from, while in the unknown suboptimal case, the decision maker does not know which hypothesis class H they are choosing from.",2106.10394v2-Figure3-1.png," We analyze IDT for optimal decision makers and two cases of suboptimal decision makers. (a) In the optimal case (Section 4.1), the decision maker chooses the optimal decision rule h for their loss parameter c from all possible rules. (b) In the known suboptimal case (Section 4.2), the decision maker chooses from a restricted hypothesis class H which may not contain the overall best decision rule. (c) In the unknown suboptimal case (Section 4.3), the decision maker chooses any of several hypothesis classes H ∈ H and then uses the optimal rule within that class, which may not be the optimal rule amongst all classes. This case is more difficult than (b) because we often need to identify the hypothesis class H in addition to the loss parameter c.",
2106.13531v1,Which method has the highest mean PESQ score?,The UNet method has the highest mean PESQ score.,2106.13531v1-Table4-1.png, Performance Before Linear AEC Convergence.,
2106.13531v1,Which value of α resulted in the highest mean PESQ score?,α = 0,2106.13531v1-Table5-1.png, Performance for Different Values of α.,
2106.13531v1,Which model performed the best on the PESQ metric?,The U-Net model performed the best on the PESQ metric with a mean score of 3.3.,2106.13531v1-Table3-1.png, Performance with Echo Change.,
2106.13531v1,Which method has the highest mean PESQ score?,The U-Net method has the highest mean PESQ score of 3.61.,2106.13531v1-Table2-1.png, Performance without Echo Change.,
2106.13531v1,What is the difference between SAR and SDR?,"SAR is a measure of the signal-to-noise ratio (SNR) of the near-end single talk signal, while SDR is a measure of the SNR of the double talk signal.",2106.13531v1-Table1-1.png, Performance Measures for RES.,
2106.13531v1,What is the purpose of the AEC block in the echo cancellation system?,The AEC block is used to estimate and cancel the echo signal that is present in the microphone signal.,2106.13531v1-Figure1-1.png, Echo cancellation system. Time indices are neglected.,
2107.03444v1,What are the different components of the KiS method reward?,"The KiS method reward has three components: fluency, salience, and simplicity.",2107.03444v1-Figure1-1.png," Motivating example for the KiS method, based on a CBS article (Lewis, 2021). We optimize a three-component reward: fluency, salience and simplicity. We show model outputs when trained with all three components, and with a missing component.",
2107.03444v1,Which model performed the best in the training KiS models experiment?,The 8-SCST model performed the best.,2107.03444v1-Figure5-1.png," Training KiS models comparing SCST with k-SCST. We try 4, 6 and 8 as values for k. Increasing k improves performance and stability.",
2107.03444v1,Which model achieves the highest coverage score?,The ACCESS Default model achieves the highest coverage score.,2107.03444v1-Table1-1.png," Automatic results on Newsela test-set. SARI and BLEU are reference-based metrics. %FKGL and %Lexile are percentages of model outputs lowering the grade level. Comp. is the average compression ratio (# words), and Cov. the output’s average coverage score.",
2107.03444v1,What are the factors that the Keep it Simple model considers when scoring candidate simplifications?,"Fluency, simplicity, and salience.",2107.03444v1-Figure2-1.png," Keep it Simple is an unsupervised training procedure for text simplification. The text generator (GPT-2) produces candidate simplifications, scored according to fluency, simplicity, salience. Guardrails enforce the model does not learn high-scoring shortcuts.",
2107.03444v1,What is the purpose of the S_Score function?,The S_Score function is used to calculate a score that measures the difference between the Flesch-Kincaid grade level of an original text and a simplified version of that text.,2107.03444v1-Figure3-1.png, SScore algorithm. fkgl computes the Flesch-Kincaid grade level.,
2107.03444v1,How did the libraries adapt their camps to the pandemic?,They brought camp into kids' homes by sending them materials and holding sessions virtually.,2107.03444v1-Figure6-1.png," Example Task (from a Washington Post article (Kelati, 2020)) for the Comprehension Study. Shown are two of five stimuli: original document (left), and KiS model output (right). Participants read a text and answered comprehension questions (bottom). Average completion time was 160 seconds (original) and 136 seconds (KiS model output).",
2107.03444v1,Which model has the best compression-accounted speed-up?,The KiS Model has the best compression-accounted speed-up.,2107.03444v1-Table2-1.png," Results of the Human Comprehension Study. We measure average completion time (Time), number of submissions (#Subs.), compression ratio (Comp.) and a compression-accounted speed-up (CASpeed). Each text version is assigned a symbol used to indicate statistical significance (p < 0.05).",
2107.03444v1,What is the relationship between the FKGL of the original paragraph and the change in FKGL in the Newsela rewrite?,The change in FKGL tends to be larger when the FKGL of the original paragraph is higher.,2107.03444v1-Figure4-1.png," Analysis (Kernel Density Estimate plot) of change in Flesch-Kincaid Grade Level in the paired Newsela dataset. Most simple paragraphs have lower FKGL than the original paragraphs (positive ∆FKGL). When the original paragraph’s FKGL is higher (x-axis), the change in FKGL tends to be larger (y-axis). We fit a linear approximation, which we use to compute the Sscore.",
2108.06401v1,What is the role of the transformation network in the proposed model?,The transformation network is responsible for converting audio features into imagined visual features.,2108.06401v1-Figure1-1.png," Proposed model and training pipeline. In sub-figure (a), Audio-visual pairs of the Youtube 8M dataset are used to train the encoder, decoder and transformation network (Blue lines). In sub-figure (b), The audio recording of DCASE dataset is used to train a classifier (Green lines) which predicts the category of the audio recording when the transformation network is fixed.",
2108.06401v1,Which method performs the best on unseen data?,IVF (Q),2108.06401v1-Table1-1.png, Acoustic Scene Classification Accuracy using different features on DCASE.,
2108.06401v1,Which method performed the best on the ESC-50 dataset?,Transfer learning.,2108.06401v1-Table3-1.png, Acoustic Scene Classification Accuracy on ESC-50.,
2108.06401v1,Which method performed the best on unseen cities?,Ours.,2108.06401v1-Table2-1.png, Acoustic Scene Classification Accuracy at the seen cities and unseen cities on DCASE.,
2108.08887v2,Which method performed the best when there was moderate noise and the training set size was 1000?,SPO+,2108.08887v2-Figure1-1.png," Normalized test set SPO loss for the SPO, SPO+, least squares, and absolute loss methods on portfolio allocation instances.",
2108.08887v2,How does the presence of moderate noise affect the SPO loss of the SPO+ method with a linear predictor?,The presence of moderate noise increases the SPO loss of the SPO+ method with a linear predictor.,2108.08887v2-Figure2-1.png, Test set SPO loss for the SPO+ methods with different feasible regions on the costsensitive multi-class classification instances.,
2108.08887v2,How does the normalized excess risk change as the training sample size increases?,The normalized excess risk decreases as the training sample size increases.,2108.08887v2-Figure3-1.png, Normalized test set excess risk for the SPO+ methods on instances with polyhedron and level-set feasible regions. For each value of the sample size in the above plots we run 50 independent trials.,
2108.08887v2,How does the SPO loss of the SPO+ method with barrier compare to the SPO loss of the SPO+ method without barrier when the training set size is 100 and there is moderate noise?,The SPO+ method with barrier has a lower SPO loss than the SPO+ method without barrier when the training set size is 100 and there is moderate noise.,2108.08887v2-Figure4-1.png," Test set SPO loss for the SPO+, least squares, and absolute loss methods on cost-sensitive multi-class classification instances. For each value of the polynomial degree in the above plots we run 50 independent trials.",
2108.09435v1,How does the performance of FFML change as the batch size increases?,The performance of FFML generally increases as the batch size increases.,2108.09435v1-Figure6-1.png, FFML performance across various batch sizes.,
2108.09435v1,Which dataset requires the largest number of iterations?,Crime,2108.09435v1-Table3-1.png, Hyperparameter configurations of FFML.,
2108.09435v1,Which model performs the best on the Adult dataset in terms of Demographic Parity?,The full model performs the best on the Adult dataset in terms of Demographic Parity.,2108.09435v1-Figure7-1.png, Ablation study of FFML on Adult and Crime datasets.,
2108.09435v1,What is the role of the fairness function in the meta-learning algorithm?,The fairness function is used to ensure that the learned model is fair with respect to the different fairness notions.,2108.09435v1-Table1-1.png, Important notations and corresponding descriptions.,
2108.09435v1,Which method consistently performs the best across all datasets and validation ratios?,"The method labeled ""Ours"" consistently performs the best across all datasets and validation ratios.",2108.09435v1-Figure3-1.png, Evaluation using fair metric EO at each round.,
2108.09435v1,What is the role of the task buffer in the update procedure?,The task buffer stores the tasks that are used to update the parameter pair.,2108.09435v1-Figure1-1.png," An overview of update procedure stated in Step 3. At round 𝑡 , new task is added in the the buffer. The parameter pair (𝜽 𝑡 ,𝝀𝑡 ) are iteratively updated with fairness constraints through a bi-level optimization in which the inner and the outer interplay each other.",
2108.09435v1,Which of the methods needs the least amount of data to learn new tasks on the Communities and Crime dataset?,GenOLC,2108.09435v1-Figure4-1.png, Amount of data needed to learn each new task.,
2108.09435v1,Which model performs the best in terms of demographic parity?,"The ""full ours"" model performs the best in terms of demographic parity.",2108.09435v1-Figure5-1.png, Ablation study of our proposed models. (1) w/o inner FC: FFMLwithout inner fairness constraints; (2)w/o aug: FFML without the augmented term in Eq.(10); (3) w/o aug + outer FC: FFML without the augmented term and outer fairness constraints.,
2108.12603v3,How is the aggregated label for each text example in the weakly labeled data generated?,"The aggregated label is generated by combining the outputs of the weak rules. Each weak rule assigns a label to each text example, and these labels are then combined to generate the aggregated label.",2108.12603v3-Figure1-1.png," WALNUT, a benchmark with 8 NLU tasks with real-world weak labeling rules. Each task in WALNUT comes with few labeled data and weakly labeled data for semi-weakly supervised learning.",
2108.12603v3,"Which rule performed the best on the IMDB dataset, and what was its F1 score?","Rule 4 performed the best on the IMDB dataset, with an F1 score of 0.463.",2108.12603v3-Table16-1.png, Performance of each rule on IMDB.,
2108.12603v3,Which rule has the highest F1 score on the test set?,Rule 5.,2108.12603v3-Table15-1.png, Performance of each rule on AGNews.,
2108.12603v3,Which rule has the highest F1 score on the Yelp test set?,Rule 3.,2108.12603v3-Table17-1.png, Performance of each rule on Yelp.,
2108.12603v3,Which rule performed the best on the test set according to the F1 score?,Rule 3 performed the best on the test set with an F1 score of 0.724.,2108.12603v3-Table18-1.png, Performance of each rule on GossipCop.,
2108.12603v3,Which rule performed the best on the test set according to the F1 score?,Rule 5,2108.12603v3-Table19-1.png, Performance of each rule on NCBI.,
2108.12603v3,What does the code snippet do?,"The code snippet defines a class called CancerLike that implements a rule for detecting cancer-related entities. The class has a method called apply_instance that takes an instance as input and returns a list of labels. The method first converts the tokens in the instance to lowercase and then initializes a list of labels with the string ""ABS"" for each token. It then iterates over the tokens and checks if they end with any of the suffixes in the suffixes list. If a token ends with a suffix, the corresponding label is set to ""I"". Finally, the method returns the list of labels.",2108.12603v3-Figure8-1.png, Example of weak rule from the NCBI dataset (rule 3: CancerLike from Table 11). This rule heuristically detects entities that are relevant to cancer.,
2108.12603v3,What are the three types of number patterns that the number_generator function can detect?,"The three types of number patterns that the number_generator function can detect are cardinal, ordinal, and quantity.",2108.12603v3-Figure7-1.png, Example of weak rule from the CoNLL dataset (rule 16: number_detector from Table 10). This rule heuristically detects entities that are relevant to numbers.,
2108.12603v3, What does the StopWords class do? ," The StopWords class is a tagging rule that identifies stop words in a text and assigns them the ""O"" tag, indicating that they are not relevant to any disease. ",2108.12603v3-Figure9-1.png, Example of weak rule from the NCBI dataset (rule 11: StopWords from Table 11). This rule heuristically detects stop words and assigns the ‘O’ tag to the corresponding tokens by assuming that they are not relevant to any disease.,
2108.12603v3,Which model performs best on average across all tasks?,RoBERTa-large,2108.12603v3-Table2-1.png, Main results on WALNUT with F1 score (in %) on all tasks. The rightmost column reports the average F1 score across all tasks. (MLC is not shown for BERT-large and RoBERTa-large due to OOM.),
2108.12603v3,What is the purpose of the `apply_instance` method in the `Feelings` class?,"The `apply_instance` method takes an instance as input and returns a list of labels. The method iterates over the tokens in the instance and checks if the token is a feeling word and if the next token is ""the"". If both conditions are met, the method labels the current token, the next token, and the token after the next token as ""O"", ""O"", and ""I"", respectively. Otherwise, the method labels all tokens as ""ABS"".",2108.12603v3-Figure10-1.png, Example of weak rule from the LaptopReview dataset (rule 5: Feelings from Table 13). This rule heuristically detects entities that are relevant to laptop features based on keywords that express the user’s feelings.,
2108.12603v3, Which method has the highest performance gain over no weak supervision (C)?, DistilBERT,2108.12603v3-Table4-1.png," Overall performance gain and gap of all weak supervision methods (Weak Sup, by averaging performance of W, Snorkel, C+W, C+Snorkel, GLC, MetaWN and MLC) against no weak supervision (C) and full clean training. Note that RoBERTa-large in included here, as the standard deviation of its performance with different splits on tasks varies significantly (See Table 14 in Appendix) hence using its performance mean as an indicator is less conclusive.",
2108.12603v3,Which model architecture performed the best on average across all tasks?,RoBERTa-large,2108.12603v3-Table3-1.png," Average F1 score across the eight tasks in WALNUT. The bottom row computes the average F1 score across tasks and supervision methods. The three right-most columns report the average F1 score across model architectures and all tasks (“All”), document-level tasks (“Doc”), and token-level tasks (“Token”).",
2108.12603v3,Which dataset has the most number of weak rules?,CoNLL,2108.12603v3-Table1-1.png, Statistics of the eight document- and token-level tasks in WALNUT. See Section 3.2 for details.,
2108.12603v3,Which rule performed the best on the test set in terms of F1 score?,Rule 2,2108.12603v3-Table21-1.png, Performance of each rule on LaptopReview.,
2108.12603v3,Which rule performs the best on the test set according to the F1 score?,Rule 10.,2108.12603v3-Table20-1.png, Performance of each rule on WikiGold.,
2108.12603v3,Which model performs the best on the CONLL dataset?,RoBERTa-large,2108.12603v3-Table14-1.png, Standard deviation results on WALNUT.,
2108.12603v3,What is the relationship between precision and recall for the weak supervision rules?,"There is a negative relationship between precision and recall for the weak supervision rules. As precision increases, recall decreases, and vice versa. This is evident in all of the scatterplots, which show a downward trend in the data points.",2108.12603v3-Figure3-1.png, Scatterplots of precision-recall for weak supervision rules. Each point corresponds to a rule. (GossipCop is omitted as it contains only three rules.),
2108.12603v3,What is the purpose of the `keyword_price` function?,The `keyword_price` function is used to determine the sentiment of a piece of text with respect to price.,2108.12603v3-Figure4-1.png, Example of weak rule from the Yelp dataset (rule 6: keyword_price from Table 8).,
2108.12603v3,What does the code in the image do?,The code defines a function called `textblob_lf` that takes an argument `x` and returns a string indicating the sentiment of the text in `x`.,2108.12603v3-Figure5-1.png, Example of weak rule from the Yelp dataset (rule 1: textblob_lf from Table 8).,
2108.12603v3,What are the different types of tokens that the money_detector rule looks for?,"The money_detector rule looks for tokens that are either digits, currency codes, or currency symbols.",2108.12603v3-Figure6-1.png, Example of weak rule from the CoNLL dataset (rule 3: money_detector from Table 10). This rule heuristically detects entities that are relevant to money.,
2108.12603v3,"Which dataset shows the biggest performance gap between the ""Clean"" and ""Clean + Weak"" approaches when the number of clean instances per class is 5?",CoNLL,2108.12603v3-Figure2-1.png," F1 score by varying (in the x-axis) the number of clean instances per class considered in the clean training set (DC). The importance of weak supervision is more evident for settings with smaller numbers of instances, where the gap in performance between the “Clean” approach and “Clean+Weak” approach is larger. For a robust evaluation across tasks, WALNUT provides five clean/weak splits per task. See Section 3.3 for details.",
2109.04553v2,What is the relationship between the number of latent factors and the performance of the NMF model?,"The performance of the NMF model generally increases with the number of latent factors, but there is a point of diminishing returns.",2109.04553v2-Figure4-1.png, Ablation on K,
2109.04553v2,Which method has the highest mIoU?,The basic method has the highest mIoU.,2109.04553v2-Table2-1.png, Ablation on components of Hamburger with NMF Ham.,
2109.04553v2,How does the mean IoU change with increasing values of d and r?,The mean IoU generally increases with increasing values of d and r.,2109.04553v2-Figure3-1.png, Ablation on d and r,
2109.04553v2,How does the temperature affect the mloU?,The temperature does not appear to have a significant effect on the mloU.,2109.04553v2-Table9-1.png, Influence of temperature T with CD ham.,
2109.04553v2,What is the relationship between K and CD?,"As K increases, CD decreases.",2109.04553v2-Figure8-1.png, Impacts of K on CD,
2109.04553v2,What is the effect of increasing the value of K on the NMF evaluation score?,The evaluation score generally increases as the value of K increases.,2109.04553v2-Figure7-1.png, Impacts of K on NMF 1 2 3 4 5 6 7 8 9 10 12 15 20 30 eval,
2109.04553v2,How does the value of K affect the VQ score?,The VQ score generally decreases as the value of K increases.,2109.04553v2-Figure9-1.png, Impacts of K on VQ,
2109.04553v2,Which method has the lowest GPU load during inference?,Ham (NMF),2109.04553v2-Table3-1.png, Comparisons between Hamburger and context modules.,
2109.04553v2,"What is the relationship between the ""ham"" and the ""upper and lower bread""?","The ham represents the latent representation of the input data, which is obtained by applying a matrix decomposition to the input data. The upper and lower bread represent linear transformations that are applied to the latent representation.",2109.04553v2-Figure1-1.png, Overview of Hamburger,
2109.04553v2,Which method has the highest mIoU score?,"HamNet has the highest mIoU score, with a score of 85.9%.",2109.04553v2-Table4-1.png, Comparisons with state-of-the-art on the PASCAL VOC test set w/o COCO pretraining.,
2109.04553v2,Which method achieves the highest mIoU score?,HamNet.,2109.04553v2-Table5-1.png, Results on the PASCAL-Context Val set.,
2109.04553v2,Which method achieves the lowest FID score on ImageNet 128×128?,HamGAN-strong,2109.04553v2-Table6-1.png, Results on ImageNet 128×128. ∗ are from Tab. 1 and Tab. 2 of Zhang et al. (2019a).,
2109.04553v2,What is the difference between the Jacobian matrix of y with respect to x and the Jacobian matrix while taking h_{t-1} as a constant?,"The Jacobian matrix of y with respect to x includes the partial derivatives of y with respect to h_{t-1}, while the Jacobian matrix while taking h_{t-1} as a constant treats h_{t-1} as a constant and does not include these partial derivatives.",2109.04553v2-Table7-1.png, Summary of notations in this paper,
2109.04553v2,Which initialization method leads to the best performance for NMF?,"The ""online"" initialization method leads to the best performance for NMF.",2109.04553v2-Table8-1.png, Ablation on initializations.,
2109.04553v2,Which method has the highest accuracy for One-Step Gradient?,NMF,2109.04553v2-Table1-1.png, One-Step Gradient & BPTT,
2109.04553v2,How does the one-step gradient method work?,The one-step gradient method uses the current input (x) and the previous hidden state (h^(t-1)) to compute the current hidden state (h^t). The current hidden state is then used to compute the output (y). The gradient of the loss function with respect to the hidden state is also computed and used to update the hidden state. This process is repeated until the loss function is minimized.,2109.04553v2-Figure2-1.png, One-Step Gradient,
2109.05105v1,What is the difference between the two sentences in the figure?,"The two sentences differ in the size of the object that does not fit into the container. In the first sentence, the trophy is too big to fit into the suitcase. In the second sentence, the medal is too small to fit into the box.",2109.05105v1-Figure1-1.png," WSC sample: a) original sentence, b) perturbation (noun synonym). Task: resolve pronoun with a candidate. The trigger-word induces an answer flip.",
2109.05105v1, What are the two types of perturbations used in the proposed approach?, The two types of perturbations used in the proposed approach are synonym and tense.,2109.05105v1-Figure2-1.png," Schematic illustration of the proposed approach. Two examples xi and xj from the WSC dataset, both demonstrating the concept triplet (container, item, fit) and their generated embeddings (dashed outline) for two perturbation types: top: [SYNOYM] and bottom: [TENSE]. Loss terms defined as attraction (←→)",
2109.05105v1,Which method performed the best in zero-shot commonsense reasoning on the Winobias Pro dataset?,"The method that performed the best in zero-shot commonsense reasoning on the Winobias Pro dataset was the ""Ours"" method, which achieved an accuracy of 75.12% with BERT and 75.76% with RoBERTa.",2109.05105v1-Table1-1.png, Results for zero-shot commonsense reasoning,
2109.05105v1,Which method performed the best in terms of W.G. accuracy?,RoBERTa,2109.05105v1-Table2-1.png," Ablation study, performance in accuracy (%)",
2109.06122v2,What are the different types of questions that can be asked about the image?,"The different types of questions that can be asked about the image are original, propagation, and paraphrasing questions.",2109.06122v2-Figure1-1.png," Illustration of our approach SIMPLEAUG. We show a training image and its corresponding question-answer pairs in VQA v2 (Goyal et al., 2017), and our generated pairs. A VQA model (Anderson et al., 2018) trained on the original dataset just cannot answer these new questions on the training image correctly, and we use them to improve model training.",
2109.06122v2,How many samples are there in the SIMPLEAUG dataset that are not correctly answered by a UpDn model trained on the original dataset?,1.489K,2109.06122v2-Table1-1.png, Statistics on VQA-CP v2 training data. Missanswered: the number of SIMPLEAUG examples that a UpDn model trained on the original dataset cannot answer correctly.,
2109.06122v2,Which image shows a person riding a skateboard?,The first image.,2109.06122v2-Figure4-1.png," Examples in human study. For each image, we pick 5 triplets created by SIMPLEAUG (4 by propagation Y/N, Num, Other, Color and 1 by paraphrasing) and ask crowd workers to evaluate the IQA triplets by the question’s relatedness (1 / 0) to the image and the answer’s correctness (1 / 0) to the image and question.",
2109.06122v2,Which method performs the best on the VQA v2 val set for all answer types?,SIMPLEAUG (propagation + paraphrasing),2109.06122v2-Table2-1.png," Performance on VQA v2 val set and VQA-CP v2 test set. Our method SIMPLEAUG (cyan background) consistently improves all answer types for different base models on both VQA v2 and VQA-CP. Note that MUTANT (loss) (Gokhale et al., 2020a) (gray color) applies extra loss terms besides data augmentation.",
2109.06122v2,Which training strategy results in the highest accuracy for the UpDn model on VQA-CP v2?,"The training strategy that results in the highest accuracy for the UpDn model on VQA-CP v2 is ""O -> A -> O"".",2109.06122v2-Table4-1.png, A comparison of training strategies on VQA-CP v2 with the UpDn model. O: original triplets. A: augmented triplets by SIMPLEAUG.,
2109.06122v2,Which augmentation type resulted in the highest accuracy for the Y/N category?,"The augmentation type ""Y/N"" resulted in the highest accuracy for the Y/N category.",2109.06122v2-Table5-1.png," Effects of different augmention types (cf. § 3.2). We report results on VQA-CP v2, using the UpDn model.",
2109.06122v2,How does verification affect the performance of SIMPLEAUG on VQA-CP v2?,Verification improves the performance of SIMPLEAUG on VQA-CP v2.,2109.06122v2-Table3-1.png," SIMPLEAUG (propagation) w/ or w/o verification (cf. § 3.2) on VQA-CP v2, using the UpDn model.",
2109.06122v2,Why does the pipeline use other images besides the original one?,The pipeline uses other images besides the original one to help augment the original question-answer pairs. This is done by matching the objects in the original image to objects in other images and then propagating the question-answer pairs to those other images.,2109.06122v2-Figure2-1.png," The SIMPLEAUG pipeline. We show four original question-answer pairs of the image on the left in VQA v2, and how they are propagated to other images. The green boxes are annotated in MSCOCO or detected by Faster R-CNN; each of them is associated with an object name and/or attribute. We only show boxes matched by nouns or used to derive answers.",
2109.06122v2,Which method performed the best according to the table?,UpDn.,2109.06122v2-Table6-1.png, Learning with limited IQA triplets on VQA-CP v2. We keep a certain fraction of QA pairs per image.,
2109.06122v2,What is the effect of using different types of annotations on the performance of a VQA model?,"The performance of the VQA model generally improves when more types of annotations are used. For example, the model performs better when using both MSCOCO ground truth annotations and Faster R-CNN object detection annotations than when using only MSCOCO ground truth annotations.",2109.06122v2-Table7-1.png, Learning with weakly-labeled and unlabeled images for VQA v2. Fraction: the portion of images with annotated QA pairs. GT: MSCOCO ground truth annotations. OD: Faster R-CNN object detection. 7: supervised training with only labeled VQA training examples.,
2109.06122v2,How many people are actively playing in the game in the photograph?,Two.,2109.06122v2-Figure3-1.png," Qualitative results. We show the training image and its QA pairs from VQA-CP, and the generated QA pairs by SIMPLEAUG. 3/7 indicates if the baseline VQA model (trained without SIMPLEAUG) answers correctly/incorrectly. In augmented QA pairs, the first three are from question propagation and the last one is by paraphrasing.",
2109.06122v2,Which of the augmented QA pairs are more difficult for the baseline VQA model to answer correctly?,"The augmented QA pairs that are more difficult for the baseline VQA model to answer correctly are the ones that are not directly related to the visual content of the image. For example, the question ""What is the color of the field?"" is more difficult to answer than the question ""What color is the elephant?"" because the color of the field is not explicitly shown in the image.",2109.06122v2-Figure5-1.png," Additional qualitative results on VQA-CP. We show the original image, the generated QA pairs by SIMPLEAUG, and the original QA pairs. 3/7 indicates if the baseline VQA model (trained without SIMPLEAUG) predicts correctly/incorrectly.",
2109.10431v2,How does the accuracy of FairMIPForest compare to the accuracy of other fairness intervention approaches on the COMPAS and HSLS datasets?,FairMIPForest generally has higher accuracy than other fairness intervention approaches on both the COMPAS and HSLS datasets.,2109.10431v2-Figure2-1.png," Comparison of Fair MIP Forest with existing fairness intervention approaches (Zafar et al. 2019; Hardt, Price, and Srebro 2016; Agarwal et al. 2018) coupled with mean or k-NN imputations. Baseline indicates the result of training a decision tree without any fair intervention. Error bars show the standard error after 10 runs with different train-test splits.",
2109.10431v2, Which dataset used the largest batch size?, HSLS,2109.10431v2-Table4-1.png, Summary of hyperparameters used in Fair MIP Forest.,
2109.10431v2,What are the missing statistics for the Adult and COMPAS datasets?,The missing statistics are the marginal probabilities for each feature in each dataset. These are represented by the columns  𝑝0ms  and  𝑝1ms .,2109.10431v2-Table3-1.png, Missing statistics for Adult and COMPAS datasets we generated for our experiments.,
2109.10431v2,Which variable has the highest missing probability for females?,X1PAR2EDU (Secondary caregiver's highest level of education),2109.10431v2-Table2-1.png, Data missing probabilities of different variables between demographic groups: male vs. female or white vs. under-represented minority (URM) in the HSLS dataset,
2109.10431v2,What is the type of the variable X1FAMINC?,Continuous,2109.10431v2-Table1-1.png, Description of features used in the HSLS dataset,
2109.10431v2," 
Explain how the branching decisions are made at each node of the tree."," 
The branching decisions are made based on the values of the features and the splitting thresholds. For example, at node 1, the branching decision is based on whether feature_3 is less than or equal to 1.5 or missing. If feature_3 is less than or equal to 1.5 or missing, then the example goes to the left child node (node 2). Otherwise, it goes to the right child node (node 3).",2109.10431v2-Figure1-1.png," Demonstration of MIP notations for decision trees with MIA using a depth-2 tree example. The data dimension d = 4, and P is a 3-by-4 matrix where each row dictates which feature a branching node uses for split. In the first row of P, the third element is one, so feature 3 is used for splitting at branch node 1. The first element in q and c are used as a splitting threshold missing values splitting at branch node 1. We also show how prediction is made for the example data point xi = [−5 100 2.2 ∗]. The branching decisions wi,v’s are computed for all v (highlighted in yellow). From the computed wi,v’s, the algorithm decides the leaf node it belongs to, i.e., zi. In this case zi = [0 0 0 1]. Hence, ŷi = u4 = 0.",
2110.00171v1,Which model performed the best on the Twitter dataset according to the Macro-F1 metric?,RoBERTa4GCN,2110.00171v1-Table1-1.png, Comparisons of BERT4GCN with various baselines. The w/o pos. indicates the one without using relative position module and w/o att. is without using supplemented dependency graph. Accuracy (Acc.) and Marco-F1 are used for metrics. And we report the average of 10-fold experimental results (%).,
2110.00171v1,What is the difference between the distributions of distances between aspect and opinion terms on the Laptop and Restaurant datasets?,The distribution of distances between aspect and opinion terms on the Laptop dataset is more skewed towards smaller distances than the distribution on the Restaurant dataset. This means that aspect and opinion terms are more likely to be closer together in the Laptop dataset than in the Restaurant dataset.,2110.00171v1-Figure2-1.png, Distributions of the distances between aspect and opinion terms on Laptop and Restaurant datasets.,
2110.00171v1,Which dataset is most sensitive to the size of the relative position window?,Twitter,2110.00171v1-Figure1-1.png," Effect of the size of relative position window w on Twitter (bottom), Laptop (mid) and Restaurant (top) datasets.",
2110.00171v1,Which dataset has the most neutral examples?,Twitter,2110.00171v1-Table2-1.png, Statistics of datasets,
2110.00857v3,Which method performs better in terms of fairness as the heterogeneity across clients increases?,Local Reweighting performs better in terms of fairness as the heterogeneity across clients increases.,2110.00857v3-Figure1-1.png," Comparison of local/global debiasing under different heterogeneity levels α. Smaller α indicates more heterogeneity across clients. For the Equal Opportunity Difference (EOD) metric, values closer to 0 indicate better fairness.",
2110.00857v3,Which method achieves the best accuracy on the Adult dataset with a heterogeneity level of 0.1?,FedAvg,2110.00857v3-Table1-1.png," Performance comparison of data partition with different heterogeneity levels α. A smaller α indicates a more heterogeneous distribution across clients. We report the average of 20 random seeds. For EOD and SPD metrics, values closer to zero indicate better fairness. Positive fairness metrics indicate that the unprivileged group outperform the privileged group. For brevity, we report the values achieved by the best local debiasing baseline (without FairFed) as Local / [Best] in the table.",
2110.00857v3,Which fairness metric is most affected by changes in the fairness budget β on the Adult dataset?,EOD.,2110.00857v3-Figure3-1.png, Effects of fairness budget β for 5 clients and heterogeneity α = 0.2 on FairFed with local reweighting.,
2110.00857v3,How many clients are there in the dataset?,There are five clients in the dataset.,2110.00857v3-Table5-1.png," An example of the heterogeneous data distribution (α = 0.5) for K = 5 clients on the target variable used in the experiment on the Adult dataset, where each client is assigned only points with a single sensitive attribute value.",
2110.00857v3,What is the effect of increasing the α parameter on the Equalized Odds Difference (EOD) for the 0.8/0.2 Reweighting/FairBatch client ratio?,The EOD generally increases with increasing α.,2110.00857v3-Figure6-1.png, Effects of different local debiasing strategies (mixture of reweighting and FairBatch clients).,
2110.00857v3,Which reweighting strategy achieves the highest accuracy for clients with only data from one sensitive group?,Global Reweighting.,2110.00857v3-Figure7-1.png, Performance with clients that only contain data from one sensitive group.,
2110.00857v3,What is the difference between FairFed and other federated learning frameworks?,"FairFed is a group fairness-aware federated learning framework, while other federated learning frameworks may not be. This means that FairFed takes into account the fairness of the model for different groups of people, while other frameworks may not.",2110.00857v3-Figure2-1.png, FairFed: Group fairness-aware FL framework.,
2110.00857v3,Which state has the largest number of people in the dataset?,California,2110.00857v3-Figure4-1.png, Demographic distribution of ACSIncome dataset.,
2110.00857v3,Which method performs best on the TILES dataset in terms of EOD?,FairFed / RW.,2110.00857v3-Table3-1.png, Performance on ACSIncome and TILES datasets.,
2110.00857v3,What is the percentage of CNA workers who are female?,61%,2110.00857v3-Table2-1.png, Data distribution of TILES dataset.,
2110.00857v3,How does the reweighting client ratio affect the EOD?,"As the reweighting client ratio increases, the EOD decreases.",2110.00857v3-Figure5-1.png, Effect of only a subset of clients adopting the reweighting debiasing method.,
2110.00857v3, What is the effect of increasing the heterogeneity parameter α on the distribution of the sensitive attribute A (sex) in the Adult and COMPAS datasets?, Increasing the heterogeneity parameter α increases the degree of non-IIDness in the distribution of the sensitive attribute A (sex) in both datasets.,2110.00857v3-Table4-1.png, An example of the heterogeneous data distribution (non-IID) on the sensitive attribute A (sex) used in experiments on the Adult and COMPAS datasets for K = 5 and heterogeneity parameters α = 0.1 and α = 10.,
2110.00857v3,How does the parameter η affect the standard deviation of accuracy for the COMPAS dataset?,The standard deviation of accuracy for the COMPAS dataset increases as η increases.,2110.00857v3-Figure8-1.png, Effects of parameter η on the performance of FairFed for K = 5 clients and heterogeneity parameter α = 0.5.,
2110.00857v3,Which method has the highest accuracy on the Adult dataset with a heterogeneity level of 0.5?,FedAvg,2110.00857v3-Table6-1.png, Performance comparison of uniform accuracy constraint η on data partition with different heterogeneity levels α. We report the average performance of 20 random seeds.,
2110.00857v3,Which fairness metric is most affected by the value of η on the ACS Income dataset?,Std-Acc,2110.00857v3-Figure9-1.png, Effects of η on the ACS Income and TILES datasets.,
2110.01077v3,What are the different tasks that the multi-task training framework can solve?,The multi-task training framework can solve keyword spotting and speaker verification tasks.,2110.01077v3-Figure1-1.png, Overview of our proposed multi-task training framework with wav2vec 2.0 based SRE backbone and various downstream networks to solve voice activated tasks.,
2110.01077v3,Which combination of downstream heads and datasets achieved the best performance in terms of SV-EER?,"The combination of SRE + Linear multi and VoxCeleb2 + GSC1 (12) achieved the best performance in terms of SV-EER, with a score of 1.98%.",2110.01077v3-Table2-1.png," Results of our multi-task learning experiments using different downstream heads and dataset combinations, and performance of prior state of the art methods on the datasets.",
2110.01077v3,Which dataset has the most utterances?,VoxCeleb2 Train,2110.01077v3-Table1-1.png, Statistics of the datasets used in our experiments. We use the GSC datasets for KWS and VoxCeleb datasets of SV. #K refers to the number of speakers for SV and number of keywords for the GSC dataset.,
2110.02001v2,"What is the relationship between the word ""agency"" and the word ""needs"" in the sentence ""He says the agency seriously needs money to develop""?","The word ""agency"" is the subject of the sentence, and the word ""needs"" is the main verb. This means that the agency is the entity that is performing the action of needing.",2110.02001v2-Figure1-1.png, Illustration of the ORL structure (d) based on an example sentence (b) with its corresponding dependency structure (a) and POS tags (c).,
2110.02001v2,Which method has the highest decoding speed?,The Transition method has the highest decoding speed.,2110.02001v2-Figure5-1.png, Comparisons on decoding speed.,
2110.02001v2,What is the effect of removing the boundary detection features on the F1 score for the O-R(hd) task?,The F1 score decreases by 0.16.,2110.02001v2-Table3-1.png," The ablation results (exact F1). ‘w/o Opn’ means building UDOG without using the opinion-role structure, and ‘w/o Dep’ without using dependency structure.",
2110.02001v2,Which model achieved the highest F1 score in the O-R (tg) category?,"SyPtrTrans with BERT achieved the highest F1 score in the O-R (tg) category, with a score of 59.48.",2110.02001v2-Table1-1.png, Main results in exact F1 scores. Baselines with the superscript ‘†’ are copied from Xia et al. (2021).,
2110.02001v2,Which model performed the best in terms of binary F1 score?,Transition†,2110.02001v2-Table2-1.png, The results in binary and proportional F1 scores.,
2110.02001v2,Which model performs the best when there are more than six opinion-role pairs in a sentence?,SyPtrTrans,2110.02001v2-Figure8-1.png, Performances (O-R) with different opinion-role pair number in a sentence.,
2110.02001v2,Which model performs the best in the O-R (tg) setting?,PtrTrans,2110.02001v2-Figure6-1.png," Performance drops of an ORL system comparing to SPANOM. We re-implement a BERT+sequence-labeling model as the Pipeline baseline: i.e., first extracting terms and then role type relation classification.",
2110.02001v2,How does the performance of different term extraction methods change with increasing term length?,The performance of all term extraction methods decreases with increasing term length.,2110.02001v2-Figure7-1.png, Results of term extraction by varying term lengths.,
2110.02001v2,What is the role of the 'Ptr' column in the transition process?,The 'Ptr' column is used to detect the end boundaries of opinion or role terms.,2110.02001v2-Figure2-1.png," An illustration of the transition process. ‘Ptr’ means PointNet for end boundary detection of opinion or role terms. The underlined number in β and ‘Ptr’ marked with green or red denotes the start or end index of opinion or role terms, respectively.",
2110.02001v2,"What happens to the stack when the action ""ARC"" is predicted?",The top element of the stack is popped.,2110.02001v2-Figure3-1.png, The illustration of transition step 3 to step 4 with the vanilla neural model.,
2110.02001v2, How does the RCGA (b) encode the UDOG (a)?," The RCGA encodes the UDOG by representing each word in the sentence as a node in a graph, and connecting nodes based on their syntactic relationships. The edges in the graph are labeled with the type of relationship, such as ""nsubj"" for subject and ""obj"" for object.",2110.02001v2-Figure4-1.png," Encoding UDOG (a) via RCGA (b). The resulting high-order representation is used for action prediction, term end prediction, and role type detection.",
2110.04375v2,What are some of the applications of link prediction?,"Link prediction can be used to predict protein interactions, drug responses, and complete the knowledge graph.",2110.04375v2-Figure1-1.png, The topological organizing rules are not universal across graphs.,
2110.04375v2,"Which algorithm performed better on the Yeast dataset, WP or the second-best algorithm?",The WP algorithm performed better on the Yeast dataset than the second-best algorithm.,2110.04375v2-Table7-1.png, p-value by comparing WP and second best algorithm on eight datasets with no attributes.,
2110.04375v2,Which algorithm performs best on the E.coli dataset?,WP (ones),2110.04375v2-Table8-1.png, Prediction accuracy measured by AP on eight datasets (90% observed links) without node attributes. Boldface letters are used to mark the best results while underlined letters indicate the second best results.,
2110.04375v2,"What is the difference in the validation ratio between the ""With attributes"" and ""No attributes"" models?",There is no difference. Both models use a validation ratio of 0.05 of all edges.,2110.04375v2-Table6-1.png, Default hyperparameters for reproducing the reults. (∗): 3-hop for the Power dataset.,
2110.04375v2,Which method performs better on datasets with high clustering coefficient?,SEAL.,2110.04375v2-Figure4-1.png, Comparison of mean and variance of AUC between SEAL and WP with 90% observed links. The datasets are sorted by their clustering coefficients.,
2110.04375v2,Which dataset has the highest average clustering coefficient (ACC)?,The C.ele dataset has the highest ACC with a value of 0.292.,2110.04375v2-Table4-1.png, Benchmark dataset properties and statistics.,
2110.04375v2,What is the effect of using WP on the AUC for the {node^π} feature?,Using WP increases the AUC for the {node^π} feature from 91.88 to 92.66.,2110.04375v2-Table5-1.png, Ablation study in C.ele,
2110.04375v2,Which model performs the best on the triangle lattice dataset?,WP,2110.04375v2-Table1-1.png, AUC for synthetic graphs over 10 independent trials.,
2110.04375v2,How does WalkPool extract features from a graph?,"WalkPool extracts features from a graph by first generating enclosing subgraphs with and without a focal link. Then, it uses a GNN to compute attention-processed features, which represent random walk transition probabilities. Finally, it extracts a walk profile by applying a pooling operation to the attention-processed features.",2110.04375v2-Figure2-1.png, Illustration of WalkPool. A: The input graph and the focal link e; B: enclosing subgraphs with and without e; C: attention-processed features ≡ random walk transition probabilities; D: extracted walk profile.,
2110.04375v2,What is the relationship between the optimal value of k and the underlying generative model?,The optimal value of k is related to the underlying generative model.,2110.04375v2-Figure3-1.png," Illustration of walk profiles for τ = 2. We assume pi,j = 1/di where di is the degree of node i.",
2110.04375v2,Which method performs best on the USAir dataset?,The WLK method performs best on the USAir dataset with an AP score of 93.34±0.51.,2110.04375v2-Table11-1.png, Prediction accuracy measured by AP on eight datasets (50% observed links) without node attributes. Boldface letters are used to mark the best results while underlined letters indicate the second best results.,
2110.04375v2,Which dataset did the SEAL method perform the best on?,Yeast,2110.04375v2-Table10-1.png, Prediction accuracy measured by AUC on eight datasets (50% observed links) without node attributes. Boldface letters are used to mark the best results while underlined letters indicate the second best results.,
2110.04375v2,Which model performs best on the Cornell dataset when node attributes are available and 90% of the links are observed?,ARVGA with WP.,2110.04375v2-Table3-1.png, Prediction accuracy ( AUC) on datasets with node attributes (90% observed links).,
2110.04375v2,Which method performed best on the USAir dataset?,WLNM,2110.04375v2-Table2-1.png," Prediction accuracy measured by AUC on eight datasets (90% observed links) without node attributes. Boldface marks the best, underline the second best results.",
2110.04375v2,Which model has the highest prediction accuracy on the Pubmed dataset?,GIC with WP,2110.04375v2-Table9-1.png, Prediction accuracy measured by AP on seven datasets (90% observed links) with node attributes.,
2110.11271v1,Which region in the plot satisfies the conditions in equation D.4?,The gray-shaded area.,2110.11271v1-Figure1-1.png," The gray-shaded area is the region where certain conditions (see equation D.4) are satisfied. The orange dot marks τ∗, which is enclosed in the green-shaded area. Moreover, the red-shaded area centered at τ∗ corresponds the width-0.1R annulus A, within which the gradient is exponentially small.",
2110.11271v1,Which optimizer performs better on MNIST for both NCE and eNCE?,NGD,2110.11271v1-Figure3-1.png," Results on MNIST, plotting loss value (y-axis, log scale) against update steps (x-axis). The left plot shows NCE optimized by GD (black) and NGD (yellow), and the right shows eNCE optimized by GD (black) and NGD (blue). It can be seen that NGD outperforms GD in both cases.",
2110.11271v1,"How does the performance of NCE with gradient descent (NCE, GD) compare to the performance of eNCE with normalized gradient descent (eNCE, NGD)?",NCE with gradient descent performs worse than eNCE with normalized gradient descent.,2110.11271v1-Figure2-1.png," Results for estimating 1d (left) and 16d (right) Gaussians, plotting the best parameter distance ‖τ∗ − τ‖2 (y-axis) against the number of updates (x-axis). In both cases, when using NCE, normalized gradient descent (“NCE, NGD"", yellow curve) largely outperforms gradient descent (“NCE, GD”, red curve). When using NGD, the proposed eNCE (“eNCE, NGD”, blue curve) decays faster than the original NCE loss. The results are averaged over 5 runs, with shaded areas showing the standard deviation.",
2110.13265v1,How does the anti-gradient field relate to the level lines of the objective function?,"The anti-gradient field points in the direction of the negative gradient of the objective function. This means that the anti-gradient field points towards the direction of decreasing values of the objective function. The level lines of the objective function are lines along which the objective function has a constant value. Therefore, the anti-gradient field is always perpendicular to the level lines of the objective function.",2110.13265v1-Figure13-1.png," The landscape of the objective f(x1, · · · , xd, y) = 1 4 ∑d i=1 x 4 i − y ∑d i=1 xi + d 2y 2 for d = 1. A blue cross denotes a strict saddle point, whereas a red star corresponds to a global minimizer.",
2110.13265v1,What is the relationship between the value of $\xi$ and the difference between the bounds and the integral?,"As the value of $\xi$ increases, the difference between the bounds and the integral decreases.",2110.13265v1-Figure6-1.png, Numerical verification of Lemma 12. Integral computed numerically using the MATLAB integral function.,
2110.13265v1,Which of the methods requires the most parameters?,RSPI.,2110.13265v1-Table1-1.png, Hyperparameters for the leading eigenvector task.,
2110.13265v1,Which algorithm is the fastest to find the leading eigenvector of a 350-dimensional random matrix?,AHDS is the fastest algorithm to find the leading eigenvector of a 350-dimensional random matrix.,2110.13265v1-Figure5-1.png, Empirical performance in finding the leading eigenvector of a 350-dimensional random matrix. Confidence intervals show min-max intervals over five runs. All algorithms are initialized at a strict saddle point.,
2110.13265v1,What is the relationship between the suboptimality increase and σ2?,The suboptimality increase is a function of σ2. It decreases to a minimum value and then increases rapidly as σ2 increases.,2110.13265v1-Figure9-1.png, Selection of the value of σ2 which yields the best decrease.,
2110.13265v1,Which algorithm performed the best in terms of convergence speed and stability?,AHDS-Unif,2110.13265v1-Figure19-1.png, Empirical performance in finding the leading eigenvector of a 350-dimensional random matrix against iteration and wall-clock time. Confidence intervals show min-max intervals over ten runs. All algorithms are initialized at a strict saddle point across all runs.,
2110.13265v1,What is the range of values for the final coordinate vector and the final gradient vector?,"The range of values for the final coordinate vector is approximately 0 to 0.002, and the range of values for the final gradient vector is approximately 0 to 0.8.",2110.13265v1-Figure18-1.png, Histogram of the point coordinates (left) and gradient values (right) at the final iterate of RSPI for d = 200.,
2110.13265v1,What is the effect of the problem dimension on the probability of a decrease of at least |λd|/2 starting from the origin?,The probability of a decrease decreases exponentially with the problem dimension.,2110.13265v1-Figure1-1.png," Behavior of a step of vanilla random search on a quadratic saddle centered at the origin. The Hessian has d − 1 positive eigenvalues equal to 1 and one negative eigenvalue equal to λd. Plotted is the probability of a decrease (1e6 runs) of at least |λd|/2 starting from the origin (σ2 = 1). Performance degrades exponentially with the problem dimension, as predicted by Lemma 3.",
2110.13265v1,What is the relationship between the angle ϑ and the surface area of the spherical cap?,"The angle ϑ determines the surface area of the spherical cap. The larger the angle ϑ, the larger the surface area of the spherical cap.",2110.13265v1-Figure8-1.png, Illustration for the proof of Lemma 3. Any vector on the unit sphere whose angle that is less than ϑ = cos−1(ς) away from vd belongs to cap colored in yellow. Our goal is to bound the surface area of this spherical cap.,
2110.13265v1,Which methods have the same parameter values for all values of d?,STP and BDS.,2110.13265v1-Table3-1.png, Hyperparameters for the Rastrigin function.,
2110.13265v1,Which optimization algorithm performs the best for the Rastrigin function?,AHDS,2110.13265v1-Figure16-1.png, Empirical performance while minimizing the Rastrigin function against the number of iterations. Confidence intervals show min-max intervals over ten runs. All algorithms are initialized at a strict saddle point across all runs.,
2110.13265v1,How does the performance of the algorithms change as the input dimension increases?,The performance of the algorithms generally decreases as the input dimension increases.,2110.13265v1-Figure17-1.png, Empirical performance while minimizing the Rastrigin function against wall-clock time. Confidence intervals show min-max intervals over ten runs. All algorithms are initialized at a strict saddle point across all runs.,
2110.13265v1,Which algorithm is the fastest to converge to the optimal solution for the Rastrigin function?,AHDS,2110.13265v1-Figure4-1.png," Optimality gap on the Rastrigin function as a function of iterations (top) and running time (bottom). Confidence intervals show min-max intervals over ten runs. All algorithms are initialized at a strict saddle point and executed for a total of 500 iterations across all runs, however for d = 100, 200 no further improvement is achieved after 25 iterations.",
2110.13265v1,Which algorithm performs the best for input dimension 5?,AHDS,2110.13265v1-Figure3-1.png," Performance while minimizing the objective in Eq. (5) for different d. Confidence intervals show min-max intervals over ten runs. All algorithms are initialized at the strict saddle point across all runs. For d = 100, 200, the lines for STP, BDS and AHDS overlap each other as none of the methods achieve progress in terms of function value.",
2110.13265v1,What are the hyperparameters for the AHDS method when d = 100?," η_0 = 0.8, 
* η_max = 10.0, 
* γ = 1.25, 
* θ = 0.5, 
* ρ(x) = 0",2110.13265v1-Table2-1.png, Hyperparameters for the objective in Eq. (88).,
2110.13265v1,What is the relationship between the dimensionality of the space and the probability of finding a better solution in a random step?,The probability of finding a better solution in a random step decreases exponentially with the dimensionality of the space.,2110.13265v1-Figure7-1.png, Numerical verification of Lemma 13. Bounds can be found in Lemma 13.,
2110.13465v2,What is the difference between the original topology of the model and the topology after step 4 of the CS-Rep process?,"The original topology of the model has a 3-branch TDNN layer and a BN layer for each sequential layer. After step 4 of the CS-Rep process, the 3-branch TDNN layer and the BN layer are replaced with a single C×3 layer.",2110.13465v2-Figure1-1.png," The topology changes for our model when adopting the proposed CS-Rep: (a) original topology of model, (b) step, (c) steps 2 and 3, and (d) step 4.",
2110.13465v2, What is the difference between the network architecture during training and inference? ," During training, the network uses a multi-branch design, while during inference, it uses a plain topology. ",2110.13465v2-Figure3-1.png," The topologies of Rep-TDNN without and with CS-Rep. The context size of the head TDNN in each block is Chead ∈ [5, 1, 1, 5], the g represents the groups of layers, and the channel N of each TDNN layer and branch is 512. FC, BN, SE denote fully connected layer, batch normalization, and squeeze-excitation, respectively. (Left): The multi-branch design in the training period. (Right): The plain topology in the inference period.",
2110.13465v2,"What is the purpose of the ""bn-first"" re-parameterization in TDNN?","The ""bn-first"" re-parameterization is used to convert the original parameters of weight and BN into a form that is more efficient for inference.",2110.13465v2-Figure2-1.png," Parameters converting of the “bn-first” re-parameterization in TDNN: (a) original parameters of weight and BN, (b) steps 1 and 2, (c) step 3, and (d) step 4.",
2110.13465v2,Which of the proposed models achieves the highest speed and lowest EER on the VoxCeleb1-test set?,Rep-TDNN (CS-Rep),2110.13465v2-Table1-1.png," The benchmark on VoxCeleb1 test, VoxCeleb1-E, and VoxCeleb1-H. Models were inferred on the single GPU to calculate the inference speed. Speed4 in this table was defined as frame-numbers/processing-time.",
2110.15766v1,Which task uses the larger learning rate?,"MNLI, QNLI, SST-2",2110.15766v1-Table3-1.png, NxMTransformer Training Hyperparameters. Smaller tasks utilize larger learning rates and penalty parameters (ρ) since ADMM iterations for these tasks are much shorter (See Appendix A.1).,
2110.15766v1,Which model performs better on the MNLI task?,The Baseline (BERTbase) model performs better on the MNLI task.,2110.15766v1-Table1-1.png," The dev set results on the GLUE benchmark. The results show that NxMTransformer is able to achieve higher accuracy than ASP for NxM sparsity, especially when the downstream tasks have low data resources.",
2110.15766v1,Which layers of the BERT model are sparsified by NxMTransformer?,The FFN1 and FFN2 layers are sparsified by NxMTransformer.,2110.15766v1-Figure1-1.png," The layers sparsified by NxMTransformer are highlighted in blue in the block structure of a BERT model. As shown for FFN1, NxMTransformer simultaneously finetunes the pretrained representation while inducing NxM semi-structured sparsity using ADMM. This sparse model can be trivially converted to the deployment format for compatible hardware.",
2110.15766v1,How does the average parameter magnitude change before and after fine-tuning on a 10-epoch STS-B experiment?,The average parameter magnitude generally increases after fine-tuning.,2110.15766v1-Figure4-1.png," Comparison of average parameter value before and after fine-tuning on a 10-epoch STS-B experiment (learning rate: 5e-5, batch size: 16, ρ: 1e-3) based on the number of times it was present in the sparse subnetwork mask.",
2110.15766v1,Which model performs the best on the QNLI task?,DistilBERT,2110.15766v1-Table2-1.png, The dev set results on the GLUE benchmark with knowledge distillation. The results show NxMTransformer retains 97.6% of the DistilBERT model.,
2111.01662v1,Which model achieves the best bpd on CIFAR10 when pretrained on CIFAR10?,FineTune v3,2111.01662v1-Table2-1.png," Theoretical bpd values between vanilla OSOA and baselines (PreTrain, FineTune v1, v2 and v3). The bpd values of baselines less than those of vanilla OSOA are shown in green, e.g., 0.132 = 3.505− 3.373. The effective bpd values by saving the models with FineTune baselines, defined as #trainable_parameters× bits/parameter× 1/#dataset_total_dims, are shown in red. HiLLoC CIFAR10 denotes the CIFAR10 pretrained HiLLoC model.",
2111.01662v1,"What is the probability of the sequence ""a1 a2 a3 a4 a5"" according to the arithmetic coding scheme shown in the figure?","The probability of the sequence ""a1 a2 a3 a4 a5"" is 0.769504.",2111.01662v1-Figure5-1.png, Arithmetic coding for Toy Example 1.,
2111.01662v1,What is the probability of symbol a2?,The probability of symbol a2 is 0.08.,2111.01662v1-Table5-1.png, Information needed of rANS for Toy Example 1,
2111.01662v1,Which learning rate led to the lowest bpd for OSOA with HiLLoC?,The learning rate that led to the lowest bpd for OSOA with HiLLoC is approximately 0.0025.,2111.01662v1-Figure12-1.png," The learning rate smile: bpd of OSOA with HiLLoC (left) and IDF++ (right) of different learning rates on SET32. ""NaN"" denotes the failure of training with larger learning rates.",
2111.01662v1,Which pretrained model has a lower mean value of OSOA?,The CIFAR10 pretrained IDF++ model has a lower mean value of OSOA.,2111.01662v1-Figure11-1.png, Violin plots of OSOA with different random seeds shown with CIFAR10 pretrained HiLLoC (left) and CIFAR10 pretrained IDF++ (right).,
2111.01662v1,Which method has the lowest bpd values on average in the SET128 dataset?,OSOA - FineTune v3.,2111.01662v1-Figure2-1.png," Left: Theoretical bpd values of each batch during training HiLLoC from scratch on SET32 (ReTrain HiLLoC) versus the batch bpd of OSOA, showing the significant time cost of training from scratch. OSOA final average is the bpd value after compressing all the batches with OSOA. Middle and Right: the differences between the theoretical bpd values of OSOA and the theoretical bpd values of the baselines of each batch in SET32 (Middle) and SET128 (Right) with the CIFAR10 pretrained HiLLoC. Negative values indicate the advantages of coding with OSOA instead of the corresponding baseline. The disadvantage of OSOA (versus FineTune) tends to decrease and the advantage of OSOA (versus PreTrain) tends to increase as the online adaptation being conducted.",
2111.01662v1,What is the real bpd value for HiLLoC (CIFAR10) with FineTune v3?,3.257 (0.308),2111.01662v1-Table6-1.png, Real bpd values,
2111.01662v1,Which symbol in the Huffman tree has the lowest frequency?,Symbol $a_4$,2111.01662v1-Figure4-1.png, The Huffman tree for Toy Example 1.,
2111.01662v1,Which symbol has the highest probability?,a5,2111.01662v1-Table4-1.png, The simplex of the Toy Example 1,
2111.01662v1,"In the illustration, what does the notation ""dynamics"" represent?","The ""dynamics"" represents the function that updates the latent state p based on the current latent state and the current observation.",2111.01662v1-Figure8-1.png, An illustration of OSOA encoding with bits back asymmetric numerical system (bb-ANS) for Toy Example 4. C.push denotes the encoding operation of the ANS codec.,
2111.01662v1,How does the theoretical bpd of HiLLoC and IDF++ change as the number of updates per batch increases?,The theoretical bpd of HiLLoC and IDF++ generally decreases as the number of updates per batch increases.,2111.01662v1-Table3-1.png, The theoretical bpd of HiLLoC and IDF++ with OSOA of multiple optimisation updates per batch on test dataset SET32.,
2111.01662v1,What is the purpose of the dynamics block in the figure?,"The dynamics block is responsible for generating the next state of the system, given the current state and the current input.",2111.01662v1-Figure1-1.png," An illustration of OSOA Encoding with FILO style entropy coders, where m = 3 and T = 6 (bb-ANS as an example). C.push denotes the encoding operation of the ANS codec. We refer readers to Appendix A for complete demos of FIFO style and FILO style entropy coders.",
2111.01662v1,What is the relationship between the dynamics and the ANS codec in the figure?,"The dynamics block provides the input to the ANS codec, which then encodes the input into a compressed representation. The ANS codec also decodes the compressed representation back into the original input.",2111.01662v1-Figure9-1.png, An illustration of OSOA decoding with bits back asymmetric numerical system (bb-ANS) for Toy Example 4. C.pop denotes the decoding operation of the ANS codec.,
2111.01662v1,What is the role of the C.push operation in the OSOA encoding process?,The C.push operation is used to encode the arithmetic code for each batch of data.,2111.01662v1-Figure6-1.png, An illustration of OSOA encoding with arithmetic coding (AC) for Toy Example 3. C.push denotes the encoding operation of the AC codec.,
2111.01662v1,What is the purpose of the C.pop blocks in the OSOA decoding process?,The C.pop blocks represent the decoding operation of the arithmetic coding (AC) codec. They are responsible for decoding the compressed bitstream into the original symbols.,2111.01662v1-Figure7-1.png, An illustration of OSOA decoding with arithmetic coding (AC) for Toy Example 3. C.pop denotes the decoding operation of the AC codec.,
2111.01662v1,"What is the difference between PreTrain, FineTune v3, and ReTrain?",PreTrain is a HiLLoC model pretrained on CIFAR10. FineTune v3 is a HiLLoC model that has been fine-tuned on SET32 for 20 epochs. ReTrain is a HiLLoC model trained on SET32 from scratch.,2111.01662v1-Figure3-1.png," 36 images sampled from (a) PreTrain: CIFAR10 pretrained HiLLoC, (b) FineTune v3: fine-tuning CIFAR10 pretrained HiLLoC for 20 epochs on SET32, (c) ReTrain: HiLLoC trained on SET32 from scratch, (d) and (e) OSOA 1 and 100 updates/batch: the final checkpoint of vanilla OSOA with 1 and 100 update steps per batch from CIFAR10 pretrained HiLLoC, respectively. (f) The theoretical bpd values of vanilla OSOA with different learning rate values, with HiLLoC (CIFAR10).",
2111.01662v1,Which training method produces the most realistic images?,The FineTune v3 method produces the most realistic images.,2111.01662v1-Figure10-1.png," 36 images sampled from (a) ReTrain: HiLLoC trained on SET32 from scratch, (b) PreTrain: CIFAR10 pretrained HiLLoC, (d) FineTune v3: fine-tuning CIFAR10 pretrained HiLLoC for 20 epochs on SET32, (c) and (e) OSOA 1 and 100 updates/batch: the final checkpoint of vanilla OSOA with 1 and 100 update steps per batch from CIFAR10 pretrained HiLLoC, respectively.",
2111.01662v1,Which model has the lowest theoretical bits per dimension (bpd) value on the SET128 dataset?,IDF++,2111.01662v1-Table1-1.png," Theoretical bits per dimension (bpd) values of HiLLoC, IAF RVAE and IDF++ models training from scratch (ReTrain) on the three target datasets SET32/64/128.",
2112.00544v2,Which method performed the best on the ToxCast dataset?,KCL (G),2112.00544v2-Table3-1.png," The performance of KCL under the linear protocol on 6 datasets, compared with contrastive learning baselines. The metric is ROC-AUC.",
2112.00544v2,Which model performs best on the BACE dataset in terms of classification?,KCL(KMPNN),2112.00544v2-Table2-1.png," The property prediction performance (lower is better for regression) of KCL under the fine-tune protocol, compared with supervised learning (first group) and pre-training methods (second group) baselines on 8 datasets.",
2112.00544v2,Which component of KCL has the largest impact on performance?,The K-initialization component has the largest impact on performance.,2112.00544v2-Figure3-1.png, Performance of KCL with different settings under the fine-tune protocol (lower is better for regression).,
2112.00544v2,Which dataset has the largest number of compounds?,ToxCast,2112.00544v2-Table5-1.png, Dataset information.,
2112.00544v2,What is the value of the learning rate used in the pre-training process?,0.00001,2112.00544v2-Table6-1.png, The pre-train hyper-parameters.,
2112.00544v2,How does the Chemical Element KG help to build associations between atoms?,The Chemical Element KG builds associations between atoms that are not directly connected by bonds but are related in fundamental chemical attributes.,2112.00544v2-Figure1-1.png," Chemical Element KG builds associations between atoms that are not directly connected by bonds but related in fundamental chemical attributes, as denoted by red arrows.",
2112.00544v2, What is the relationship between the element chlorine and the molecule shown on the right?, Chlorine is an atom in the molecule shown on the right. ,2112.00544v2-Figure4-1.png," An attention visualization example of different types of neighbors (attributes and atoms) in the BBBP dataset. The attention weights assigned for bonds connected to the two C atoms are visualized on the right. The darker the color, the higher the attention.",
2112.00544v2,Which model performed the best on the classification task?,KCL(GIN),2112.00544v2-Table4-1.png," Ablation results under the fine-tune protocol. Each value represents the average result of the task, and the underline marks the best in the group.",
2112.00544v2,"What is the role of the ""Message Aggregate Attention"" module in the KMPNN architecture?","The ""Message Aggregate Attention"" module is responsible for aggregating the messages from neighboring nodes and applying attention to them.",2112.00544v2-Figure5-1.png, Architecture of KMPNN.,
2112.00544v2,How many different types of relations are there in the Chemical Element KG?,There are 17 different types of relations in the Chemical Element KG.,2112.00544v2-Table1-1.png, The statistics of Chemical Element KG.,
2112.00544v2,What is the purpose of the knowledge-guided graph augmentation module in KCL?,The knowledge-guided graph augmentation module in KCL is used to convert the original molecular graph G into the augmented molecular graph G′ based on Chemical Element KG.,2112.00544v2-Figure2-1.png," An illustrative example for KCL. We ignore edge directions in four molecular graphs due to space limitation (the direction of an edge between an attribute and an atom is from the former to the latter, while an edge between atoms is bidirectional). Module 1: Knowledge-guided graph augmentation converts the original molecular graph G into the augmented molecular graph G′ based on Chemical Element KG. Module 2: Knowledge-aware graph representation captures representations from two graph views separately. Module 3: Contrastive objective trains the encoders and the projection head to maximize agreement between positives and disagreement between hard negatives (e.g., Gj act as the hard negative of Gi) via a contrastive loss.",
2112.00544v2,Which graph encoder performs the best on the ClinTox dataset for the classification task?,KMPNN.,2112.00544v2-Table11-1.png, Results comparison with different graph encoders.,
2112.00544v2,"Which dataset, BACE or ToxCast, considers the ""Periodic"" property of an element as more important when determining its properties?",BACE,2112.00544v2-Figure7-1.png, Attention visualization examples of attributes in the BACE and ToxCast datasets.,
2112.00544v2,"Which protocol performs better for classification tasks, fine-tuning or linear?",The fine-tuning protocol performs better for classification tasks.,2112.00544v2-Table12-1.png, Results comparison between KCL(KMPNN) and KMPNN without contrastive learning.,
2112.00544v2,What is the range of values for the learning rate hyper-parameter?,0.00001 to 0.1,2112.00544v2-Table7-1.png, The downstream hyper-parameters.,
2112.00544v2,Which protocol and method combination results in the best performance according to the table?,The linear protocol with the KCL(KMPNN) method results in the best performance.,2112.00544v2-Table8-1.png, The performance of KCL under the linear protocol and fine-tune protocol.,
2112.00544v2,Which dataset shows the least performance improvement when using all of the techniques together (ALL) compared to not using any of them (w/oALL)?,ESOL,2112.00544v2-Table9-1.png, Ablation results on molecular graphs.,
2112.00544v2,Which configuration of KCL(KMPNN) achieved the highest average performance on the classification tasks?,"The ""ALL"" configuration achieved the highest average performance on the classification tasks, with an average accuracy of 0.852.",2112.00544v2-Table10-1.png, Ablation results on the augmented molecular graphs.,
2112.00544v2,What are the properties of carbon?,"The properties of carbon are shown in the left-hand part of the figure. They include electronegativity, conductivity, melting point, radius, weight, heat, modulus, density, boiling point, electron affinity, abundance, ionization, periodicity, metallicity, and state.",2112.00544v2-Figure6-1.png, Another example in the BBBP dataset.,
2201.09427v1,Which model achieved the highest accuracy on the IH dataset?,The BiLSTM model with EFpd and BERT achieved the highest accuracy on the IH dataset with a score of 94.34%.,2201.09427v1-Table3-1.png, Performance of different systems on PD in the in-house (IH) dataset and JNAS. PD1 and 2 use a morphological analyzer only. EFPD denotes the POS. * denotes a statistically significant difference with the best performance at p < .05.,
2201.09427v1,What is the role of the Polyphone Disambiguation (PD) module in the TTS front-end pipeline for Japanese?,The PD module resolves pronunciation ambiguity for words with multiple possible pronunciations.,2201.09427v1-Figure1-1.png, Pipeline of TTS front-end for Japanese,
2201.09427v1,Which features are used in all three tasks?,"EF1, EF2, EF3, EF4, EF5, and EF6 are used in all three tasks.",2201.09427v1-Table1-1.png, Description of explicit features (EFs). The right three columns show the EFs used in each task.,
2201.09427v1,Which system performs the best on the APBP dataset for explicit features?,"EFAPBP (+ n-gram) BERT performs the best on the APBP dataset for explicit features, with an F1-score of 96.30.",2201.09427v1-Table4-1.png, F1-score of different systems on APBP. EFAPBP denotes the explicit features for APBP (see Sec. 3.1).,
2201.09427v1,Which system had the highest MOS score?,Oracle symbols,2201.09427v1-Table7-1.png, MOS evaluation of TTS with 95% confidence intervals computed from the t-distribution for different systems.,
2201.09427v1,Which system achieved the highest accuracy on long accent phrases?,EFANPP Flair achieved the highest accuracy on long accent phrases with 92.98%.,2201.09427v1-Table5-1.png, Accuracy of different systems on ANPP. EFANPP denotes the explicit features for ANPP (see Sec. 3.1).,
2201.09427v1,Which system performs the best on the JSUT dataset in terms of Mora-Accuracy?,"AP2, which uses the EF_ANPP + Flair model, performs the best on the JSUT dataset in terms of Mora-Accuracy, with a score of 97.33.",2201.09427v1-Table6-1.png, Performance of different systems on overall accent prediction in the in-house (IH) dataset and JSUT.,
2202.13785v3,What are the two challenges highlighted in the figure?,The two challenges highlighted in the figure are invalid negative sampling and uncertainty of fact-view link prediction.,2202.13785v3-Figure1-1.png," Two examples exhibit the challenges that needed to be addressed. Challenge 1: Given a positive triple, some generated negative triples are falsenegative or low-quality. Challenge 2: For link prediction, the entity California ranks higher than the correct entity U.S.A. due to the uncertainty of KG embeddings, but the correct answer entity should belong to the concept Country in the view of commonsense.",
2202.13785v3,Which dataset has the most entities?,DBpedia-242,2202.13785v3-Table1-1.png," Statistics of the experimental datasets. #Rel, #Ent, #Con represent the number of relations, entities and concepts of each dataset, respectively.",
2202.13785v3,Which model performs best on the FB15K dataset in terms of Hits@10?,TransE+CANS.,2202.13785v3-Table2-1.png, Link prediction results on four datasets. Bold numbers are the best results for each type of model.,
2202.13785v3,What is the role of commonsense in this example of explainable link prediction?,"Commonsense helps to refine the query and provide additional context for the link prediction task. In this example, the commonsense knowledge that ""sportsteam"" and ""team plays in league"" are related to ""sportsleague"" helps to identify the correct entity concept for the query.",2202.13785v3-Figure4-1.png, A case study of explainable link prediction with commonsense and entity concept on NELL-995.,
2202.13785v3,Which model performed the best on the NELL-995 dataset according to the Hits@1 metric?,The CAKE model performed the best on the NELL-995 dataset according to the Hits@1 metric.,2202.13785v3-Table4-1.png, Ablation study of integrating each model into the basic model HAKE on FB15K237 and NELL-995.,
2202.13785v3,Which NS technique achieves the best Hits@10 performance on the FB15K dataset?,TransE+CANS (Ours),2202.13785v3-Table3-1.png," Comparison results of various NS techniques. Unifo, NoSamp, NSCach, Domain and SAdv denote uniform sampling, none sampling, NSCaching, domain-based NS and self-adversarial NS strategies, respectively.",
2202.13785v3,What is the role of the CANS module in the CAKE framework?,The CANS module is responsible for selecting candidate concepts based on commonsense and complex relations.,2202.13785v3-Figure2-1.png," An overview of the CAKE framework. The orange dotes indicate the entities. The green dotes represent the entity concepts. In the CANS module, r1−1, r1−N , rN−1 and rN−N denote the diverse complex relations of 1-1, 1-N, N-1 and N-N, respectively. cjhi and cjti indicate the i-th head concept and tail concept that are selected by the commonsense and the characteristics of complex relations specific to the j-th relation.",
2202.13785v3,How does the CANS module generate high-quality negative triples?,"The CANS module generates high-quality negative triples by using the ""attentive concept-to-entity converting"" technique. This technique involves using a weighted probability to select candidate concepts that are most likely to be related to the corrupted head and tail entities. The weighted probability is based on the concept-to-entity attention mechanism, which takes into account the similarity between the corrupted entities and the candidate concepts.",2202.13785v3-Figure3-1.png, An example of generating the high-quality negative triples containing an N-1 relation by our designed CANS module on NELL-995.,
2203.07004v2,"What does the increase in I(z1, v1) represent in the figure?","The increase in I(z1, v1) represents the introduction of more non-shared task-relevant information in the representation z1.",2203.07004v2-Figure1-1.png," Demonstration of our motivation using information diagrams. Based on the (approximately minimal) sufficient representation learned in contrastive learning, increasing I(z1, v1) approximately introduces more non-shared task-relevant information.",
2203.07004v2,Which model performed best on the CIFAR10 dataset?,BYOL+RC (ours),2203.07004v2-Table1-1.png," Linear evaluation accuracy (%) on the source dataset (CIFAR10, STL-10 or ImageNet) and other transfer datasets.",
2203.07004v2,Which model performs the best on the object detection task for the VOC07+12 dataset?,SimCLR+RC (ours),2203.07004v2-Table2-1.png," Object detection and instance segmentation on VOC07+12 and COCO. The models on COCO are fine-tuned using the default 2× schedule. In magenta are the gaps of at least +0.5 point to the baseline, SimCLR.",
2203.07004v2,What is the role of the encoders in the contrastive learning process?,"The encoders are responsible for generating representations of the input data. These representations are then used to calculate the contrastive loss, which encourages the representations of similar data points to be close together in the latent space.",2203.07004v2-Figure2-1.png, Internal mechanism of contrastive learning: the views provide supervision information to each other.,
2203.07004v2,Which model performs the best on CIFAR10?,SimCLR performs the best on CIFAR10.,2203.07004v2-Figure4-1.png, Linear evaluation accuracy on the source dataset (CIFAR10 or STL-10) and the averaged accuracy on all transfer datasets with varying hyper-parameter λ.,
2203.07004v2,Which method performs best on CIFAR10 and STL-10?,SimCLR performs best on both CIFAR10 and STL-10.,2203.07004v2-Figure5-1.png, Linear evaluation accuracy on the source dataset (CIFAR10 or STL-10) and the averaged accuracy on all transfer datasets with varying epochs.,
2203.07004v2,Which model performed the best on the CUBirds dataset?,SimCLR+MIB,2203.07004v2-Table3-1.png, Linear evaluation accuracy (%) on CIFAR10 and the transfer datasets. † represents adding Gaussian noise to the representations.,
2203.07004v2, What is the difference between a random representation and a sufficient representation? ," A random representation contains both shared and non-shared task-relevant information, while a sufficient representation only contains shared task-relevant information.",2203.07004v2-Figure3-1.png," Information diagrams of different representations in contrastive learning. We consider the situation where the non-shared task-relevant information I(v1, T |v2) cannot be ignored. Contrastive learning makes the representations extracting the shared information between views to obtain the sufficient representation which is approximately minimal. The minimal sufficient representation contains less task-relevant information from the input than other sufficient representations.",
2203.07004v2,Which model performs best on the MNIST dataset?,BarTwins+LBE (ours),2203.07004v2-Table5-1.png, Linear evaluation accuracy (%) on the source dataset (CIFAR10 or STL-10) and other transfer datasets.,
2203.07004v2,What is the purpose of the figure?,"The figure demonstrates the reconstruction effect of Implementation I, which uses SimCLR contrastive loss and CIFAR10 as the training dataset.",2203.07004v2-Figure6-1.png, Demonstration of the reconstruction effect of our Implementation I. We provide the original input images and the reconstructed images for comparison. We use SimCLR contrastive loss and take CIFAR10 as the training dataset.,
2203.07004v2,Which model performs the best on the CUBirds dataset?,The Supervised+LBE (ours) model performs the best on the CUBirds dataset with an accuracy of 9.72% for CIFAR10 and 11.89% for CIFAR100.,2203.07004v2-Table4-1.png, Linear evaluation accuracy (%) on the source dataset (CIFAR10 or CIFAR100) and the transfer datasets.,
2203.13560v2, What are the key elements of the COMET conversation strategy as illustrated in the figure?," The COMET conversation strategy, as shown in the figure, consists of five key elements: **C**ompassion, **O**pen-mindedness, **M**otivation to help, **E**mpathy, and **T**houghtfulness. These elements are demonstrated through specific actions such as reflecting feelings, providing suggestions, and offering affirmation and reassurance.",2203.13560v2-Figure1-1.png, An Emotional Support Conversation Example.,
2203.13560v2,"What is the effect of removing the different components of the MISC model on the D-1, B-2, R-L, and M metrics?","Removing the different components of the MISC model generally leads to a decrease in the D-1, B-2, R-L, and M metrics.",2203.13560v2-Table4-1.png, Evaluation Results of Ablation Study.,
2203.13560v2,Which model has the highest level of agreement among human annotators?,MISC,2203.13560v2-Table3-1.png," Manual Evaluation Results. The Fleiss Kappa score (Fleiss and Cohen, 1973) reaches 0.445, indicating a moderate level of agreements.",
2203.13560v2,Which model performed the best in terms of D-1 score?,MT Transformer,2203.13560v2-Table2-1.png, Automatic Evaluation Results on ESConv.,
2203.13560v2,Which model provided the most empathetic response?,MISC,2203.13560v2-Table5-1.png, Responses generated from MISC and other compared models. Some words are omitted due to space limit.,
2203.13560v2,What is the percentage increase in M for MISC and MISE?,"11.05% and 10.53%, respectively.",2203.13560v2-Table6-1.png, Results of MISC with Different Emotions.,
2203.13560v2,What is the most common type of response according to the bar graph?,Reflection of feelings,2203.13560v2-Figure4-1.png, The visualization of how the MISC organizes the response under the effect of multiple factors.,
2203.13560v2,"Which method performs better for Top-k strategy prediction accuracy, MSC or BlenderBot_joint?",BlenderBot_joint performs better than MSC for Top-k strategy prediction accuracy.,2203.13560v2-Figure5-1.png, The Top-k Strategy Prediction Accuracy.,
2203.13560v2,Which strategy yielded the highest value for M?,Mixture strategy.,2203.13560v2-Table7-1.png, Comparison of different strategy modeling.,
2203.13560v2,Which of the three panels shows the most similar distribution of strategies throughout the conversation?,Panels (a) and (c) show the most similar distribution of strategies throughout the conversation.,2203.13560v2-Figure3-1.png, The strategy distribution in the different stage of conversation.,
2203.13560v2,Which set of data has the highest average number of words per dialogue?,The Train set.,2203.13560v2-Table1-1.png, The statistics of processed ESConv dataset.,
2203.13560v2,What is the role of the Mental State-enhanced Encoder in the MISC architecture?,The Mental State-enhanced Encoder is responsible for encoding the seeker's post and the context into a hidden representation that takes into account the seeker's mental state.,2203.13560v2-Figure2-1.png," The overview of the proposed MISC which consists of a mental state-enhanced encoder, a mixed strategy learning module, and a multi-factor-aware decoder.",
2203.13560v2, What is the most common strategy used in the ESConv dataset?," The most common strategy used in the ESConv dataset is ""Restatement or Paraphrasing,"" which accounts for 20.68% of the total strategies.",2203.13560v2-Figure6-1.png, The strategy distribution in the original ESConv dataset.,
2204.00442v1,Which method achieved the best performance on the CelebA-HQ dataset according to the LPIPS metric?,MCL-Net,2204.00442v1-Table1-1.png," Comparing MCL-Net with state-of-the-art image translation methods: The comparisons were performed over three public datasets with three widely used evaluation metrics FID, SWD and LPIPS.",
2204.00442v1,Which method achieves the highest score for semantic consistency?,CoCosNet v2 achieves the highest score for semantic consistency with a score of 0.887.,2204.00442v1-Table2-1.png, Quantitative evaluation of style relevance (color and texture) and semantic consistency on ADE20K.,
2204.00442v1,Which of the methods shown in the figure produces the most realistic images?,"It is difficult to say definitively which method produces the most realistic images, as this is subjective and depends on the specific image and task. However, the figure shows that the MCL-Net method generally produces images that are more visually appealing and closer to the ground truth than the other methods.",2204.00442v1-Figure6-1.png," Qualitative comparison of MCL-Net and state-of-the-art image translation methods over three different types of conditional inputs including semantic segmentation, key-points, and edge map, respectively, from top to bottom.",
2204.00442v1,Which model configuration achieves the lowest FID score?,+COR+MCL+SCM,2204.00442v1-Table3-1.png, Ablation studies of our MCL-Net designs over CelebAHQ [17]: The baseline is SPADE [25] that conduct translation without building correspondence. COR denotes building correspondence between conditional inputs and exemplars. MCL and SCM mean to include the proposed marginal contrastive loss and self-correlation map in building correspondence. Model in the last row is the standard MCL-Net.,
2204.00442v1,How does the proposed framework learn domain-invariant features?,"The proposed framework learns domain-invariant features by using a shared feature encoder (EZ) for both the ground truth and the exemplar image. This allows the framework to extract features that are common to both domains, regardless of the specific content of the images.",2204.00442v1-Figure2-1.png," The framework of our proposed conditional image translation network: The Conditional Input and Ground Truth are fed to feature encoders EX and EZ to extract feature vectors X and Y . The proposed self-correlation map (SCM) then encodes structure information for building correspondence, where the proposed Marginal Contrastive Loss drives the encoders to learn domain-invariant features. With the shared feature encoder EZ , domain-invariant features can be extracted from the Exemplar Image and feature correspondences between the conditional input and exemplar can be established. The exemplar image can then be warped to align with the conditional input, which provides accurate style guidance for the Generation Network.",
2204.00442v1,What is the effect of adding marginal contrastive learning (MCL) and self-correlation map (SCM) to the baseline model?,"Adding MCL and SCM improves the performance of the baseline model, as evidenced by the increase in PSNR and SSIM values.",2204.00442v1-Table4-1.png," Parameter studies of marginal contrastive learning (MCL) and self-correlation map (SCM) on warped exemplars on DeepFashion dataset. The baseline is CoCosNet [46], and the last row denotes the standard MCL-Net.",
2204.00442v1,What are the different types of conditional inputs and exemplars that can be used with MCL-Net?,"The different types of conditional inputs and exemplars that can be used with MCL-Net are:
* Semantic map
* Key points
* Edge map",2204.00442v1-Figure7-1.png, Qualitative illustration of our proposed MCL-Net with different types of conditional inputs and exemplars.,
2204.00442v1,What is the difference between vanilla contrastive learning and marginal contrastive learning?,"Vanilla contrastive learning tends to learn compact feature clusters, while marginal contrastive learning learns more dispersed feature clusters.",2204.00442v1-Figure4-1.png," Toy illustration of learned feature representations with vanilla contrastive learning and our proposed marginal contrastive learning. Dots indicate image features, angles θii and m denote the compactness of the learnt feature clusters and the angular margin penalty, respectively. Zoom-in for details.",
2204.00442v1," 
Which of the methods shown in the figure results in the most realistic-looking translation?"," 
MCL + SCM",2204.00442v1-Figure3-1.png," Comparison of warped exemplars and translation results by different methods: Warping and Result denote the warped exemplars and the final translation results. MCL and SCM denote our proposed marginal contrastive learning and self-correlation map, respectively.",
2204.04487v2,What is the relationship between the sentiment Y and the target Z?,The sentiment Y is caused by the target Z.,2204.04487v2-Figure1-1.png," An instance from the toy model. The upper part of the figure corresponds to fX , the function that generates the text via a PCFG (see fig. 2): nodes represent non-terminals in the grammar and edges represent context-free derivations. The lower part of the figure corresponds to the causal model of the sentiment Y and target Z. Here nodes represent random variables and edges represent causal relationships.",
2204.04487v2,What are the causes of Z in the causal model?,X1 and Nz are the causes of Z.,2204.04487v2-Figure2-1.png," Causal model for the toy example shown in fig. 1. NU , NX , NY , NZ indicate independent noise variables, and fX , fY , fZ indicate deterministic functions that map from causes to effects (for more details on the notation, see Peters et al., 2017).",
2204.05961v1,Which measurand was measured by both van der Lee et al. (2017) and Mille et al. (2021)?,Fluency,2204.05961v1-Table3-1.png, Conditions of measurement for two measurements each for three evaluation measures (measurands) and the PASS system. vdL&al = van der Lee et al. (2017); M&al = Mille et al. (2021).,
2204.05961v1,"What is the coefficient of variation (CV*) for the ""Clarity"" measure?","The CV* for the ""Clarity"" measure is 13.193.",2204.05961v1-Table2-1.png," Precision (CV∗) and component measures (mean, standard deviation, standard deviation, confidence intervals) for measured quantity values obtained in two measurements for each of the three human-assessed evaluation measures for the PASS system. Columns 6–9 calculated on shifted scores (see Section 3.2).",
2204.05961v1,Which script version gave the most precise BLEU score measurements for NTS_def?,"The script version that gave the most precise BLEU score measurements for NTS_def was b4, which is the SacreBLEU script.",2204.05961v1-Table4-1.png," Precision (CV∗) and component measures (mean, standard deviation, standard deviation confidence intervals) for measured quantity values obtained in multiple measurements of the two NTS systems. Outputs 1 = test set outputs as generated by Nisioi et al. (2017); outputs 2 = test set outputs regenerated by Cooper and Shardlow (2020); outputs 3 = test set outputs regenerated by the present authors. s1 = SARI script (always the same); b1 = Nisioi et al.’s BLEU script, run by Nisioi et al.; b2 = Nisioi et al.’s BLEU script, run by Cooper & Shardlow; b3 = Nisioi et al.’s BLEU script with different version of NLTK tokeniser (see in text), run by the present authors; b4 = SacreBLEU (Xu et al., 2016), run by the present authors.",
2204.05961v1,What is the difference between OTE and OITE evaluation methods?,"OTE stands for outputs vs. targets evaluation, while OITE stands for outputs vs. inputs and targets evaluation.",2204.05961v1-Table5-1.png," Conditions of measurement for each measurement carried out for the NTS systems. OTE = outputs vs. targets evaluation, OITE = outputs vs. inputs and targets evaluation. Shaded cells: evaluation of the same system outputs, i.e. the reproductions did not regenerate outputs. Bold: evaluation of (potentially) different system outputs, i.e. the reproductions did regenerate outputs.",
2204.05961v1,Which measurement method was used for all the measurements?,OTE,2204.05961v1-Table7-1.png, Conditions of measurement for each measurement carried out for the multilingual essay scoring systems. OTE = outputs vs.targets evaluation.,
2204.05961v1,What is the average precision of the essay scoring systems for the first seed of the first test data set?,The average precision of the essay scoring systems for the first seed of the first test data set is 0.687.,2204.05961v1-Table6-1.png," Precision (CV∗) and component measures (mean, standard deviation, standard deviation confidence intervals) for measured quantity values obtained in multiple measurements of the essay scoring systems. Seed i = different approaches to random seeding and cross-validation; ei = different compile/run-time environments; ii = different test data sets and/or cross-validation folds.",
2204.05961v1,What is the most common evaluation measure used in the studies reported in the table?,wF1,2204.05961v1-Table1-1.png, Summary overview of the 18 object/measurand combinations taht were QRA-tested for this paper.,
2204.13074v2,What is the purpose of the feedback memory in the TeachMe system?,The feedback memory stores user feedback on previous questions and answers. This feedback is then used to improve the system's answers to future questions.,2204.13074v2-Figure1-1.png," TeachMe augments the basic questionanswering model with a memory of user feedback. (A) Given a new question, facts retrieved from memory are used as additional context for the model, influencing its answers and proofs. (B) If the user disagrees with an answer, they localize the error in the explanation and offer corrective feedback, which is added to memory. (C) These new facts can then be retrieved if the query is re-asked, helping the system avoid repeating mistakes. Note that these also help improve answers on new, similar questions that are asked later, helping the system improve over time.",
2204.13074v2,Does TeachMe's performance improve with simulated user feedback?,Yes.,2204.13074v2-Figure4-1.png," TeachMe’s performance on the hidden test sets improves with simulated user feedback (from red to yellow), improving over direct QA and coming close (within≈ 1%) of the upper bound of using feedback on all answers (grey).",
2204.13074v2,How does the performance of TeachMe compare to the baseline model as the amount of training data increases?,"TeachMe's performance improves as it sees a larger fraction of training data, while the baseline model's performance remains constant.",2204.13074v2-Figure5-1.png, TeachMe’s performance on OBQA test improves as it sees a larger fraction of training data and stores feedback for wrong answers in its memory.,
2204.13074v2,What is the role of the user in the TeachMe architecture?,"The user acts as a teacher, providing feedback on the generated answers and proofs. If the answer is wrong, the user identifies erroneous model beliefs and adds corrections to the memory.",2204.13074v2-Figure2-1.png," TeachMe’s architecture contains a model and memory. Given a question, TeachMe generates multiple answers and proofs, discards those not consistent with its own beliefs (verification), and presents the best to the user (teacher). If the answer is wrong, the user interacts to identify erroneous model beliefs, and add corrections to memory, which in turn modifies future QA behavior without model retraining.",
2204.13074v2,What percentage of TeachMe's correct answers were due to spurious facts in the proof?,24%,2204.13074v2-Figure6-1.png, TeachMe was right for the right reasons in ≈75% of its correct answers. (Examples are shown in Table E1 in Appendix E).,
2204.13074v2,Which factor contributed the most to TeachMe's incorrect answers?,Retrieval failure.,2204.13074v2-Figure7-1.png, Causes of failure (%) for TeachMe’s incorrect answers. (Examples in Table E2 in Appendix E).,
2204.13074v2,What are the different ways a user can interact with the TeachMe system?,"The user can ask a question, provide feedback on the answer, or provide additional information.",2204.13074v2-Figure3-1.png," TeachMe’s dialog tree, showing the different ways a user can interact with the system.",
2204.13074v2,What is the effect of user interaction on TeachMe's performance?,User interaction improves TeachMe's performance.,2204.13074v2-Figure8-1.png," TeachMe’s performance (% correct) substantially improves on a hidden test set (from 38% to 55%), a subset of OBQA, after users correct/expand its knowledge for the training questions. (Results are averaged over 8 users).",
2204.13314v1, What is the main difference between the proposed method (d) and the existing methods (a-c)?," The proposed method uses region-level contrastive property and consistency, while the existing methods mainly focus on pixel-level consistency and/or contrastive property.",2204.13314v1-Figure1-1.png," Comparison with existing semi-supervised semantic segmentation methods. (a) Pixel-level label consistency [French et al., 2020], (b) Pixel-level feature contrastive property [Lai et al., 2021], (c) Combination of pixel-level label consistency and feature contrastive property [Zhong et al., 2021]. (d) Region-level contrastive property and consistency (Ours). “ // ” on → means stop-gradient. P: pseudo labels, Y: predicted probability maps, RF: region features, RM: region masks, RC: region classes, SM: semantic masks.",
2204.13314v1, What is the effect of the number of queries on the VOC Train and VOC Aug scores? , The VOC Train and VOC Aug scores both increase as the number of queries decreases. ,2204.13314v1-Table4-1.png, Study on the number of queries.,
2204.13314v1,What is the effect of increasing the unsupervised loss weight on the VOC Aug (1/4) performance?,The VOC Aug (1/4) performance increases as the unsupervised loss weight increases.,2204.13314v1-Table3-1.png, (a) Ablation study of each component. The first row is the result of our baseline model. (b) Effect of different unsupervised loss weights.,
2204.13314v1,What is the difference between the ground truth and the supervised baseline?,"The ground truth is the manually annotated segmentation mask, while the supervised baseline is the segmentation mask predicted by a model trained on a labeled dataset.",2204.13314v1-Figure3-1.png," Prediction results on PASCAL VOC 2012 Val set under 1/2 (732) partition. (a) original image, (b) ground truth, (c) supervised baseline, (d) semi-supervised consistency baseline, (e) ours.",
2204.13314v1, What is the purpose of the Region feature contrastive loss in the proposed architecture?, The Region feature contrastive loss aims to encourage the student model to learn region-level representations that are similar to those of the teacher model.,2204.13314v1-Figure2-1.png," Illustrate the architecture of our proposed Region-level Contrastive and Consistency. ps (or pt) and fs (or f t) denote mask logits and mask embeddings of the student model (or teacher model), respectively. F s (or F t) is the pixel decoder output feature map of the student model (or teacher model). rs (or rt) represents region features of the student model (or teacher model). H and W are the height and width of the input image. “MAP” denotes the mask average pooling. N denotes the number of queries in an image from student model, and N t denotes the number of masks in an image from teacher model, respectively. C is the embedding dimension.",
2204.13314v1,Which combination of τm and τf values leads to the highest VOC Aug score?,The combination of τm = 1.0 and τf = 0.5 leads to the highest VOC Aug score of 79.71.,2204.13314v1-Table5-1.png, Effect of different temperature hyper-parameters.,
2204.13314v1,"What are the differences between the ground truth and the predictions made by the supervised baseline, semi-supervised consistency baseline, and the proposed method?","The ground truth is the most accurate representation of the objects in the image. The supervised baseline makes some mistakes, such as not correctly identifying the bus in the first image. The semi-supervised consistency baseline is more accurate than the supervised baseline, but it still makes some mistakes, such as not correctly identifying the people in the last image. The proposed method is the most accurate of the three methods, and it correctly identifies all of the objects in the image.",2204.13314v1-Figure4-1.png," Prediction results on PASCAL VOC 2012 Val set under 1/2 VOC Train. (a) original image, (b) ground truth, (c) supervised baseline, (d) semi-supervised consistency baseline, (e) ours.",
2204.13314v1,Which method performs the best on the VOC Train 1/2 split?,RC^2 (L) performs the best on the VOC Train 1/2 split with a score of 77.06.,2204.13314v1-Table1-1.png," Comparison with the state-of-the-art methods on VOC 2012 Val set. We use labeled data under all partition protocols to train an original MaskFormer as the supervised baseline. Previous works [Ouali et al., 2020; Ke et al., 2020] used the segmentation model which is pretrained on COCO [Lin et al., 2014] dataset for all partition protocols. We only take COCO pretrained model on 1/4, 1/8 and 1/16 VOC Train, and 1/16 VOC Aug. For the rest of partition protocols, we use the backbone pretrained on ImageNet [Deng et al., 2009], and initialize the weight of MaskFormer head randomly. All methods are based on ResNet101 backbone.",
2204.13314v1,Which method performed the best on the Cityscapes Val set?,RC^2L (ours),2204.13314v1-Table2-1.png, Comparison with the state-of-the-art methods on Cityscapes Val set. We use the model which is pretrained on COCO dataset. All methods are based on ResNet101 backbone.,
2205.01543v2,Which model performs the best on the summarization task and how much better is it than the second best model in terms of ROUGE-1 score?,"The PtG model performs the best on the summarization task with a ROUGE-1 score of 42.40. This is 0.61 points higher than the second-best model, MT MODEL (Tuning), which has a score of 41.79.",2205.01543v2-Table1-1.png, Cross-task transferability performance comparisons of different methods in fully-supervised setting. Bold and underline fonts denote the best and the second best methods (the same as below).,
2205.01543v2,Which model performed the best on the CNN/Daily Mail dataset?,PTG,2205.01543v2-Table2-1.png, Cross-dataset transferability performance comparisons of different methods in fully-supervised setting.,
2205.01543v2,Which model performs the best on the Daily Mail dataset when the number of instances is 500?,PTG,2205.01543v2-Table4-1.png, Cross-dataset transferability performance comparisons of different methods in few-shot setting.,
2205.01543v2,Which model performs best on the DIALOG task when the number of instances is 50?,MODELT,2205.01543v2-Table3-1.png," Cross-task transferability performance comparisons of different methods in few-shot setting. B-n, R-n, D-n, and MODELT are short for BLEU, ROUGE, Distinct and MULTI-TASK MODELTUNING (the same as below).",
2205.01543v2,How does the Adaptive Attention Mechanism select the most relevant prompt for the target task?,"The Adaptive Attention Mechanism uses the query, task level, and instance level information to select the most relevant prompt from the Source Prompt Pool.",2205.01543v2-Figure1-1.png, Overview of our proposed model PTG.,
2205.01543v2,Which dataset has the most training examples?,NEWSROOM,2205.01543v2-Table6-1.png," Statistics of our datasets after preprocessing. #Train, #Valid and #Test denote the number of examples in training, valid and test datasets, respectively. #Input and #Output denote the average number of tokens in the input text and output text.",
2205.01543v2,Which model performed best on the summarization task for the CNN/Daily Mail dataset according to ROUGE-1 and ROUGE-2 scores?,The PTG w/o Prompt Pool model performed best on the summarization task for the CNN/Daily Mail dataset according to ROUGE-1 and ROUGE-2 scores.,2205.01543v2-Table5-1.png, Ablation analysis on cross-task transferability experiments.,
2205.01543v2,Which two datasets have the most similar sentence pairs?,SQuADxSum and CNN.,2205.01543v2-Figure2-1.png, Similarity analysis of 14 datasets within our six generation tasks.,
2205.02303v1,Which of the methods shown is most successful at retrieving results for questions with typos in non-stopwords?,DR + Data augm. + CL (ours),2205.02303v1-Table3-1.png," Retrieval results for the settings of (i) questions with typos in non-stopwords (Typos in Non-stopwords), and (ii) questions with typos in highly discriminative utterances (Typos in Discriminative Utterances). Stat. sig. difference w/ paired t-test (𝑝 < 0.05) DR=d; DR+Data augm.=a; DR+CL=c.",
2205.02303v1,Does data augmentation or contrastive learning (CL) improve retrieval performance when there are typos in the questions?,"Yes, both data augmentation and contrastive learning improve retrieval performance when there are typos in the questions. This can be seen by comparing the AR and MRR scores for the ""Original"" and ""Typos in Random Words"" columns.",2205.02303v1-Table2-1.png," Retrieval results for the settings of (i) clean questions (Original), and (ii) questions with typos (Typos in Random Words). Statistical significance difference with paired t-test (𝑝 < 0.05) DR=d; DR+ Data augm.=a; DR+CL=c.",
2205.02303v1,Which method performs best for retrieving documents with typoed words?,The method DR + Data augm. + CL (ours) performs best for retrieving documents with typoed words.,2205.02303v1-Figure2-1.png, Retrieval results w.r.t the relevant importance of the typoed words; on MS MARCO (Dev). Questions are split into bins w.r.t the relevant importance their typoed words.,
2205.02303v1,"Which dataset has more questions, MS MARCO or Natural Questions?",MS MARCO has more questions than Natural Questions.,2205.02303v1-Table1-1.png," Number of questions in each dataset, and the average length of question.",
2205.02303v1,Which method performs best for typoed words that are infrequent in the training set?,DR + Data argum. + CL (ours),2205.02303v1-Figure1-1.png," Retrieval performance (Average Recall@50) w.r.t the frequency on the training set, of the typoedwords at testtime; onMSMARCO (Dev). Questions are split into binsw.r.t the frequency of their typoed words.",
2205.02750v1,What is the maximum gain of the directional weighting function?,The maximum gain of the directional weighting function is approximately 1.0.,2205.02750v1-Figure2-1.png," Directional weighting function w when v̂0 = [−1/ √ 2,−1/ √ 2, 0]T, γ = 0.01, and β = 100. The gain is given by the distance from the origin to the surface, quantified according to the bar on the right.",
2205.02750v1,What is the spatial relationship between the sources and receivers in the experimental setup?,"The sources and receivers are located in two separate spheres, with the source sphere located above the receiver sphere.",2205.02750v1-Figure3-1.png, Experimental setup utilized for the simulations. Red diamonds represent sources and blue circles represent receivers.,
2205.02750v1,Which method performs the best at higher frequencies?,The Directional - Tukey method performs the best at higher frequencies.,2205.02750v1-Figure4-1.png, NMSE performance of the methods compared.,
2205.02750v1,What is the effect of using different estimation methods on the accuracy of the ATFs?,"The different estimation methods have different effects on the accuracy of the ATFs. The uniform method produces ATFs that are less accurate than the true ATFs, while the directional methods (SQE and Tukey) produce ATFs that are more accurate than the uniform method.",2205.02750v1-Figure5-1.png," Distributions of true and estimated ATFs in ΩR from the center of ΩS at [0.35, 0.43, 0.29]T m for 950 Hz. The black circle indicates the bounds of ΩR.",
2205.02750v1,Which of the three distributions of NSEs in ΩR is the most uniform?,The uniform distribution.,2205.02750v1-Figure6-1.png, Distributions of NSEs in ΩR.,
2205.02750v1,What is the function h(r|s) used for in the schematic?,The function h(r|s) is used to calculate the acoustic transfer function (ATF) between a source point s and a receiver point r.,2205.02750v1-Figure1-1.png, Schematic diagram of a region-to-region ATF interpolation problem.,
2205.07324v1,What is the purpose of the dynamic skim in the Transkimmer model?,"The dynamic skim is used to prune tokens during the processing of Transformer layers. This means that the model can focus on the most important tokens in the input sequence, which can improve its performance.",2205.07324v1-Figure1-1.png, Overview of Transkimmer dynamic token skimming method. Tokens are pruned during the processing of Transformer layers. Note that actually we don’t need all the tokens given to the downstream classifier in this sequence classification example. We show the full length output embedding sequence to demonstrate the forwarding design of Transkimmer.,
2205.07324v1,Which dataset has the highest average sample length?,20News,2205.07324v1-Table2-1.png, Summary of evaluation datasets. The input sequence length matches the setting of prior works POWERBERT and LTP. It is determined by covering 99 percentile of input samples without truncation.,
2205.07324v1,Which model achieves the highest accuracy on the 20News dataset?,TR-BERT,2205.07324v1-Table4-1.png, Performance and FLOPs evaluation on several downstream tasks and datasets with BERTbase as backbone model. The speedup results are emphasized considering the padding setting.,
2205.07324v1,Which model achieves the highest accuracy on the QQP task?,RoBERTabase achieves the highest accuracy on the QQP task.,2205.07324v1-Table3-1.png, Performance and FLOPs (speedup) on GLUE benchmark with BERTbase and RoBERTabase as backbone model. Transkimmer is adopted on DistilBERT and ALBERT to shows its applicability to general model compression methods.,
2205.07324v1,Which of the models listed in the table uses a learned discard strategy?,LTP and Transkimmer.,2205.07324v1-Table1-1.png," Summary of prior token reduction works and their design choices including POWER-BERT, LengthAdaptive Transformer (LAT), Learned Token Pruning (LTP) and TR-BERT. The design details are discussed in Sec. 3.",
2205.07324v1,Which dataset is the most sensitive to layer-wise skimming?,MNLI,2205.07324v1-Figure4-1.png, Layer-wise skim strategies analysis of datasets from GLUE benchmark. The normalized area under curve is viewed as an approximate speedup ratio with reference to sequence length.,
2205.07324v1,Which model achieves the highest accuracy on the MRPC dataset when the speedup is 1.5?,Transkimmer,2205.07324v1-Figure3-1.png, Trade-off results between accuracy and speedup of MRPC and SQuAD-v2.0 datasets by tuning the harmony coefficient. Note that different padding settings are used for each baseline while Transkimmer doesn’t count any padding.,
2205.07324v1,What does the color of the text in the figure represent?,"The color of the text in the figure represents the Transformer layer index where the token is pruned. Specifically, the black tokens are fully processed without being skimmed.",2205.07324v1-Table5-1.png," Post-hoc case study of SST-2 sentimental analysis and SQuAD QA tasks from Transkimmer model with BERTbase setting. The color indicated by the colorbar represents the Transformer layer index where the token is pruned. Specifically, the black tokens are fully processed without being skimmed.",
2205.07324v1,What is the role of the Skim Mask Mi in the Transkimmer architecture?,The Skim Mask Mi is used to select a subset of tokens from the input sequence to be processed by the Transformer layer.,2205.07324v1-Figure2-1.png, Architecture and end-to-end optimization objective of Transkimmer. The dashed token embeddings are directly forwarded to the final output of Transformer layers without further processing.,
2205.10312v2,"What is the purpose of the ""Repeat Normalizing Similarity Matrix For batch 2"" step in the figure?","This step is used to fuse the mini-batch similarity matrices to ensure high accuracy. The similarity matrix of each batch focuses only on its own entities, so it needs to be normalized with respect to the other batches before being used to train the classifier model.",2205.10312v2-Figure1-1.png, An example of normalizing mini-batch similarity matrix with ClusterSampler strategy • How to fuse the mini-batch similarity matrices to ensure high accuracy? The similarity matrix of each batch focuses only on its,
2205.10312v2,Which method has the highest H@1 score on DBP1M_EN-FR?,ClusterEA - Global Sim has the highest H@1 score on DBP1M_EN-FR.,2205.10312v2-Table3-1.png, The result of ablation study,
2205.10312v2,Which model performed the best on the IDS15K EN-FR dataset?,Dual-AMN-S,2205.10312v2-Table1-1.png, Overall EA results on IDS15K and IDS100K,
2205.10312v2,Which method achieved the highest H@1 score on DBP1M EN-FR?,LargeEA-G,2205.10312v2-Table2-1.png, Overall EA results on DBP1M,
2205.10312v2,Which step of ClusterEA takes the longest time in both EN-FR and EN-DE settings?,Calculating global similarity takes the longest time in both settings.,2205.10312v2-Figure4-1.png, Scalability analysis vs. variants of ClusterEA,
2205.10312v2,Which dataset has the largest number of triples?,DBP1M,2205.10312v2-Table4-1.png," Statistics of IDS15K, IDS100K, and DBP1M",
2205.10312v2,Which batch sampling strategy has the highest overlap for all datasets?,VPS,2205.10312v2-Figure3-1.png, Comparison results of different batch sampling strategies,
2205.10312v2,What is the role of the SparseFusion module in the ClusterEA framework?,"The SparseFusion module is responsible for fusing the mini-batch similarities into a final similarity matrix. This is done by first using k-NN to find the k nearest neighbors for each entity in the mini-batch. Then, the similarities between each entity and its k nearest neighbors are normalized. Finally, the normalized similarities are fused using the Sp-CSLS algorithm to produce the final similarity matrix.",2205.10312v2-Figure2-1.png, The overall ClusterEA framework,
2205.12393v4,What is the relationship between the two sentences in the image?,The second sentence is a rephrased version of the first sentence.,2205.12393v4-Table1-1.png," An instance from T0 training set (Sanh et al., 2022) where a summarization task is reformatted as a natural language response to a natural language input.",
2205.12393v4,Which task has the lowest relative gain at the beginning of the sequential learning process?,Simplification (green and orange),2205.12393v4-Figure2-1.png," Progressive Relative Gain results for CT0 (11B) during the sequential learning(Y axis) vs Number of Training steps(X axis). The curves for tasks T0, ...T7 are displayed respectively at step 0, ..., i such that only the first task, Simplification (green and orange) is present at step 0, then HGen (red) etc.",
2205.12393v4,Which emotion is most likely to be classified as positive?,Grateful.,2205.12393v4-Figure3-1.png," Emotion Generalization: Percentage of Haiku classified as positive, when adding emotion specific constraints to the Haiku instruction. We used an open source binary sentiment analysis classifier.6",
2205.12393v4,Which constraint generalization method performs the best when using three constraints?,HGen,2205.12393v4-Table4-1.png," Table showing Constraint generalisation i.e % of instructions completely respected, when providing constraints for unseen prompts. CT0NoCons corresponds to providing the same input without constrain.",
2205.12393v4,Which model has the highest BLEU4 score on the T0 zero-shot tasks?,"The model with the highest BLEU4 score on the T0 zero-shot tasks is UB_pp, with a score of 85.3.",2205.12393v4-Table3-1.png," Results for the starting checkpoints T0_3B and T0pp(11B), their upper bounds scores and our final models as well as LAMOL. Bolded result means there less than 2% forgetting. T0tr and T0zs denote respectively for T0 train-tasks and T0 zero-shot-tasks and are the average accuracy obtained on all their evaluation datasets. B4, R1, BS denote BLEU4, ROUGE-1 and BERTScore. Note that we detail the intermediate steps results in the Appendix.",
2205.12393v4,What are the eight new tasks that T0 has learned continually?,"The eight new tasks that T0 has learned continually are: text simplification, headline generation, haiku generation, COVID QA, inquisitive question generation, empathetic dialogue generation, explanation generation, and Twitter stylemetry.",2205.12393v4-Table2-1.png, Example Instructions with their respective ground-truth for 8 new tasks learned continually from T0.,
2205.12393v4,Which model performs the best on the BS metric?,CT03B,2205.12393v4-Table5-1.png," Results including T5-small and T5-3B, T0_3B, and a 3B Transformer randomly initialised. We can observe that 1) only CTrand largely degrades w.r.t. its UB, UB_rand; 2) even T5_small is able to mostly maintain its performance indicating that scale is not what matter the most.",
2205.12393v4,"What are the different outputs for HGen, Haiku, and Exp from T0pp and CT0?","The outputs for HGen, Haiku, and Exp from T0pp and CT0 are shown in the table. For HGen, T0pp generated the title ""sri lanka closes schools as tamil rebels advance"", while CT0 generated the title ""sri lanka closes schools as war with tamils escalates"". For Haiku, T0pp generated the haiku ""a lone tree in the mountains is haunted by the wind"", while CT0 generated the haiku ""mountain winds haunt, the hollow of the stones, voices echo there."". For Exp, both T0pp and CT0 correctly identified that the two sentences do not entail each other.",2205.12393v4-Table6-1.png," Outputs for HGen, Haiku and Exp from T0pp and our continually learned final model CT0.",
2205.12393v4,Which model performs better when trained with 0.25% of the training data?,Simp,2205.12393v4-Figure1-1.png," Rehearsal ablation with 0.0, 0.25 and 1.0% of training data showing target task performance along with T0 zero-shot performance(T0zs) with Relative Gain in Y axis vs Number of training steps in X axis. The results are normalised in % such that -1 corresponds to 100% decrease and +1 means +100% increase w.r.t. the initial performance.",
2205.12393v4,Which model performs best on the continual training setup?,The best performing model on the continual training setup is T0 11B + Twst.,2205.12393v4-Table7-1.png," Progressive results for T0 3B and 11B results for continual training set up with best 3B results underlined & best 11B results bolded. T0zs denotes T0 zero-shot and is the average accuracy obtained on 12 eval datasets. B4, R1, BS denote BLEU-4, ROUGE-1 and BERTScore.",
2205.14521v1,Which approach achieved the highest ROUGE F1 score for desired length 8?,Our replication of Schumann et al. (2020),2205.14521v1-Table1-1.png," Results on the Gigaword headline generation test set. Len: Average length of predicted summaries. R-1, R-2, R-L: ROUGE-1, ROUGE-2, ROUGE-L. ∆R: The difference of total ROUGE (sum of R-1, R-2, and R-L) in comparison with the (previous) state-of-the-art search method under replication. Inf.Time: Average inference time in seconds for one sample on an i9-9940X CPU and a RTX6000 GPU. Speedup: Relative to Schumann et al. (2020). †Results quoted from previous papers; others are given by our experiments.",
2205.14521v1,Which model achieved the highest ROUGE Recall score on the DUC2004 dataset?,"NAUS (length control) achieved the highest ROUGE Recall score on the DUC2004 dataset, with an R-1 score of 26.71, an R-2 score of 7.68, and an R-L score of 23.06.",2205.14521v1-Table2-1.png, Results on the DUC2004 dataset. †Quoted from previous papers.,
2205.14521v1,Which model achieves the highest ROUGE-L score in Group A?,"Our NAUS (LC) model achieves the highest ROUGE-L score in Group A, with a score of 25.51.",2205.14521v1-Table3-1.png, Model analysis on headline generation. AR: Autoregressive models. NAR enc-dec: Nonautoregressive encoder–decoder. NAR enc-only: Nonautoregressive encoder-only. T: Truncating. LC: Length control. All AR and NAR models use the Transformer architecture.,
2205.14521v1,Which approach achieved the highest ROUGE score for Group A?,NAUS8->8,2205.14521v1-Table7-1.png," Analysis of length-transfer summary generation. A subscript i→ j (or j%) refers to a model trained with i words and tested for j (or j%) words. Len: Average length of predicted summaries. R-1, R-2, R-L: ROUGE-1, ROUGE-2, ROUGE-L. ∆R: The difference of total ROUGE (sum of R-1, R-2, and R-L) in comparison with the (previous) state-of-the-art model (Schumann et al., 2020) under replication. Inf.Time: Average inference time in seconds for one sample on an i9-9940X CPU and a RTX6000 GPU. Speedup: Relative to Schumann et al. (2020). †Results quoted from previous papers; others are given by our experiments. Su et al. (2021)’s approach has a soft length penalty to encourage short output, but cannot generate longer summaries than trained.",
2205.14521v1,How is the training process different from the inference process in the NAUS approach?,"In the training process, the model is trained on a dataset of text and corresponding labels. The goal of training is to learn the relationship between the input text and the output labels. In the inference process, the trained model is used to predict the output labels for new, unseen text.",2205.14521v1-Figure1-1.png," The overview of our NAUS approach. In each search step, input words corresponding to grey cells are selected. Besides, the blue arrow refers to the training process, and the green arrow refers to inference.",
2205.14521v1,Which decoding strategy is better for overall quality?,Length control.,2205.14521v1-Table5-1.png," Human evaluation comparing truncating and length control for our NAUS model on 50 samples in the Gigaword headline generation task. The results are statistically significant, where the p-value is given by a one-sided binomial test.",
2205.14521v1,What is the difference between the NAUS (truncate) and NAUS (length control) summaries?,"The NAUS (truncate) summary simply truncates the original sentence at a certain length, while the NAUS (length control) summary generates a new sentence that is shorter than the original but still conveys the same meaning.",2205.14521v1-Table6-1.png, Example summaries for Gigaword headline generation. The gray words are truncated for fair comparison.,
2205.14521v1,What are the conditions for the length-control algorithm to generate a token at generation slot s and time slot t?,The conditions are that either the previous token at generation slot s-1 and time slot t-1 is not the end-of-sentence token or the previous token at generation slot s-1 and time slot t is the beginning-of-sentence token.,2205.14521v1-Figure2-1.png, Illustration of our length-control algorithm.,
2205.14521v1,Which method has a higher average ΔR?,Length Control,2205.14521v1-Figure4-1.png, Comparing our length-control NAUS and the truncated CTC beam search on the Gigaward headline generation test set.,
2205.14521v1,Does the performance of NAUS increase or decrease with the number of training samples?,Increase.,2205.14521v1-Figure3-1.png," Performance versus the number of training samples in the setting of Group B, Table 1. Notice that NAUS is trained by pseudo-groundtruth given by unsupervised edit-based search (Schumann et al., 2020). Thus, our approach is indeed unsupervised.",
2206.01367v1,What is the effect of adding RCAD to the base methods on the test accuracy of the models?,Adding RCAD to the base methods generally increases the test accuracy of the models.,2206.01367v1-Figure2-1.png," Main results on supervised image classification benchmarks: We plot the mean test accuracy and 95% confidence intervals over 10 independent runs for models trained with base methods: Data Augmentation (DA), Label Smoothing (LS), CutOut/CutMix (CutOut+Mix) augmentation, MixUp and compare them with the test accuracies of the models trained with the RCAD objective in Equation 2, in addition to the base methods.",
2206.01367v1,How does RCAD regularization improve test accuracy?,RCAD regularization improves test accuracy by reducing confidence along adversarial directions.,2206.01367v1-Figure1-1.png," Reducing confidence along adversarial directions (RCAD) is a simple and efficient regularization technique to improve test performance. (Left) For RCAD, examples are generated by taking a large step (10× typical for adversarial examples) along the gradient direction. We see that generated images thus look very different from the original, with accentuated spurious components responsible for the model’s flipped predictions on adversarial images. (Right) RCAD achieves greater test accuracy than data augmentation (DA), label smoothing (LS), and methods that minimize cross-entropy on adversarial examples: adversarial training via FGSM [13] and ME-ADA [72].",
2206.01367v1,Which method performs best in the low-data regime?,RCAD + LS,2206.01367v1-Figure3-1.png," (Left)How should we use adversarial examples to improve test accuracy? We compare RCAD with adversarial baselines including label smoothing on adversarial samples (LS on Adv), measuring the improvement in test accuracy relative to ERM. We also compare RCAD with ME-ADA when combined with label smoothing. (Right) RCAD is more effective in low-data regime. We compare the test accuracy improvement over ERM for baselines: Label Smoothing (LS), Data Augmentation (DA), adversarial training, and ME-ADA with RCAD + LS on different sub-samples of CIFAR-100 training set (0.5k → 50k). We find RCAD achieves the largest gains in the low data regime. In both plots, we plot the mean and 95% confidence intervals over 10 independent runs.",
2206.01367v1,Which regularizer performs the best when the dataset size is small?,RCAD.,2206.01367v1-Table4-1.png," RCAD is more effective than other existing regularizers in low data regime. We evaluate the effectiveness of RCAD over other baseline regularizers as we decrease the size of the training data for CIFAR-100. We find that the performance gain from using RCAD is higher as training data size reduces, possibly since RCAD is more robust to spurious correlations in the training data (compared to other baselines), and deep models are more vulnerable to such correlations in the low data regime [67]. Figure 3b also plots the same results, relative to the performance of ERM.",
2206.01367v1,Which model performs the best on the ImageNet classification task?,The model finetuned with RCAD performs the best on the ImageNet classification task.,2206.01367v1-Table5-1.png," Preliminary results on Imagenet classification. We compare the performance of a pretrained ResNet-50 model, with the models obtained by finetuning the pretrained model with either the cross-entropy loss (finetuned with ERM) or the RCAD objective (finetuned with RCAD). We show the mean accuracy and 95% confidence interval evaluated over 7 independent runs.",
2206.01367v1,What is the effect of label smoothing on the performance of RCAD and ME-ADA?,Label smoothing improves the performance of both RCAD and ME-ADA.,2206.01367v1-Table3-1.png," (Left) RCAD outperforms adversarial baselines even when combined with existing regularizers. We compare the performance of RCAD and ME-ADA when both methods are trained with label smoothing (smoothing parameter ε = 0.2). We find that label smoothing improves the performance of both methods, but the benefit of RCAD still persists. Figure 3a also plots the same results, relative to the performance of ERM. (Right) RCAD can outperform baselines when used with larger backbones. Here, we train our method RCAD and baselines ERM, ME-ADA with the larger architecture Wide ResNet 28-10 on CIFAR-100 (C100), CIFAR-100-2k (C100-2k) and CIFAR-100-10k (C100-10k) benchmarks. We find that while the performance gains of RCAD over ME-ADA and ERM are higher with the ResNet-18 backbone, RCAD continues to outperform them here as well. Figure 4a also plots the same results, relative to the performance of ERM.",
2206.01367v1,What is the difference between the ERM and RCAD decision boundaries for the noisy feature x2?,"The ERM decision boundary is almost vertical, while the RCAD decision boundary is almost horizontal. This suggests that the ERM solution is more sensitive to noise in the x2 feature than the RCAD solution.",2206.01367v1-Figure5-1.png," RCAD corrects ERM solution: (Left) We plot the training data and high-dimensional decision boundaries for all three estimates ŵerm and ŵrcad with α = 0.01, 0.70. Here, we plot the linear boundary projected onto the set of planes {x1, xi}i=8 i=2 spanned by the true feature x1 and noisy features x2, . . . , x8. Across the ERM and RCAD training iterations, we plot the weight norms along the true feature |w1| and noisy dimensions ‖w2‖2 in the (Center) plot, and on the (Right) we plot the train/test accuracies.",
2206.01367v1, What is the difference between cross-entropy and label smoothing? ," Cross-entropy minimizes the negative log-likelihood of the correct class, while label smoothing adds a small amount of probability to all other classes. ",2206.01367v1-Table1-1.png, Regularization objectives: We summarize prior works that employ adversarial examples or directly regularize model’s predictions pw(y | x) along with the scalar hyperparameters (in [·]) associated with each.,
2206.01367v1,Which method consistently outperforms the others in terms of test accuracy?,RCAD consistently outperforms the other methods in terms of test accuracy.,2206.01367v1-Table2-1.png," RCAD consistently outperforms adversarial baselines. In Section 4.2 we investigated the performance of RCAD against baseline methods that use the adversarially generated perturbations differently, i.e., methods that reduce cross-entropy loss on adversarial examples. This table presents the test accuracies for RCAD and other baselines with 95% confidence interval computed over 10 independent runs. Figure 3a plots the results reported here by looking at the performance improvement of each method over the ERM baseline.",
2206.01367v1,Which method is most robust to different types of noise?,Our Objective (RCAD),2206.01367v1-Figure6-1.png, Robustness to natural shifts in distribution: Plots comparing the performance of standard ERM training and ME-ADA against our method (RCAD) trained on CIFAR-10 benchmark and tested on various distribution shifts in the corrupted CIFAR-10-C benchmark.,
2206.01367v1,Which method is the most robust to FGSM attacks on CIFAR-100-2k?,RCAD,2206.01367v1-Table6-1.png," Robustness to adversarial perturbations compared against in-distribution test accuracy: Clean (evaluation on iid test data) and Robust test accuracies of adversarial methods (adversarial training (FGSM [13]), ADA and ME-ADA), ERM and RCAD when the adversary carries out attacks using the Fast Gradient Sign (FGSM) method when the strength of the attack in l1 norm is 0.05. We show the mean accuracy and 95% confidence interval evaluated over 10 independent runs.",
2206.01367v1,How does the test accuracy of RCAD compare to that of ERM and ME-ADA on the CIFAR-100-2k dataset?,The test accuracy of RCAD is higher than that of ERM and ME-ADA on the CIFAR-100-2k dataset.,2206.01367v1-Figure4-1.png," RCAD can be used with larger networks and is also effective on regression tasks. (Left) We compare the improvements in test accuracy (over ERM) for RCAD and ME-ADA when all methods use Wide ResNet 28-10 [66]. (Center) For CIFAR-100-2k, we show the effect on test accuracy of the step size α that RCAD takes while generating out-of-distribution examples along the adversarial direction. (Right) We compare the negative log-likelihood on test samples for RCAD with baselines ERM and ME-ADA on five regression datasets from the UCI repository. For the above we show the mean and 95% confidence intervals across 10 runs.",
2206.01367v1,Which method performs the best when the dimension of the input is 1000 and the standard deviation of the noise is 0.1?,RCAD,2206.01367v1-Table7-1.png," In the toy non-linear classification problem, RCAD is found to be more robust to spurious correlations in the training data. We show the test performance of RCAD vs. baseline methods ERM, adversarial training (FGSM) and ME-ADA as we vary two parameters: d which is the dimension of the input, and σ which is the standard deviation of the noise added to every dimension.",
2206.01367v1," Which method is more robust to spurious features, ERM or RCAD? ", RCAD. ,2206.01367v1-Figure7-1.png," RCAD objective learns decision boundary using only generalizable features: We simulate a high-dimensional non-linear classification problem by projecting a simple 2-d dataset into a 625-dimensional space. (a) Standard ERM training overfits to this dataset, achieving perfect training accuracy by picking up on spurious features. Plotting a 2D projection of the decision boundary along the true features, we see that it poorly separates the data along the generalizable (true) features. (b, c, d) Visualizing our method (RCAD) at different snapshots throughout training, we see that it converges to the true decision boundary using only the generalizable features, thereby achieving a higher test accuracy.",
2206.04153v2,Which key event in the 2019 Hong Kong protests involved the storming of the Legislative Building?,Key Event 1: July 1 Storming Legislative Building,2206.04153v2-Figure1-1.png, An illustrative example of key event detection task.,
2206.04153v2,Which dataset has more documents per event?,The Ebola dataset has more documents per event.,2206.04153v2-Table1-1.png, Datasets statistics.,
2206.04153v2,"What is the relationship between the actions ""squirt pepper spray"" and ""unfurl umbrella"" in the context of the 2019 Hong Kong protests?","The actions ""squirt pepper spray"" and ""unfurl umbrella"" are related as a cause and effect in the context of the 2019 Hong Kong protests. Specifically, the riot police squirting pepper spray at protesters caused them to unfurl umbrellas for protection.",2206.04153v2-Figure2-1.png," An example of event structure hierarchy. documents about HK Legislative Building Storming and HK Airport Sit-In from a corpus related to “2019 HK protests”) [12]. Another line of work [8, 10, 23, 34] is action extraction which tries to extract concrete actions (represented as text spans) from input documents. For example, one action extracted from the sentence in Figure 2 can be “Riot police squirt pepper spray at protesters”. These methods typically require a predefined event schema along with massive human-labeled documents for model learning. Besides, their output event mentions are highly redundant as one real-world event can usually be expressed in different ways in multiple documents, which further prevents humans from seeing the overall picture of the event.",
2206.04153v2,Which dataset shows the best performance for the 5-F1 metric when varying 𝑤?,The Ebola dataset shows the best performance for the 5-F1 metric when varying 𝑤.,2206.04153v2-Figure4-1.png, Performances of EvMine on the Ebola (top) andHK Protest (bottom) corpus when varying𝑤 (left) or 𝑟 (right).,
2206.04153v2,What are the two major events that are shown in the figure?,The 2019 Hong Kong Protests and the 2014 Ebola Outbreak.,2206.04153v2-Figure5-1.png," Example outputs of our framework onHK Protest (top) and Ebola (bottom), including the titles (or the first sentences for Ebola) of some top-ranked event-related news articles predicted by our method that matched with real-world key events.",
2206.04153v2,What is the relationship between the two documents shown in the figure?,"The two documents are both related to Hong Kong, but they discuss different topics. Document 529 is about an outbreak of African swine fever in Hong Kong, while Document 960 is about the high cost of housing in Shenzhen, China.",2206.04153v2-Figure6-1.png, Outlier documents found in HK Protest. Key phrases related to themajor theme are colored red and those related to documents’ actual topics are colored green.,
2206.04153v2,What are the different stages of the EvMine framework for unsupervised key event detection?,"The EvMine framework consists of four main stages: 

1. Event-related peak phrase detection
2. Event-indicative peak phrase clustering
3. Iterative key event document selection
4. Document classification",2206.04153v2-Figure3-1.png, An overview of our proposed unsupervised key event detection framework EvMine.,
2206.07687v3,Which pruning scheme has the highest PSNR for REDS4 dataset?,The pruning scheme with the highest PSNR for REDS4 dataset is BasicVSR-uni.,2206.07687v3-Table1-1.png," Quantitative comparison (average PSNR/SSIM). Pruning schemes applied on bidirectional and unidirectional BasicVSR (“bi” and “uni”) and marked in rouse and gray, respectively. ∗ means the space-time knowledge distillation scheme [47]. We mark the best results among comparing pruning schemes in bold. The FLOPs and runtime are computed based on an LR size of 180× 320.",
2206.07687v3,How does RSC differ from the other pruning schemes in the figure?,"RSC does not require aligning the pruned indices on the first and last Convs, while the other schemes do. This allows RSC to fully utilize restoration information and prune the first and last Convs of residual blocks without restrictions.",2206.07687v3-Figure1-1.png," Illustration of different schemes for pruning residual blocks of recurrent networks. (a) Structure of the residual block in the VSR network. (b) The residual block pruning schemes [7, 23, 42] do not prune the last Conv. (c) ASSL [57] and SRPN [58] prunes the same indices on skip and residual connections to keep channel alignment, which abandons some channels of input and output feature maps. (d) RSC preserves all channels of input and output feature maps, which does not need to align the pruned indices on the first and last Convs in recurrent networks, can fully use restoration information, and can prune the first and last Convs of residual blocks without restrictions.",
2206.07687v3,How does the performance of SSL-bi compare to the other methods in terms of PSNR and FLOPs?,"SSL-bi achieves the highest PSNR among the four methods, while also requiring the highest number of FLOPs.",2206.07687v3-Figure4-1.png, PSNR (dB) comparison on REDS4 (×4) between SSL and three other methods obtaining the same small network.,
2206.07687v3,Which of the four SSL methods has the highest PSNR?,SSL4 (Ours),2206.07687v3-Table2-1.png," Validation of the components in our SSL. PSNR (dB) results evaluated on REDS4 [33] (4×). The backbone is BasicVSR [2], and the pruning ratio is set to 0.5.",
2206.07687v3,What is the effect of pruning on the quality of video super-resolution?,Pruning can improve the quality of video super-resolution.,2206.07687v3-Figure3-1.png," Qualitative comparison between various VSR and pruning schemes on REDS4 [33], Vid4 [27], and Vimeo90K-T [49], separately.",
2206.07687v3,What is the general trend of the pruning ratios for the Conv layers in the forward network?,The pruning ratios for the Conv layers in the forward network generally decrease as the residual block index increases.,2206.07687v3-Figure5-1.png," (a), (b), and (c) show the pruning ratios of Conv layers in forward, backward and upsampling networks, respectively.",
2206.07687v3,What happens to the average scaling factors of the kept and pruned filters as the number of iterations increases?,"The average scaling factors of the kept filters increases, while the average scaling factors of the pruned filters decreases.",2206.07687v3-Figure6-1.png, The SSL pruning process of Convs in the BasicVSR [2].,
2206.07687v3,Which pruning ratio results in the highest PSNR for the local criteria?,0.3,2206.07687v3-Table3-1.png, PSNR (dB) comparison on REDS4 (4×) for our pruning scheme (SSL) with different pruning criteria and pruning ratios. The unpruned model is BasicVSR [2] baseline.,
2206.07687v3," 
What are the two types of networks that make up the VSR network? "," 
The VSR network is made up of a forward network and a backward network. ",2206.07687v3-Figure2-1.png," (a) The basic architecture of the VSR methods with the bidirectional recurrent network. The forward and backward networks both consist of numerous residual blocks. The upsampling networks contain multiple pixel-shuffle operations and Convs. (b) The pruning scheme for the pixel-shuffle. For the 2× upsampling pixel-shuffle [37] operation, we take four channels with consecutive indices as the pruning unit to guarantee the accuracy of channel-space conversion after pruning. (c) The application of SSL on VSR models.",
2206.14969v1,What is the difference between the two sentences in the image?,"The first sentence describes a physical property of the factory, while the second sentence describes a non-physical property.",2206.14969v1-Figure1-1.png, Two long-term tag dependency examples in English.,
2206.14969v1,What is the best performance achieved by the MPoSM model on the 45-tag English Penn WSJ dataset?,The best performance achieved by the MPoSM model on the 45-tag English Penn WSJ dataset is 78.6 (±1.7) for the M-1oR metric and 77.9 (±1.8) for the M-1 metric.,2206.14969v1-Table3-1.png," POS induction performance on the 45-tag English Penn WSJ dataset. Numbers are the 5-run averages plus standard deviations. In the last row group, we include the numbers of baselines that have unspecified model selection procedures and no official code available (denoted by **), or use a more carefully designed model selection method (denoted by *).",
2206.14969v1,Which model performs the best on the universal treebank?,Gupta et al. (2022),2206.14969v1-Table2-1.png, Performance on the universal treebank. Gupta et al. (2022) also leverages pretrained mBERT model. All the other models do not use pretrained models or embeddings. Subscript OR denotes models evaluated by oracle M-1 and ** refers to unspecified model selection. Standard deviations and non-oracle numbers are in the Appendix D.,
2206.14969v1,What is the difference between the sentences generated for D(0) and MORPH?,"The sentences generated for D(0) have randomly generated characters in every word, while the sentences generated for MORPH have words with useful morphological features.",2206.14969v1-Figure5-1.png," Illustration of the tag-level regular expression used to generate sentences for D(0) and MORPH. For MORPH, each word has useful morphological features, while all the characters in every word are randomly generated in D(0).",
2206.14969v1,Which model achieved the highest percentage of perfect runs on the synthetic dataset?,MPoSM (full),2206.14969v1-Table10-1.png, The Oracle M-1 score of different models on the synthetic dataset. The number in the bracket is the percentage of perfect runs (100 M-1).,
2206.14969v1,How do the sizes of the predicted clusters compare to the sizes of the gold clusters?,The predicted clusters are generally larger than the gold clusters.,2206.14969v1-Figure6-1.png, Log-scale sizes of the predicted clusters and the gold clusters for pt-br in the universal treebank.,
2206.14969v1,What is the performance of the MP-OSM model with a width of 2 on the German language dataset?,68.5 ± 2.8,2206.14969v1-Table4-1.png, Oracle M-1 performance of different context types on the four different languages.,
2206.14969v1,"What is the maximum number of times the sequence ""o1 o2"" can appear in a sentence in D(2-4)?",Two.,2206.14969v1-Figure3-1.png, Illustration of the tag-level regex for sentences in D(2-4). D(0) sentences can be generated by removing the “o1 o2” block between n and v.,
2206.14969v1, Which of the three synthetic subsets has the most specific regular expression?, D(0),2206.14969v1-Table8-1.png, Tag-level regular expressions and the distances between n and v for each synthetic subset.,
2206.14969v1,Which context type performs the best on Korean?,MPoSM (width=2),2206.14969v1-Table9-1.png, Oracle M-1 Performance of different context types on English and Korean.,
2206.14969v1,What is the difference between the local POS prediction module and the masked POS reconstruction module?,"The local POS prediction module predicts the POS tags for each word in the sentence, while the masked POS reconstruction module reconstructs the POS tags for masked words.",2206.14969v1-Figure2-1.png, Illustration of our MPoSM. The model consists of two parts: the local POS prediction module (blue part at the bottom) and the masked POS reconstruction module (green part at the top).,
2206.14969v1,"Which language has the highest mutual information between the tag-level context and the tag zi when the context type is z_{i-2}, z_{i-1}?",Swedish (sv),2206.14969v1-Table7-1.png, Mutual information between the tag-level context and the tag zi on all 10 languages in the universal treebank.,
2206.14969v1,Which model performed the best on the Universal Dependency dataset for the German language (de)?,MPoSM + mBERTOR,2206.14969v1-Table6-1.png, Performance on the Universal Dependency dataset. Gupta et al. (2022) also leverage pretrained mBERT model. All the other models do not use pretrained models or embeddings. Subscript OR denotes models evaluated by oracle M-1 and ** refers to unspecified model selection.,
2206.14969v1,How does the size of the predicted clusters compare to the size of the gold clusters?,The predicted clusters are generally smaller than the gold clusters.,2206.14969v1-Figure4-1.png, Log-scale sizes of the predicted clusters and the gold clusters.,
2206.14969v1, Which method performed the best on the D(2-4) dataset?, MPoSM (full),2206.14969v1-Table5-1.png, Oracle M-1 on the synthetic datasets with the percentage of perfect runs (100 M-1) in the bracket.,
2206.14969v1,Which language has the largest vocabulary size?,Japanese,2206.14969v1-Table1-1.png," Dataset statistics. For each row, the language with the largest number are in bold and the language with the smallest number is underlined. Computation details about the tag mutual information is in Appendix E.",
2207.01079v3,What is the relationship between the amount of TeO2 and the density of the LBPT samples?,There is a positive correlation between the amount of TeO2 and the density of the LBPT samples.,2207.01079v3-Figure4-1.png," Multi-cell composition tables (a) Complete information (Koudelka et al., 2014) (b) Partial information (Epping et al., 2005)",
2207.01079v3,Which table type has the most tuples in the training set?,NC,2207.01079v3-Table4-1.png," Number of (a) each of the table types and (b) journals from which the tables are obtained, materials in the tables, and the tuples for the three splits.",
2207.01079v3,What are the different types of embeddings used by TAPAS-ADAPTED to represent table cells?,"The different types of embeddings used by TAPAS-ADAPTED to represent table cells are: cell embeddings, column index embeddings, row index embeddings, MatSciBERT embeddings, token embeddings, position embeddings, and segment embeddings.",2207.01079v3-Figure7-1.png, Schematic of TAPAS-ADAPTED baseline model,
2207.01079v3, What is the relationship between the amount of bromine and the percentage of selenium in the (GeSe2)1−x(GeBr4)x and (GeSe2)1−xBrx samples?, The percentage of selenium decreases as the amount of bromine increases.,2207.01079v3-Figure6-1.png," Percentages as variables (Uemura et al., 2001)",
2207.01079v3,What is the difference between the glass compositions of GLSC10 and GLSC40?,"The glass composition of GLSC10 is (80GeSe2-20Ga2Se3)90-(CsCl)10, while the glass composition of GLSC40 is (72GeSe2-28Ga2Se3)90-(Li2S)10.",2207.01079v3-Figure1-1.png," Examples of composition tables (a) Multi-cell complete-info (Moguš-Milanković et al., 2003) (b) Multicell partial-info with caption on top (Marmolejo et al., 1999) (c) Single-cell (Brehault et al., 2014)",
2207.01079v3, What are the two possible outcomes of the SCC Predictor? ," The two possible outcomes of the SCC Predictor are ""Yes"" and ""No."" ",2207.01079v3-Figure3-1.png, The design of DISCOMAT,
2207.01079v3,Which hyper-parameter has the biggest difference between GNN1 and GNN2?,Peak LR for LM.,2207.01079v3-Table5-1.png, Hyper-parameters for DISCOMAT.,
2207.01079v3,What is the most common type of table that is misclassified by the model?,MCC-CI,2207.01079v3-Figure5-1.png, Confusion matrix for all table types,
2207.01079v3, Which table type has the highest ID F1 score? , MCC-CI ,2207.01079v3-Table3-1.png, DISCOMAT performance on the table-types.,
2207.01079v3,Which model performed the best on the subset of data containing only MCC-CI and NC table types according to the F1 score?,TABBIE-ADAPTED performed the best with an F1 score of 80.18.,2207.01079v3-Table1-1.png, Performance of V-DISCOMAT vs baseline models on the subset of data containing only MCC-CI and NC table types.,
2207.01079v3,Which model performs best in terms of TT Acc.?,DISCOMAT w/o features,2207.01079v3-Table2-1.png, Contribution of task-specific features and constraints in DISCOMAT on the complete dataset.,
2207.01079v3,What is the composition of sample 10?,The composition of sample 10 is 0.85TeO2-0.15WO3 + 0.056 wt%CeO2.,2207.01079v3-Figure8-1.png," Examples of corner case composition tables (a) (Murata et al., 2005) (b) (Duclère et al., 2009) (c) (Shin et al., 2002)",
2207.01079v3,Which of the three glass compositions in (b) has the highest amount of SiO2?,"The HILWMS-8 glass composition has the highest amount of SiO2, with 33.50 mol%.",2207.01079v3-Figure9-1.png," Some more examples of corner case composition tables (a) (Kaur et al., 2015) (b) (McKeown et al., 2003) (c) (Omrani et al., 2014)",
2207.06989v1,Which training and testing dataset combination results in the highest classification accuracy for the 5-shot setting?,The highest classification accuracy for the 5-shot setting is achieved when training on miniImageNet and testing on CUB-200-2011.,2207.06989v1-Table1-1.png, Classification accuracy (%) results comparison with 95% confidence intervals for cross-domain evaluation with rotation3. More results see Appendix A.4.,
2207.06989v1,Which method achieves the best 1-shot accuracy on the miniImageNet dataset?,HTS (ours),2207.06989v1-Table2-1.png, Accuracy (%) with rotation3. Best results are displayed in boldface. Train/test-SSL/-DA mean using data augmentation and self-superivised learning during training/testing. KD means knowledge distillation and & means “and” operation.,
2207.06989v1,What is the difference between the learning process for few-shot image classification using pretext tasks in previous works and the work presented in this paper?,"Previous works use all images indiscriminately, while the work presented in this paper exploits the hierarchical tree structure to adaptively select useful feature representations.",2207.06989v1-Figure1-1.png," Differences in the learning process for few-shot image classification using pretext tasks between previous and our works. (a) shows the process of generating augmented images using FSL images. (b) and (c) show the learning process for previous works under the DA or SSL setting, which uses all images indiscriminately. (d) and (e) show the learning process for our work under the DA or SSL setting, which can exploit the hierarchical tree structure to adaptively select useful feature representations.",
2207.06989v1,What is the role of the subscript and superscript in the figure?,"The subscript represents the number of child nodes, and the superscript represents the number of levels.",2207.06989v1-Figure2-1.png," The learning process of gated selection aggregating component. (a) shows that the aggregator to sequentially and hierarchically aggregate the information from bottom to top level. (b) details the internal aggregation of TreeLSTM (e.g., two levels). The subscript marked in this figure represents the number of child nodes, the superscript represents the number of levels and different colors are different LSTM cells.",
2207.06989v1,Which pretext task and dataset combination achieves the highest accuracy?,+DA and CUB-200-2011,2207.06989v1-Figure3-1.png," The results (5-shot) for motivation on miniImageNet (MINI), CUB-200-2011 (CUB), CIFAR-FS (CIFAR) and tieredImageNet (TIERED) with two pretext tasks.",
2207.06989v1,How does the number of child nodes affect the accuracy of the ProtoNet and Rotation methods?,The accuracy of the ProtoNet and Rotation methods increases as the number of child nodes increases.,2207.06989v1-Figure6-1.png, (a) and (b) indicate different pretext tasks with the same number of child nodes. (c) and (d) indicate different numbers of child nodes with same pretext task.,
2207.06989v1,Which dataset shows the most improvement in accuracy when the number of levels increases from 1 to 4 in the 5-way-1-shot setting?,CIFAR-FS,2207.06989v1-Figure5-1.png," (a) is the forgetting gate for different child nodes (vertical axis) with the same raw image (horizontal axis). (b) is the correlations of different augmented images (vertical axis is rotation1, horizontal axis is color perm1), where the diagonal (or off-diagonal) line is based on the same raw (or different raw) images. (c)-(d) are classification accuracy (%) for different numbers of levels under ProtoNet with HTS.",
2207.06989v1,Which dataset has the highest number of images?,"miniImageNet and CIFAR-FS, both with 600,000 images.",2207.06989v1-TableA.4-1.png,Table A.4. The details for four benchmark datasets.,
2207.06989v1,Which method appears to be the best at separating the five classes of data?,TreeLSTM.,2207.06989v1-Figure4-1.png, (a)-(c) represent the t-SNE of the five class features. (b) and (c) are our method.,
2207.06989v1,Which method has the highest accuracy on the miniImageNet dataset for the 5-shot setting?,"MatchingNet + HTS.SSL has the highest accuracy on the miniImageNet dataset for the 5-shot setting, with an accuracy of 74.76%.",2207.06989v1-Table3-1.png, Accuracy (%) by incorporating HTS (two-level trees) into each method with rotation3 and the Conv4-64. Best results are displayed in boldface and performance improvement in red text. Appendix A.2 shows the formulation of these methods.,
2208.06646v2,What is the difference between the red and blue regions in the figure?,"The red regions represent areas where traffic information is unobservable, while the blue regions represent areas where traffic information is entirely recorded.",2208.06646v2-Figure9-1.png, Road networks for synthetic data set. 𝐷4×4 has 16 intersections with bi-directional traffic. Green dot lines represent traffic flow direction on each lane. Traffic information in the red regions is unobservable while in the blue region information is entirely recorded.,
2208.06646v2,How do the road networks in Gudang Sub-district and Upper-eastside differ in terms of their layout and traffic flow?,"The road network in Gudang Sub-district appears to be more organic and less grid-like than the road network in Upper-eastside. Additionally, Gudang Sub-district has both uni-directional and bi-directional traffic, while Upper-eastside only has uni-directional traffic.",2208.06646v2-Figure10-1.png," Road networks for real-world datasets. Red polygons are the areas we select to model, blue dots are intersections with traffic signals. (a) 𝐷𝐻𝑍 , 16 intersections with uni& bi-directional traffic. (b) 𝐷𝑁𝑌 , 196 intersections with unidirectional traffic.",
2208.06646v2,How does the inclusion of traffic management actions as input affect the performance of the ASTGCN model?,The inclusion of traffic management actions as input improves the performance of the ASTGCN model.,2208.06646v2-Figure1-1.png, Two perspectives in modeling traffic flow. (a) Existing data-driven models directly predict the next state 𝑠𝑡+1 based on its past states with amodel 𝑃 . (b) Modeling the traffic state transition takes both past states and traffic management actions. (c) The method with traffic management actions as input (ASTGCN [8] w/ action) shows better performance.,
2208.06646v2,Which method performed the best on the D4x4 dataset according to the MAE metric?,WaveNet,2208.06646v2-Table1-1.png," The MAE, RMSE and MAPE of different methods on synthetic data and real-world data. The lower, the better.",
2208.06646v2,Which method performed the best on the DHZ dataset?,DTIGNN,2208.06646v2-Table2-1.png," Performance of DTIGNN against different baselines and their corresponding two-step variants on 𝐷𝐻𝑍 . The lower, the better. Vanilla baselinemethods andDTIGNN learn from sparse observations, while the two-step variants impute sparse observations first and train on the imputed data. DTIGNN achieves the best performance in all cases. Similar trends are also found on other datasets.",
2208.06646v2,What is the predict time slice of DTIGNN?,1,2208.06646v2-Table4-1.png, Hyper-parameter settings for DTIGNN .,
2208.06646v2,Which dataset has the most unobserved intersections?,D_NY has the most unobserved intersections.,2208.06646v2-Table3-1.png, Statistics of datasets.,
2208.06646v2,What is the difference between the static adjacency matrix and the phase-activated adjacency matrix?,"The static adjacency matrix represents the connectivity between road segments based on the road network structure, while the phase-activated adjacency matrix represents the connectivity between road segments based on the current state of the traffic signals.",2208.06646v2-Figure2-1.png," Illustration of traffic signals and their influence on the connectivity between road segments. (a) An intersection with eight connecting road segments and traffic signals. (b) The signals set and their controlled road segments in the example, with right-turn signals illustrated in grey cells. (c) The phase representation for the activated signals in the example intersection.(d) The static adjacency matrix between road segments induced by road network structure. (e) the dynamic adjacency matrix induced by traffic signals. Best viewed in color.",
2208.06646v2,"Which method performs better in terms of queue length when using MaxPressure, baseline or DTIGNN?",DTIGNN performs better than baseline in terms of queue length when using MaxPressure.,2208.06646v2-Figure8-1.png," The performance of the baseline method and DTIGNN under sparse data on 𝐷𝐻𝑍 for two sequential tasks: traffic state modeling w.r.t. RMSE, and traffic signal control w.r.t. queue length using MaxPressure. The x-axis and yaxis indicate the location of the intersections in the road network of 𝐷𝐻𝑍 . The black boxes indicate the intersections without any observation. DTIGNN has lower prediction error in predicting next traffic states. These predictions are then used by MaxPressure [24] to achieve better performance with a lower average queue length. Best viewed in color.",
2208.06646v2,"How does DTIGNN perform compared to the baseline models in terms of MAE, RMSE, and MAPE?","DTIGNN achieves better performance than the baseline models in terms of MAE, RMSE, and MAPE.",2208.06646v2-Figure6-1.png," MAE, RSME and MAPE of the variants of DTIGNN against different baseline models on 𝐷𝐻𝑍 . The lower, the better. The variants of DTIGNN are implemented on corresponding baseline models. DTIGNN achieves the better performance over corresponding baseline models. Similar trends are also found on other datasets.",
2208.06646v2,How does the performance of the DTIGNN model compare to the ASTGNN model under different levels of sparsity on DHZ?,The DTIGNN model consistently outperforms the ASTGNN model under different levels of sparsity on DHZ.,2208.06646v2-Figure7-1.png," MAE, RSME and MAPE of DTIGNN against baseline models under different levels of sparsity on 𝐷𝐻𝑍 . The lower, the better. The x-axis indicates the percentage of sparsity, which is calculated with the number of unobserved intersections divided by total number of intersections in the network (here 𝐷𝐻𝑍 has 16 intersections) in percentage. Our proposed model achieves consistent better performance under different sparsity.",
2208.06646v2,What is the role of the Attention Matrix in the DTIGNN network?,The Attention Matrix is used to learn the importance of different spatial and temporal dependencies between nodes in the graph. This allows the network to focus on the most relevant information for making predictions.,2208.06646v2-Figure3-1.png," Model framework and training pipeline. (a) The framework of DTIGNN network. The network can be built upon exiting spatial-temporal GNNs with our Attention based Spatial-Temporal GNNs Layer appended after existing GNNs. (b) The training pipeline of DTIGNN . One training round goes through data in an auto-regressive way, and the predictions from previous time steps are used as imputations to update the unobserved data at the current time step. ( ¤X𝑡−𝑇+1, ¤X𝑡−𝑇+2, · · · , ¤X𝑡 ) as input, and output an attention matrix 𝐴𝑡𝑡 . Grounded by the transition equations from transportation, the Neural Transition Layer takes the Γ along with dynamic Phaseactivated Adjacency Matrix A and ¤X to predict the state tensor X̂𝑡+1. As is shown in Figure 3(b), there are iterative imputation steps from the time step 𝑡 −𝑇 + 1 toward 𝑡 + 1, where the unobserved part of the predict state tensor ¥̂X𝑡−𝜏+1 from the time step 𝑡 − 𝜏 would be merged with ¤X𝑡−𝜏+1 and used for the prediction for ¤X𝑡−𝜏+2. Then the Output Layer generates a prediction of X𝑡+1.The details and implementation of the models will be described in the following",
2208.06646v2,Which method performs the best on the DHZ dataset?,"The method labeled ""Ours"" performs the best on the DHZ dataset.",2208.06646v2-Table5-1.png," Performance of DTIGNN using different loss functions on 𝐷𝐻𝑍 . The lower, the better. Similar trends are also found on other datasets.",
2208.06646v2,What is the meaning of the red and green circles in the figure?,The red and green circles represent vehicles traveling on the road.,2208.06646v2-Figure4-1.png, The transition of traffic flows for road 𝑞.,
2209.01106v4,Which similarity measure is the most sensitive to small changes in similarity?,The cosine similarity measure.,2209.01106v4-Figure2-1.png," Histograms showing the distributions of the different similarity measures (top: TF-IDF, bottom: embedding based) evaluated on 100 000 sentence pairs.",
2209.01106v4,"Which algorithm variant performed the best in terms of precision, recall, and F1-score?","The MST-LIS algorithm variant performed the best in terms of precision, recall, and F1-score.",2209.01106v4-Figure3-1.png," (top) Precision, recall and F1-score for all algorithm variants evaluated on the ground truth. (bottom) Manual alignment classification accuracy for the manually labelled matches.",
2209.01106v4,How many sentences are there in the German dataset?,"5,942.",2209.01106v4-Table4-1.png," Overview of the number of aligned sentence pairs yielded by our proposed algorithm variant of maximum similarity with MST-LIS matching and a similarity threshold of 1.5, where we count the number of unique sentences in German with n corresponding Simple German sentences.",
2209.01106v4,Which alignment strategy and similarity measure combination yields the highest average similarity?,MST-LIS and maximum.,2209.01106v4-Table5-1.png, Average similarity values and number of matched sentences for all combinations of similarity measure and alignment strategy.,
2209.01106v4,Which website in the table offers information about different social services?,"There are three websites that offer information about different social services: (beb), (lmt) and (soz).",2209.01106v4-Table3-1.png," Overview of websites used for the corpus. One website (apo) offers general health information, three websites (bra), (mdr) and (taz) are news websites, three websites (beb), (lmt) and (soz) offer information about different social services, and (koe) provides administrative information about the city of Cologne. The last column describes the type of Simple German found on that website ‘Einfache Sprache’ (ES) or ‘Leichte Sprache’ (LS).",
2209.01106v4,Which combination of similarity measure and matching algorithm results in the highest number of matched sentences?,"The combination of ""maximum"" similarity measure and ""MST"" matching algorithm results in the highest number of matched sentences (32,499).",2209.01106v4-Table2-1.png, Number of matched sentences for all combinations of similarity measure and matching algorithm.,
2209.01106v4,Which of the following sentences is an example of a partially aligned sentence pair?,"""Diverse öffentliche Verkehrsmittel bieten eine optimale Anbindung an die Hamburger Innenstadt, die Autobahn sowie den Flughafen."" and ""They go all the way to the highway and the airport.""",2209.01106v4-Figure1-1.png," Example sentence pairs aligned between Simple German [LS] and German [AS] and their translations. Examples show successfully, partially and wrongly aligned sentences.",
2209.01106v4,What is the main reason for the increase in the price of waste paper?,The main reason for the increase in the price of waste paper is the increased demand for packaging materials due to the rise in online shopping during the Corona crisis.,2209.01106v4-Figure4-1.png, Simple GUI used to create the ground truth of sentence alignments.,
2209.01106v4,What is the purpose of the GUI shown in the figure?,The GUI is designed to allow users to evaluate the results of sentence matching.,2209.01106v4-Figure5-1.png," GUI for the second manual evaluation. We show different examples of matches, and how they are evaluated.",
2209.01106v4,Which source has the highest average number of tokens per sentence?,bra,2209.01106v4-Table1-1.png," Statistics of the article-aligned corpus. (a) articles and (t) tokens per source; all further values are reported as averages: (s/a) sentences per article, (t/s) tokens per sentence, and (w/a) number of different words per article.",
2209.01106v4,Which matching strategy achieved the highest alignment classification accuracy with the MST-LIS method?,sbert,2209.01106v4-Table7-1.png," Alignment classification accuracy results from the second manual evaluation. All algorithm variants were tested with a threshold of 1.5. Given two sentences, the annotators evaluate whether the sentence in Simple German is a (partial) translation of the German sentence.",
2209.01106v4,Which similarity measure performed the best overall?,"The average precision, recall, and F1-score for each similarity measure are shown in the last row of the table. The bipartite similarity measure has the highest average precision and F1-score, while the 4-gram cosine similarity measure has the highest average recall. Therefore, the bipartite similarity measure performed the best overall.",2209.01106v4-Table6-1.png," Precision, recall, and F1-score results from the first evaluation on the ground truth per website. We compare the results of each similarity measure applied with the MST-LIS matching algorithm and a similarity threshold of 1.5.",
2209.10063v3,Which model performed best on the TriviaQA dataset in the zero-shot open-domain QA setting?,DPR + InstructGPT,2209.10063v3-Table1-1.png," Zero-shot open-domain QA performance. Our proposed GENREAD with the InstructGPT reader (named GENREAD (InstructGPT)) can significantly outperform the original InstructGPT, achieving new state-of-the-art performance on three open-domain QA benchmarks (previous SoTA: 1GLaM (Du et al., 2022), 2FLAN (Wei et al., 2021)) under this setting without using any external document. Our GENREAD can achieve comparable or even better performance than zero-shot retrieve-then-read models that use a retriever or search engine to first obtain contextual documents. To ensure reproducibility, we use greedy search in decoding. All prompts used are shown in the §B.1. Note: fix numbers in v2 by adding average performance of different prompts, see details in Table 20.",
2209.10063v3,"What is the most common answer to the query ""What is the collection of the districts to the east of the Jordan River?""",Jordan,2209.10063v3-Table19-1.png, Case study of retrieved documents and generated documents from language models (4/4).,
2209.10063v3,Which dataset has the largest number of training examples?,TriviaQA,2209.10063v3-Table6-1.png," Datasets splits and statistics. For FEVER and WoW, labels in the test are hidden, so the model performance should be evaluated at https://ai.facebook.com/tools/kilt/.",
2209.10063v3,How does the recall of the proposed clustering-based prompting method compare to DPR-Multi and Google search on the TriviaQA and WebQ test sets?,The proposed clustering-based prompting method has higher recall than DPR-Multi and Google search on both the TriviaQA and WebQ test sets.,2209.10063v3-Figure2-1.png," Recall@K on test sets, measured as the percentage of top-K documents that contain the answer. Our proposed clustering-based prompting method can outperform DPR and Google search, also two variants of using LLMs to generate documents. Exact numbers are reported in Table 6.",
2209.10063v3,"Which type of music did Mozart compose the most, according to the GPT models?",Chamber music.,2209.10063v3-Table16-1.png, Case study of retrieved documents and generated documents from language models (1/4).,
2209.10063v3,What are the different ways in which the image depicts the relationship between Monsanto and agriculture?,"The image shows that Monsanto is a major player in the agricultural industry, both through its production of agricultural chemicals and seeds, and through its ownership of agricultural businesses. The image also shows that Monsanto is a controversial company, with some people believing that it is an ""evil corporation"" that is responsible for producing harmful GMOs.",2209.10063v3-Table17-1.png, Case study of retrieved documents and generated documents from language models (2/4).,
2209.10063v3,Which model performs best on the FEVER dataset?,The model that performs best on the FEVER dataset is FiD.,2209.10063v3-Table3-1.png, Supervised performance on fact checking (FEVER and FM2) and open-domain dialogue system (WoW).,
2209.10063v3,Which model performed the best on the TriviaQA open test?,GENREAD (FiD-xl) (clustering),2209.10063v3-Table2-1.png," Supervised open-domain QA performance. By only using generated documents from InstructGPT, our GENREAD with FiD reader (named GENREAD (FiD)) can achieve better performance than baseline methods on TriviaQA and WebQ. Through our detailed analysis of NQ, we found the performance gap mainly due to the temporality issue, which will be elaborated in §A.7.",
2209.10063v3, What is the purpose of the clustering-based prompting method?, The clustering-based prompting method is used to generate diverse documents that can be used to answer a question. ,2209.10063v3-Figure1-1.png," An overall framework of clustering-based prompting method. It leverages distinct questiondocument pairs sampled from each embedding cluster as in-context demonstrations to prompt a large language model to generate diverse documents, then read the documents to predict an answer.",
2209.10063v3,What are the different types of errors that GENREAD makes?,"The different types of errors that GENREAD makes are: 
- Data collection and annotation errors (12%)
- Model prediction errors (39%)",2209.10063v3-Table12-1.png," Case study on 100 GENREAD predictions in the NQ dataset. Among 100 examples, there are 49 correct predictions, i.e., EM = 49%. We further categorized 51 incorrect predictions of our GENREAD, including errors caused by data collection and annotation, and errors caused by model prediction. In addition, we provide more case studies in Tables 13-15 (Table 13 for the temporal question issue; Table 14 for the incomplete answer issue; Table 15 for the hallucination issue).",
2209.10063v3,"Based on the figure, what is the effect of adding large language model generated documents (LLM) to the DPR retrieved documents?","Adding large language model generated documents (LLM) to the DPR retrieved documents significantly improves the retrieval performance, as measured by Recall@k.",2209.10063v3-Figure5-1.png, Additional retrieval performance evaluation for Figure 3 of experiments on combining DPR retrieved documents and large language model generated documents. Merging documents from two sources achieved significantly better performance than using DPR retrieved documents only.,
2209.10063v3,How does sampling documents from each cluster affect the performance of the model compared to sampling documents from the entire data?,Sampling documents from each cluster generally leads to better performance than sampling documents from the entire data.,2209.10063v3-Table11-1.png, Ablation study on the strategy of sampling documents as in-context demonstrations.,
2209.10063v3,Which model in the table achieves the highest performance on the WebQ open test set?,GENREAD.,2209.10063v3-Table9-1.png," Additional numbers for Table 1. Zero-shot open-domain QA performance, compared to recent large language models. All models in the table do not leverage any external corpus for document retrieval. Compared to InstructGPT, our proposed GENREAD can improve the EM score by +6.9 on average. GENREAD can achieve state-of-the-art performance on open test sets.",
2209.10063v3,Which model performed the best on the TriviaQA dataset according to Recall@20?,"DPR (Karpukhin et al., 2020) performed the best on the TriviaQA dataset according to Recall@20 with a score of 79.0.",2209.10063v3-Table10-1.png," Retrieval performance evaluated by Recall@K of baselines and different GENREAD variants. Some numbers in the table overlaps with those in Figure 2. The table aims to show the performance of more methods, and to provide accurate recall numbers for future research comparisons.",
2209.10063v3,Which prompt achieved the highest validation score?,"Prompt #1: ""Generate a background document from Wikipedia to answer the given question.""",2209.10063v3-Table21-1.png," Top-10 human prompts, evaluated on merged validation set of NQ, TriviaQA and WebQ.",
2209.10063v3,Which model performed the best on the NQ dataset when using a retriever and trained directly on the dataset?,DPR + InstructGPT,2209.10063v3-Table20-1.png, Zero-shot QA performance under different prompts. The prompts are listed in §B.1.,
2209.10063v3,Which prompt resulted in the highest average performance across all three test sets?,Prompt #1 (Generate ...),2209.10063v3-Table22-1.png," Performance on NQ, TriviaQA and WebQ test sets of top-10 human prompts.",
2209.10063v3,Which city is mentioned the most in the retrieved and generated documents as being the city that Zeus was the patron god of?,Olympia,2209.10063v3-Table18-1.png, Case study of retrieved documents and generated documents from language models (3/4).,
2209.10063v3,"Which method obtains the highest answer coverage for NQ, TriviaQA, and WebQ?","DPR (Karpukhin et al., 2020) obtains the highest answer coverage for NQ (67.9%), TriviaQA (76.5%), and WebQ (62.1%).",2209.10063v3-Table4-1.png, Answer coverage (%) over 10 retrieved or generated documents. Case studies are provided in Tables 16-19 in Appendix.,
2209.10063v3,Which document retrieval method achieves the highest accuracy on WebQ?,GENREAD (clustering prompts) achieves the highest accuracy on WebQ with a score of 66.7.,2209.10063v3-Table5-1.png, Readability study on retrieved documents and generated documents. See detailed analysis in §4.4.,
2209.10063v3,What is the effect of combining DPR retrieved documents and LLM generated documents on the EM score?,Combining DPR retrieved documents and LLM generated documents can achieve significantly better performance than using DPR retrieved documents only.,2209.10063v3-Figure3-1.png," Combining DPR retrieved documents and large language model (LLM) generated documents can achieve significantly better performance than using DPR retrieved documents only. For a fair comparison, instead of adding LLM generated documents to the model, we replace 10 documents retrieved by DPR with 10 documents generated by LLM so the total number of documents is the same. In this experiment, we use FiD-l (i.e., FiD-large) as the reader model because when the documents scale to more than 20, FiD-xl (i.e., FiD-3B) causes out-of-memory issues on A100 GPUs.",
2209.10063v3,What is the purpose of the table?,The table shows the results of a study on the temporality issues of the NQ dataset. The study compared the answers generated by different models (DPR and GENREAD) to the correct answers.,2209.10063v3-Table13-1.png," Case studies of temporality issues of the NQ dataset. All these questions are drawn from Zhang & Choi (2021), which contains a subset of NQ data examples with time-dependent questions.",
2209.10063v3,Why is it easier for DPR-FiD to predict correct answers on the NQ dataset than for other models?,"Because the labels in NQ are spans identified from Wikipedia passages, and DPR-FiD is specifically designed to retrieve relevant passages from Wikipedia.",2209.10063v3-Table14-1.png," Case studies of the incomplete answers issue of the NQ dataset. Since the labels in NQ are spans identified from Wikipedia passages, it is easier for DPR-FiD to predict correct answers.",
2209.10063v3,Which model performs better on WebQ when using InstructGPT as a context generator?,DPR-FiD,2209.10063v3-Figure4-1.png, Model performance with different size of InstructGPT as context generators.,
2209.10063v3,Which dataset required the highest total training steps for GENREAD (FiD-l)?,Wow,2209.10063v3-Table7-1.png," Hyperparaters settings and validation performance for open-domain QA (numbers reported in Table 2), fact checking and dialogue system (numbers reported in Table 3). The upper part numbers are from GENREAD (FiD-l) and the lower part numbers are from GENREAD (FiD-xl).",
2209.10063v3,Which open-source large language model achieved the highest EM score on TriviaQA?,Codex,2209.10063v3-Table8-1.png, Exact match (EM) score with using DPR and different open-source large language models such as OPT and Codex to generate contextual documents.,
2210.05078v2,"Which gesture is used to represent the ""Push-Pull"" action?","The ""Push-Pull"" action is represented by the gesture of two hands moving towards and away from each other in a horizontal direction.",2210.05078v2-Figure2-1.png, Four human activities (gestures) used for experiments.,
2210.05078v2,What is the distance between AP 1 and the user?,3.15 meters,2210.05078v2-Figure3-1.png," Detailed floor plan of the data collection setup in an indoor office, which includes location of access points (APs), user, transmitter (Raspberry Pi), and collector (modem).",
2210.05078v2,Which model performs best in terms of accuracy for activity classification?,CMAP,2210.05078v2-Table1-1.png," Classification performance results and standard deviation (in %) over all activity and orientation classes, averaged over 10 independent runs. Acc: Accuracy; BAcc: Balanced accuracy; MCC: Matthews correlation coefficient.",
2210.05078v2,Which activity is the easiest to classify?,"The easiest activity to classify is ""Up-Down"".",2210.05078v2-Table2-1.png," Classification accuracy results and standard deviation (in %) per activity class and orientation class, averaged over 10 independent runs.",
2210.05078v2,What is the difference between the CMAP and AMAP models?,"The CMAP model concatenates the features extracted from each access point, while the AMAP model aggregates them.",2210.05078v2-Figure1-1.png, Proposed feature extraction and joint orientation-activity classification models using a single access point (AP) and multiple APs.,
2210.07670v1,Which method produced the most accurate reconstruction of the Buddha statue?,"The ""Ours"" method produced the most accurate reconstruction of the Buddha statue, as indicated by the F-score of 0.993.",2210.07670v1-Figure3-1.png, Comparison of MVPS reconstructions on DiLiGenT-MV benchmark [29]. We report F-score metric results for numerical comparison. We can observe that our method recovers fine details and provides high-quality reconstructions of challenging objects.,
2210.07670v1,Which method category performs the best on average across all datasets?,View-Synthesis,2210.07670v1-Table1-1.png," F-score comparison of standalone method reconstructions on DiLiGenT-MV benchmark [29]. Our method outperforms standalone multi-view stereo, photometric stereo and view synthesis methods in all of the object categories.",
2210.07670v1,Which method performs best on the COW dataset?,Ours.,2210.07670v1-Table2-1.png, F-score comparison of MVPS reconstructions on DiLiGenT-MV benchmark [29]. Our method performs consistently well on various objects and is better than others on average.,
2210.07670v1,What are the advantages of the proposed method over the UA-MVPS method?,The proposed method is able to correctly recover the fine object’s details for anisotropic and glossy material object.,2210.07670v1-Figure1-1.png," (a) The classical MVPS setup as outlined in Hernández et al. [16] work. (b) The advantage of our method over current state-ofthe-art deep-MVPS method i.e., UA-MVPS [23]. It can be observed that our method is able to correctly recover the fine object’s details for anisotropic and glossy material object. The 3D model used for the above illustration is taken from [33] dataset.",
2210.07670v1,How does the performance of the proposed method compare to that of UA-MVPS on texture-less objects?,The proposed method performs better than UA-MVPS on texture-less objects.,2210.07670v1-Figure4-1.png," (a) Chamfer L2 comparison of our method with UA-MVPS [23] on our synthetic dataset (lower is better). (b) We show depth and surface normal predictions on texture-less object. Pixels marked with yellow color indicate confident MVS or PS predictions (cmvs i and cps i ). Note that MVS cannot predict depth reliably on texture-less surface, which leads to inferior results in UA-MVPS [23]. On the other hand, our uncertainty-aware volume rendering approach can recover missing surface information, and therefore, provides better reconstructions. (c) Comparison of our method with TSDF Fusion algorithm [7]. We report F-score metric for numerical comparison.",
2210.07670v1,Which of the two reconstructions is more accurate according to the surface profile plot?,The reconstruction of the Buddha is more accurate than the reconstruction of the READING figure.,2210.07670v1-Figure5-1.png," Surface profile of our reconstructions on a randomly chosen path. Clearly, our surface profile overlaps with the groundtruth(GT), which indicates the high quality of our reconstructions.",
2210.07670v1,Which setting performs best for the BEAR dataset?,Ours,2210.07670v1-Table3-1.png," Contribution of MVS, PS, rendering loss terms and uncertainty modeling to our reconstruction quality. We report Chamfer L2 metric for comparison (lower is better). Clearly, our proposed loss in Eq:(8) produces best results on average.",
2210.07670v1, What is the role of the Deep Multi-View Stereo Network and the Deep Photometric Stereo Network in the proposed method?," The Deep Multi-View Stereo Network and the Deep Photometric Stereo Network are used to obtain highly confident 3D position and surface normal predictions of the object, respectively.",2210.07670v1-Figure2-1.png," Method overview (Left to Right): We obtain highly confident 3D position and surface normal predictions of the object via uncertainty-aware deep-MVS and deep-PS networks, respectively. Then, we learn the signed distance function representation of the object surface. Finally, our optimization uses the volume rendering technique to recover the missing details of the surface, providing high-quality 3D reconstructions of challenging material types.",
2210.08502v1,Which model has the largest difference between the Hessian and Fisher traces?,Inception-V3,2210.08502v1-Figure1-1.png," Hessian and EF Parameter traces for four classification models. The Hessian and EF traces for the parameters are very similar. For Inception-V3, this holds up to a constant scaling factor.",
2210.08502v1,"Which method is faster, the EF or the Hessian?",The EF is faster than the Hessian.,2210.08502v1-Table1-1.png," Representative examples of the typical speedup associated with using the EF over the Hessian. Iteration times and variances are computed as sample statistics over multiple runs of many iterations, with batch size of 32. The resulting speedup is denoted for a fixed tolerance, which can be practically computed via a moving variation of the mean trace. Early stopping is possible when we first reach the desired tolerance. The measurements were performed on an NVidia 2080Ti GPU.",
2210.08502v1, How does the quantization process affect the distribution of weights in a neural network?," The quantization process reduces the number of possible weight values, resulting in a more discrete and less smooth distribution of weights. This is evident in the comparison between the weight distribution before and after quantization.",2210.08502v1-Figure9-1.png," ResNet-18, block 12, quantization distribution analysis showing the validity of the uniform noise assumption.",
2210.08502v1,Which experiment had the highest rank correlation coefficient for the FIT method on the Cifar-10 dataset?,Experiment A,2210.08502v1-Table2-1.png, Rank correlation coefficient for each combination of sensitivity and quantization experiment. W/A subscript indicates using only either weights or activations. BN indicates the presence of batch-normalisation within the architecture.,
2210.08502v1,Which of the four models has the fastest convergence rate for the Hessian trace?,ResNet-18.,2210.08502v1-Figure2-1.png, Comparison between Hessian and EF trace convergence for four classification models.,
2210.08502v1,Which model has the lowest average iteration time for the Hessian across all batch sizes?,ResNet-18.,2210.08502v1-Table4-1.png," Iteration times associated with the EF and Hessian for a variety of batch sizes, averaged over 3 runs of 200 iterations for each model and batch size.",
2210.08502v1,How does the variance of the Hessian estimator change with increasing batch size for ResNet-18?,The variance of the Hessian estimator decreases with increasing batch size for ResNet-18.,2210.08502v1-Table3-1.png," Estimator Variances associated with the EF and Hessian for a variety of batch sizes, over 3 runs of 200 iterations for each model and batch size.",
2210.08502v1,How many layers are there in the convolutional classifier architecture?,There are four layers in the convolutional classifier architecture.,2210.08502v1-Figure8-1.png, Small convolutional classifier architecture used in the experiments detailed in Section 4,
2210.08502v1,Which model has the most consistent activation trace across all blocks?,ResNet-18,2210.08502v1-Figure7-1.png, EF Activation traces for four classification models.,
2210.08502v1,Which block has the highest activation trace?,Block 17,2210.08502v1-Figure4-1.png, EF weight (a) and activation (b) traces for U-Net architecture on the Cityscapes dataset. (c) FIT against mIoU for 50 random MPQ configurations of U-Net on Cityscapes semantic segmentation,
2210.08502v1,How does the FIT parameter affect the post-QA accuracy in each experiment?,"The FIT parameter has a negative correlation with post-QA accuracy in each experiment. This means that as the FIT parameter increases, the post-QA accuracy decreases.",2210.08502v1-Figure3-1.png, Plots of the chosen predictive heuristic against final model performance.,
2210.08502v1,What is the relationship between the input angle θ and the output angle θ_a?,The output angle θ_a is a quantized version of the input angle θ.,2210.08502v1-Figure6-1.png," Overview QAT - Q(θ) is applied during the forward pass, and an STE is used in the backwards pass.",
2210.08502v1,What is the relationship between noise and parameter magnitude?,Noise and parameter magnitude are positively correlated.,2210.08502v1-Figure5-1.png," (a) Noise vs parameter magnitude. The line indicates equal magnitude, and is shown for reference. (b) FIT against final training accuracy for experiment D. The correlation coefficient, in this case, is 0.98.",
2210.11940v2,What does the figure show?,"The figure shows the output of the JRDB-Pose system, which provides high frequency annotations of tracks and body joints in long scenes of crowded indoor and outdoor locations featuring dynamic motion and occlusion. The system is able to track the movements of multiple people in a scene and identify their body joints, even when they are occluded by other people or objects.",2210.11940v2-Figure1-1.png, JRDB-Pose provides high frequency annotations of tracks and body joints in long scenes of crowded indoor and outdoor locations featuring dynamic motion and occlusion.,
2210.11940v2,Which scene has the highest number of keypoints with all three types of visibility annotations?,The gates-elevators scene.,2210.11940v2-Figure7-1.png, Distribution of keypoint visibility annotations for each JRDB-Pose train and validation scene. Best viewed in color.,
2210.11940v2,What types of environments are shown in the image?,The image shows a variety of indoor and outdoor environments on a university campus.,2210.11940v2-Figure8-1.png," Visualization of JRDB-Pose annotations from each of the 27 training sequences in JRDB-Pose. The images feature indoor and outdoor areas on a university campus with varying lighting conditions, motion, pedestrian density, and activities. Locations include roads, strip malls, sidewalks, restaurants, plazas, parks, halls, classrooms, laboratories, and office buildings. These scenes show a wide range of scenarios and human poses that a social robot would encounter during operations.",
2210.11940v2,Which dataset in the table has the largest number of poses?,MotSynth† has the largest number of poses with 40M.,2210.11940v2-Table1-1.png," Comparison of existing public datasets related to 2D pose estimation and tracking. For each dataset we report the numbers of poses, boxes, as well as the availability of tracking information, crowd data, people per frame (ppF), occlusion labels, action labels, scene type, and if the data comes from robot navigation in human environments. We mark if a dataset has data modalities besides RGB frames, and if it contains annotations for multi-task types. Note that JRDB-Pose is a multi-modal dataset captured from a social navigation robot, addressing different research challenges than many existing works. †Synthetic dataset. unk: Unknown.",
2210.11940v2,Which method has the highest per-keypoint average contribution to the Opose localization error for occluded joints?,CID,2210.11940v2-Table4-1.png," We break down Opose localization error from Tab. 3 for visible (V), occluded (O), and invisible (I) joints. The left and right columns represent the total and average per-keypoint contribution (scaled by 1M) to the Opose localization error, respectively.",
2210.11940v2,Which method has the highest average precision (AP)?,YOLO-Pose has the highest AP with a score of 47.9.,2210.11940v2-Table3-1.png," Multi-person pose estimation baselines evaluated on JRDB-Pose stitched annotations. Loc and Card are the Opose localization and cardinality error, respectively.",
2210.11940v2, What is the relationship between the number of frames and the number of tracks? , The number of tracks decreases as the number of frames increases. ,2210.11940v2-Figure2-1.png," Various statistic for JRDB-Pose, which provides visibility labels and track ids consistent across long periods of occlusion. From left to right: 1) The distribution of track lengths, with the long tail truncated. 2) A log-scaled distribution showing all track lengths. JRDBPose track lengths are varied and as high as 1700 frames long. 3) Number of pose annotations in each panoramic frame. 4) Number of keypoints in each pose annotated as visible.",
2210.11940v2,What is the percentage of visible joints in the JRDB-Pose train set?,70.2%,2210.11940v2-Figure3-1.png," Distribution of keypoint visibility annotations in the JRDB-Pose train/validation and test splits. While most joints are visible, JRDB-Pose contains a large number of occluded and invisible joints.",
2210.11940v2,What are the colors of the bounding boxes and skeletons in the image?,"The bounding boxes are blue, and the skeletons are green, blue, and red.",2210.11940v2-Figure11-1.png, Visualization of Yolo-Pose predictions on 27 testing sequences in JRDB-Pose dataset (cont.),
2210.11940v2,Which pose estimation and tracking method combination performs the best in terms of MOTA?,Yolo-Pose [27] (COCO → JRDB-Pose) and OCTTrack [8],2210.11940v2-Table5-1.png, Multi-person pose tracking baselines evaluated on JRDB-Pose individual camera images.,
2210.11940v2,Which training strategy achieved the best AP on the Stitched dataset?,COCO -> JRDB-Pose,2210.11940v2-Table6-1.png, We study how training protocols affect performance of YoloPose. Finetuning from COCO achieves best results.,
2210.11940v2,What does the pink outline represent in the image?,The pink outline represents the pose history of a person.,2210.11940v2-Figure4-1.png, A portion of the panoramic frame with annotated pose instances. We show some of our tracking annotations by visualizing pose histories for two people. The gaps in the pose history correspond to periods of occlusion denoted by the numbers marking the length of the occlusion in frames. JRDB-Pose provides track IDs which are consistent even across long periods of occlusion.,
2210.11940v2,Which annotations are new in JRDB-Pose?,2D Head Bounding Box and 2D Body Pose.,2210.11940v2-Table2-1.png," A summary of annotations in by JRDB [28], JRDBAct [15], and JRDB-Pose, which together provide a unique multimodal dataset suitable for multi-person detection, pose estimation tracking, and activity recognition. Bolded annotations are introduced in this paper.",
2210.11940v2,Which pose estimation and tracking method combination achieved the highest MOTA score on the JRDB-Pose stitched camera images?,The Yolo-Pose [27] and ByteTrack [55] combination achieved the highest MOTA score of 63.02.,2210.11940v2-Table7-1.png, Multi-person pose tracking baselines evaluated on JRDB-Pose stitched camera images.,
2210.11940v2, What are the different types of poses that are shown in the image?," The image shows a variety of poses, including walking, standing, sitting, and biking.",2210.11940v2-Figure9-1.png, Visualization of JRDB-Pose annotations on the 27 training sequences (cont.),
2210.11940v2,What does the image show?,The image shows the results of Yolo-Pose pose estimation predictions on 27 testing sequences in JRDB-Pose.,2210.11940v2-Figure10-1.png," Visualization of Yolo-Pose pose estimation predictions on 27 testing sequences in JRDB-Pose. The weights of the model are initialized from scratch, demonstrating that JRDB-Pose is sufficiently large for the model to learn accurate pose representations even without finetuning.",
2210.11940v2,Which scene has the smallest range of bounding box scales?,gates-meeting_0,2210.11940v2-Figure5-1.png, Distribution of bounding box scales in the train and validation scenes. JRDB-Pose contains a wide distribution of pose scale that is different for each scene.,
2210.11940v2,How does the distribution of bounding box areas differ between the training and test sets?,"The distribution of bounding box areas is similar between the training and test sets. Both sets have a wide distribution of pose scales, with a peak around 10,000 and a tail that extends to 40,000.",2210.11940v2-Figure6-1.png, Bounding box distribution of the train/validation and test splits. JRDB-Pose contains a wide distribution of pose scales.,
2210.14019v3,What happens to the K-NN probing accuracy of the projector layers when the training data is labeled with random labels?,The K-NN probing accuracy of the projector layers increases sharply when the training data is labeled with random labels.,2210.14019v3-Figure4-1.png," K-NN probing of a ResNet18 on CIFAR10, based on (flattened) representation of different layers. Dashed blue lines indicate K-NN accuracies fitted on the clean training data and evaluated w.r.t. clean test data. Solid lines indicate K-NN training accuracies based on the random labels. Notice the sharp phase transition when reaching the projector layers“P” and “O”, i.e. the projector reaches strong performance on the noisy labels (dashed) but randomly guesses on the clean data (solid). The opposite happens for the embedding layer, reaching 76% K-NN accuracy.",
2210.14019v3,Which model and dataset combination achieved the highest accuracy under the CLEAN + DA setting?,ResNet18 and CIFAR10.,2210.14019v3-Table4-1.png, Linear probing accuracies (in percentage) of the embeddings for various datasets under different settings. DA refers to training under data augmentation. INIT refers to the performance at initialization.,
2210.14019v3,What is the relationship between the number of samples and the number of augmentations?,"The number of samples and the number of augmentations are inversely proportional. As the number of augmentations increases, the number of samples required to achieve a given level of accuracy decreases.",2210.14019v3-Figure14-1.png," K-NN probing for the downstream class assignment on unseen test points for the toy example, after training on random labels.",
2210.14019v3,What is the effect of adding noise to an image on the signal between the original image and its augmentation?,Adding noise to an image can decrease the signal between the original image and its augmentation.,2210.14019v3-Figure5-1.png," Illustration of the thought experiment in Sec. 5. The true cat x1 labeled as “giraffe” is augmented, leading to a positive signal between the augmentations but to more noise due to x2, that has as true label “dog”.",
2210.14019v3,Which dataset has the most examples in the training split?,TinyImageNet 3,2210.14019v3-Table5-1.png, Statistics for the datasets used.,
2210.14019v3,What is the probability of a horizontal flip being applied to an image during training?,0.5,2210.14019v3-Table6-1.png, Hyperparameters for the random labels experiments.,
2210.14019v3,What is the role of the projector in the encoder-projector pair?,The projector takes the embedding from the encoder and projects it to a lower-dimensional space.,2210.14019v3-Figure1-1.png, (Left) Encoder-projector pair. (Right) Standard data augmentations.,
2210.14019v3,What is the relationship between label noise and k-NN accuracy?,"As label noise increases, k-NN accuracy decreases.",2210.14019v3-Figure11-1.png, Nearest-neighbour probing during training for the CIFAR10 and TinyImageNet datasets during training.,
2210.14019v3,What is the effect of data augmentation on the performance of a ResNet18 trained with label noise?,Data augmentation improves the performance of a ResNet18 trained with label noise.,2210.14019v3-Figure10-1.png, Nearest-neighbour probing accuracy for transfer learning to different datasets as a function of label noise. A ResNet18 is trained based on random labels on CIFAR10 with (right) and without (left) data augmentation.,
2210.14019v3,Does the type of augmentation affect the training loss of the ResNet18 model?,"Yes, the type of augmentation does affect the training loss of the ResNet18 model. The figure shows that the training loss for label-preserving augmentations (blue line) is lower than the training loss for non-label preserving augmentations (orange line). This suggests that label-preserving augmentations are more effective in reducing the training loss of the model.",2210.14019v3-Figure6-1.png, Illustration of nearest-neighbour probing accuracy (left) and (running) training loss (right) of a ResNet18 on CIFAR10 with increasing number of standard augmentations. We consider both label-preserving (blue) and completely random labeling of augmentations (red).,
2210.14019v3,How does the training loss of a ResNet18 model trained on clean labels compare to the training loss of a ResNet18 model trained on random labels?,The training loss of the model trained on clean labels is slightly lower than the training loss of the model trained on random labels.,2210.14019v3-Figure3-1.png, Fitting random labels on unaugmented data on CIFAR10 with a ResNet18 is not significantly slower than fitting clean labels.,
2210.14019v3,How does the strength of data augmentation affect the accuracy of a nearest-neighbor classifier trained on random labels?,The accuracy of the nearest-neighbor classifier generally increases as the strength of data augmentation increases.,2210.14019v3-Figure12-1.png, Nearest-neighbour probing accuracy when trained on random labels with varying strength of (infinite/online) augmentations.,
2210.14019v3,Does training with augmentations help improve the accuracy of the model when there is label noise?,"Yes, training with augmentations helps improve the accuracy of the model when there is label noise.",2210.14019v3-Figure13-1.png, Nearest-neighbour probing accuracy when trained on varying levels of label noise with and without augmentations.,
2210.14019v3,What is the relationship between the size of the projector and the K-NN accuracy?,The K-NN accuracy increases as the size of the projector increases.,2210.14019v3-Figure8-1.png, Nearest-neighbour probing accuracy as function of the size of the projector.,
2210.14019v3,What is the relationship between the number of epochs and the K-NN probing accuracy?,The K-NN probing accuracy generally increases with the number of epochs.,2210.14019v3-Figure9-1.png, Nearest-neighbour probing accuracy as a function of the number of classes.,
2210.14019v3,Which method results in the most invariant representations?,Random initialization.,2210.14019v3-Figure7-1.png," Averaged normalized invariance of a ResNet18 on CIFAR10, as a function of epochs, lower value means more invariant. The dashed line representsK-NN probing error of the embedding layer.",
2210.14019v3,Which model performs the best on the CIFAR10 dataset with data augmentation?,ResNet18,2210.14019v3-Table2-1.png," K-NN probing accuracies (in percentage) of the embeddings for various datasets under different settings. DA refers to training under standard data augmentation. INIT refers to the performance at initialization. Except for INIT, all models reach perfect (unaugmented) training accuracy.",
2211.14676v2,What is the difference between the first and second steps of the positional Voter process?,"In the first step, the red node with value 1+δ influences the green node with value 1. In the second step, the green node with value 1 influences the red node with value 1.",2211.14676v2-Figure1-1.png," Two steps of the positional Voter process. Red (resp., green) nodes carry the mutant trait A (resp., resident trait B); orange circles mark biased nodes.",
2211.14676v2,How does the optimal placement of biased nodes on a cycle graph change as the bias (δ) increases?,"For small values of δ, the optimal biased nodes are consecutive. For large values of δ, the optimal biased nodes are spaced apart.",2211.14676v2-Figure2-1.png," Optimal biased node sets S (in orange) on a cycle graph for different values of bias δ and budget k; for small δ (top), the optimal biased nodes are consecutive, yet for large δ (bottom), they are spaced apart. For intermediate values (middle), the optimal strategy varies depending on k.",
2211.14676v2,Which set of nodes has the highest fixation probability for a small invasion bias?,"The set {2, 5} has the highest fixation probability for a small invasion bias.",2211.14676v2-Figure3-1.png," Fixation probabilities for different biased sets of size k = 2; when δ ∈ [0, 0.1] the fixation probability is roughly linearly dependent on δ; the set {2, 6} is worst for small δ but best for large δ.",
2211.14676v2,Which centrality measure is the most consistent across different values of k?,Closeness centrality.,2211.14676v2-Figure5-1.png, Performance under strong bias (δ →∞),
2211.14676v2,Which centrality measure has the highest performance when k = 10%?,Vertex Cover,2211.14676v2-Figure6-1.png, Performance under weak bias (δ → 0),
2211.14676v2,What does the figure show?,"The figure shows three different graphs, each with a different number of nodes. The leftmost graph has four nodes, the middle graph has four nodes, and the rightmost graph has nine nodes.",2211.14676v2-Figure4-1.png, Counterexamples to submodularity.,
2211.16246v1,Which method achieves the lowest estimated error for the 20k sample size?,CIV.VAE,2211.16246v1-Table1-1.png, The table summarises the estimated errors εACE (Mean±STD) over 30 synthetic datasets in each sample size. The lowest estimated errors are marked in boldface. Note that CIV.VAE relies on the least domain knowledge among all estimators and obtain the smallest εACE among all methods compared.,
2211.16246v1,"In the causal DAG, what is the relationship between the variables $S$ and $W_s$?",$S$ and $W_s$ are indistinguishable in the data.,2211.16246v1-Figure1-1.png," An exemplar causal DAG shows the indistinguish ability between a CIV and its conditioning set in the data with a latent confounder UC . T and Y are treatment and outcome variables. S and WS are a CIV and its conditioning set, and they are indistinguishable in the data since both are dependent on T and Y and conditional dependent on Y given T . Dashed edges indicate thatUC cannot be measured. As from the DAG, we know that in the dataset generated, both S and WS are associated with T , given any other observed variable(s). Moreover, while S is associated with Y given any other observed variable(s), WS is also associated with Y given any other observed variable(s) due to the unmeasured confounder UC .",
2211.16246v1,Which estimator has the highest estimated causal effect on Schoolingreturns within the 95% confidence interval?,TSLS,2211.16246v1-Table2-1.png, The estimated causal effects of all estimators on the three real-world datasets. We highlight the estimated causal effects within the 95% confidence interval on Schoolingreturns and 401(k). ‘-’ is used for the corresponding IV-based estimator on Sachs because there is not a known IV. Note that DeepIV cannot work on 401(k) and is also marked as ‘-’.,
2211.16246v1,Which variable is a collider?,Variable X is a collider.,2211.16246v1-Figure4-1.png," The true causal DAG with a latent confounder U between T and Y is used to generate the simulated datasets. UC = {U}, U′ = {U1, U2, U3, U4} are four latent confounders, and X = {S,X1, X2, X3, X4, X5} are pretreatment variables. Note that S is a CIV conditioning on {X1, X2}.",
2211.16246v1,What is the difference in the parameter settings between the simulation and the real-world datasets?,There is no difference in the parameter settings between the simulation and the real-world datasets.,2211.16246v1-Table3-1.png, Details of the parameter settings in CIV.VAE method for both simulated data and real-world datasets.,
2211.16246v1,What is the role of the latent confounders in the causal graph?,"The latent confounders (U_c) represent unobserved variables that influence both the treatment (T) and the outcome (Y), potentially creating a spurious association between them.",2211.16246v1-Figure2-1.png," A causal graph representing the proposed causal representation learning scheme for discovering CIVs and their conditioning sets. T , Y , X, and UC are the treatment, the outcome, the set of measured pretreatment variables, and latent confounders between T and Y , respectively. The two grey circles denote the representation ZT containing the CIV information in X and ZC holding the information of the conditioning set of ZT generated given X.",
2211.16246v1,What is the role of the auxiliary predictors in the inference network?,"The auxiliary predictors are used to predict the treatment assignment T and the outcome Y, given the latent representations ZT and ZC.",2211.16246v1-Figure3-1.png," The proposed CIV.VAE architecture which consists of the inference network and the generative network for learning the latent representations of CIV ZT and its conditioning set ZC. A grey box denotes the drawing of samples from the respective distribution, a white box indicates the parameterised deterministic neural network transitions, and a circle indicates switching paths according to the value of T . In the inference network, the dashed arrows indicate the two auxiliary predictors q(T |ZT ,ZC) and q(Y |T,ZC).",
2212.06331v2,What is the difference between a poor global registration and a perfect global registration?,"In a poor global registration, the different versions of the anchor frames (red puzzle pieces) do not line up perfectly with each other when transformed from different poses. This is indicated by the two-way arrows in the figure. In a perfect global registration, all the corresponding points of the different versions of the anchor frames line up perfectly with each other.",2212.06331v2-Figure5-1.png," Illustration of consistency loss. Red puzzle represents different versions of anchor frames and blue puzzle represents the neighbor frames. The loss measures the euclidean distance of all the corresponding points of different versions of anchor frames that transformed from different poses, as indicated by the twoway arrows. Red points in the figure show one of the points for illustration.",
2212.06331v2,How does the mapping result of KO+DeepMapping2 compare to that of KO?,The mapping result of KO+DeepMapping2 is more complete and accurate than that of KO.,2212.06331v2-FigureVI-1.png,Figure VI. Mapping result on NeBula. The color of point indicates the frame index in the trajectory.,
2212.06331v2,What is the effect of adding DM2 to LeGO-LOAM?,Adding DM2 to LeGO-LOAM improves the mapping quality.,2212.06331v2-Figure6-1.png, Mapping results on NCLT dataset. The red boxes indicate the places where our method has a better mapping quality than LeGO-LOAM. The ATEs are also listed. Other labels and legends follow Fig. 1. Best viewed in color.,
2212.06331v2,Which method performed the best on Drive_0018 according to T-ATE?,LeGO-LOAM+DM2 performed the best on Drive_0018 according to T-ATE with a score of 1.63m.,2212.06331v2-Table1-1.png, Quantitative result on the KITTI dataset. The quality of the predicted trajectory is measured by absolute trajectory error (ATE) compared with the GPS trajectory. Results on two trajectories are listed. “T-ATE” means translation ATE (m) “R-ATE” means rotation ATE (deg). The last two rows are our results.,
2212.06331v2,Which method performs the best according to the T-ATE metric?,LeGO-LOAM+DM2,2212.06331v2-Table2-1.png, Quantitative results on the NCLT dataset. The notation is the same as in Tab. 1. The last two rows are our results.,
2212.06331v2,"Which method, LeGO-LOAM or LeGO-LOAM + DM2, performs better in the region highlighted by the red boxes?",LeGO-LOAM + DM2 performs better in the region highlighted by the red boxes.,2212.06331v2-FigureIV-1.png,Figure IV. NCLT. Heat map visualization. The red boxes highlights the regions where DM2 improves over LeGO-LOAM.,
2212.06331v2,"What is the difference between the mapping results in (a), (b), (c), and (d)?","The mapping results in (a), (b), and (c) are based on trajectories with 1024 frames, while the mapping result in (d) is based on a trajectory with 2048 frames.",2212.06331v2-FigureV-1.png,Figure V. Mapping and trajectory plot on Simulated point cloud dataset [1] We include five mapping results including (a)(b)(c) DeepMapping2 mapping results on three different trajectories with 1024 frames (d) DeepMapping2 mapping results on a trajectory with 2048 frames.,
2212.06331v2,Which method has the highest translation error?,DGR,2212.06331v2-FigureII-1.png,Figure II. KITTI drive 0018. Heat map visualization of both translation and rotation ATE for each frame in the dataset. Note that the color bar is clipped for better visualization. Best viewed in color.,
2212.06331v2,Which method has the least amount of translation error?,DeepMapping2 has the least amount of translation error.,2212.06331v2-FigureIII-1.png,Figure III. KITTI drive 0027. Heat map visualization.,
2212.06331v2,How does the addition of batch organization and consistency loss affect the translation ATE of the DM method?,The addition of batch organization and consistency loss to the DM method decreases the translation ATE.,2212.06331v2-Figure8-1.png, ATE versus training epoch on drive 0027 in KITTI. The legend indicates the ATEs with different components in Tab. 3.,
2212.06331v2,Which components of the DeepMapping2 pipeline are most important for achieving good T-ATE and R-ATE results?,The DM loss and batch organization components are most important for achieving good T-ATE and R-ATE results.,2212.06331v2-Table3-1.png, Ablation study on the KITTI dataset. The experiment is done by combining different components in the pipeline of DeepMapping2. All the results reported are from the 50th epoch of the training. The method without DeepMapping loss fails to converge so its result is not reported.,
2212.06331v2,How does the mapping result of kinematic odometry + DM2 compare to the survey-grade map?,"The mapping result of kinematic odometry + DM2 is very similar to the survey-grade map. The overall shape of the trajectory is very accurate, and the details of the map are also well captured.",2212.06331v2-Figure7-1.png," Mapping result on NeBula dataset. Several parts of the map are zoomed in for a clearer demonstration. As far as we know, we are the first to show this quality of mapping result of the trajectory in the Nebula dataset. Other labels and legends follow Fig. 1.",
2212.06331v2,What are the two main components of the DeepMapping2 pipeline?,The two main components of the DeepMapping2 pipeline are place-recognition-based batch organization and learning-based optimization.,2212.06331v2-Figure2-1.png," Pipeline of DeepMapping2. The pipeline mainly consists of place-recognition-based batch organization and learning-based optimization. In batch organization, the input point clouds are organized into mini-batches by topological map attained from place recognition. Each batch contains an anchor frame A and several spatially closed neighbor frames N . The transformation between the anchor frame and each neighbor frame TA N is obtained by pairwise registration. In optimization, each batch is fed into L-Net to estimate the global pose. The transformed global anchor frame is then obtained in two ways: A′ directly from the global pose of the anchor frame (TG A ) and A′′ from the global pose of the neighbor frame (TG N ) and the pairwise registration (TA N ). The consistency loss penalizes the differences between A′ and A′′. The global frames are also fed into M-Net for computing DeepMapping loss. Best viewed in color.",
2212.06331v2,Where can the detailed results of the additional experiments on the KITTI dataset be found?,The detailed results of the additional experiments on the KITTI dataset can be found in Table III.,2212.06331v2-TableIII-1.png,Table III. Additional results on the KITTI dataset.,
2212.06331v2,"Which place recognition algorithm resulted in the most accurate mapping results for Drive_0018, based on the T-ATE metric?",GPS (ground truth),2212.06331v2-TableI-1.png,Table I. Robustness on map topology. The table lists the mapping result of DeepMapping2 when running with the map topology attained by different place recognition methods. GPS theoretically provides the most ideal map topology.,
2212.06331v2,What is the relationship between the black line and the colored blocks in the figure?,"The black line represents the trajectory of the robot, while the colored blocks represent the occupancy grid.",2212.06331v2-FigureI-1.png,"Figure I. Original DeepMapping results on simulated point cloud dataset. The black line represents the trajectory, while the color block represents the occupancy grid.",
2212.06331v2,Which method is the fastest for Drive_0018 and Drive_0027?,Multiway is the fastest method for both Drive_0018 and Drive_0027.,2212.06331v2-TableII-1.png,Table II. Computation time of different methods. The three baseline methods are run on CPU. DM2 is run on RTX3090 GPU. Note that there is no NVLink when 2 GPUs are used.,
2212.09662v2,Which model performed best on the ChartQA benchmark?,MATChA performed best on the ChartQA benchmark.,2212.09662v2-Table3-1.png, Main experimental results on two chart QA benchmarks ChartQA & PlotQA and a chart summarization benchmark Chart-to-Text. Detailed introduction of the baselines can be found in Appx. §B.,
2212.09662v2,What are the two types of pretraining tasks that MATCHA defines?,Chart derendering and mathematical reasoning.,2212.09662v2-Figure1-1.png," MATCHA defines two types of pretraining tasks: (1) chart derendering (light blue boxes) and (2) mathematical reasoning (light red boxes). In chart derendering, given a chart, the model needs to decode its underlying rendering code or data table. In math reasoning, given a math question rendered as an image, the model needs to decode its answer. Chart derendering teaches the models layout understanding (including number extraction and their organizations) and math reasoning teaches the models numerical reasoning capabilities.",
2212.09662v2,"Which model performs better on Pix2Sturct tasks, MATCHA or Pix2Sturct?",MATCHA performs better on Pix2Sturct tasks than Pix2Sturct.,2212.09662v2-Table4-1.png, MATCHA vs. Pix2Sturct on Pix2Sturct tasks.,
2212.09662v2,What is the effect of removing different components of the MATCHA pretraining process on the ChartQA task?,"Removing different components of the MATCHA pretraining process leads to a decrease in performance on the ChartQA task. The largest decrease in performance occurs when the chart-to-code component is removed, followed by the real-world chart-table pairs, DROP dataset, and MATH dataset.",2212.09662v2-Table5-1.png, MATCHA pretraining ablations on ChartQA.,
2212.09662v2,Which model performed the best on the data extraction challenge?,"PaLI performed the best on the data extraction challenge, achieving an accuracy of 51.9%.",2212.09662v2-Figure2-1.png, Fine-grained by-category performance comparison on ChartQA.,
2212.09662v2,"What percentage of respondents in Oct '17 thought there were ""just the right number"" of police officers?",30%,2212.09662v2-Table7-1.png, An example that requires resolving both coreference resolution and math reasoning.,
2212.09662v2,Which region had the highest palm oil yield in 1961?,Oceania,2212.09662v2-Table8-1.png, An error made by all models including MATCHA which requires very accurate numerical computation. The answer should be ‘No’ since 6.67+5.8+5.63=18.1<18.18.,
2212.09662v2,Which country has the highest percentage of millennials who believe that success in life is determined by forces outside of our control?,Germany and Italy,2212.09662v2-Table6-1.png, An example that requires strong numerical reasoning skills. Red and green indicate correct and wrong answers respectively.,
2212.09662v2,Which task/dataset is used for math reasoning?,DROP,2212.09662v2-Table1-1.png, Mixture rates for all tasks in pretraining and the absolute size of each dataset. The mixture rate is used to sample each example within the batch.,
2212.09662v2,Which dataset has the most number of pairs?,PlotQA (v2),2212.09662v2-Table2-1.png, Statistics of the finetuning datasets.,
2301.04644v1,Which dataset has the highest correlation with the average of the other five datasets?,CCT-20,2301.04644v1-Table13-1.png," We measure the Spearman correlation between each dataset with either the average of the 5 other datasets we study, or with ImageNet. Normalization is done by logit transforming accuracies, and then standardizing to zero mean and unit variance. The results suggest that using additional datasets is more predictive of model performance than just using ImageNet.",
2301.04644v1,"Which model explains more variance in the data, a model that includes both ImageNet accuracy and the average accuracy across the 5 other datasets, or a model that includes only ImageNet accuracy?",A model that includes both ImageNet accuracy and the average accuracy across the 5 other datasets explains more variance in the data than a model that includes only ImageNet accuracy.,2301.04644v1-Table12-1.png," Results of the F-test described in Section N.1. “+Avg. across datasets” tests whether a model that includes both ImageNet accuracy and the average accuracy across the 5 other datasets explains more variance than a model that includes only ImageNet accuracy. “+ImageNet” tests whether a model that includes both predictors explains more variance than a model that includes only the average accuracy across the 5 other datasets. In addition to F and p values, we report adjusted R2 for all models. p-values < 0.05 are bold-faced.",
2301.04644v1,Which model used the most augmentation techniques during pre-training?,ShuffleNet V2x1.0 and ShuffleNet V2x0.5,2301.04644v1-Table14-1.png," For each ImageNet pre-trained model, we provide the augmentation strategy used during pre-training time.",
2301.04644v1,Does the use of simple or automatic augmentation strategies have a significant impact on the performance of different model architectures across all datasets?,"No, the impact of augmentation strategy on model performance varies across different datasets and architectures.",2301.04644v1-Figure10-1.png," Figure 1 with points colored by general pre-training augmentation strategy. Cyan points use simple augmentation (resize, crops, flips, etc.), and red points use automatic augmentation (RandAugment, AutoAugment, TrivialAugmentWide).",
2301.04644v1,Which model and augmentation strategy combination resulted in the highest accuracy on the APTOS dataset?,DeiT-small with RandAug.,2301.04644v1-Table7-1.png," We examine the effect of pre-training augmentation and fine-tuning augmentation on downstream transfer performance. The model specifies the architecture and pre-training augmentation, while each column specifies the downstream task and fine-tuning augmentation. We find that augmentation strategies that improve ImageNet accuracy do not always improve accuracy on downstream tasks. Pre-trained augmentation models are from Wightman et al. (2021).",
2301.04644v1,Does increasing the amount of data augmentation improve the Kaggle leaderboard scores?,Yes.,2301.04644v1-Table6-1.png, Comparing the effect of augmentation on Kaggle leaderboard scores. More augmentation is as described earlier in this section. Less augmentation only uses random resize crop with at least 0.65 area and horizontal flips.,
2301.04644v1,"Based on the figure, which metric is a better indicator of transfer performance from ImageNet to Melanoma?",ROC is a better metric for this task.,2301.04644v1-Figure7-1.png," Comparing transfer performance from ImageNet to Melanoma when using different metrics. Green linear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. Using accuracy implies that better ImageNet models transfer better; however, ROC is a better metric for this task.",
2301.04644v1,Which CLIP pre-trained model performs the best on the ImageNet top-1 dataset?,CLIP-L14@336,2301.04644v1-Table8-1.png," For each CLIP pre-trained model, we provide the best performing model when fine-tuned on each dataset across our LP-FT hyperparameter grid",
2301.04644v1,"Which model performs better on the EuroSAT dataset, CLIP-RN50 or IN-ViT-B/16?",CLIP-RN50,2301.04644v1-Table9-1.png," We directly compare models pre-trained on ImageNet with models pre-trained on OpenAI’s CLIP data. Specifically, we look at ResNet 50 and ViT B/16.",
2301.04644v1,Do CLIP models generally perform better than ImageNet models?,It depends on which metric is used to evaluate performance.,2301.04644v1-Figure8-1.png," Figure 4 with CLIP models overlaid (purple stars). The best CLIP models do better than all the ImageNet models, but when looking across all CLIP models, the patterns are more complicated.",
2301.04644v1,Which model performs best on the Caltech Camera Traps 20 dataset?,The model that performs best on the Caltech Camera Traps 20 dataset is ResNet-152.,2301.04644v1-Figure4-1.png," Figure 1 with error bars. Green is linear trend of all models, while blue is linear trend for models above 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.",
2301.04644v1,"Which models perform better on the Caltech Camera Traps 20 dataset, smaller models or larger models?",Smaller models tend to perform better on the Caltech Camera Traps 20 dataset.,2301.04644v1-Figure5-1.png, Figure 4 with spline interpolation fits instead of linear fits.,
2301.04644v1,What is the most common source of data for the 12 datasets studied in Kornblith et al. (2019)?,Web scraping.,2301.04644v1-Table10-1.png, We find that the 12 datasets studied in Kornblith et al. (2019) come from web scraping.,
2301.04644v1,Which model architecture has the highest accuracy on the Caltech Camera Traps 20 dataset?,ResNet-152,2301.04644v1-Figure9-1.png," We compare model size with downstream transfer performance. Again we use separate trend lines for all models (green) and only those above 70% ImageNet accuracy (blue). We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.",
2301.04644v1,Which model has the highest ImageNet top-1 accuracy?,EfficientNet B4 has the highest ImageNet top-1 accuracy of 83.4%.,2301.04644v1-Table4-1.png," We examine the effectiveness of transfer learning from a number of models pretrained on ImageNet, including both CNNs and Vision Transformers.",
2301.04644v1,Which model performs best on the Cassava dataset?,EfficientNet B0,2301.04644v1-Table3-1.png," For each ImageNet pre-trained model, we provide the best performing model when fine-tuned on each dataset across our hyperparameter grid",
2301.04644v1,Which dataset has the highest FID score?,APTOS.,2301.04644v1-Table11-1.png," We calculate FID scores between the ImageNet validation set and each of the datasets we study, as well as between the ImageNet validation set and each of the datasets in Kornblith et al. (2019). We found that dataset size affects FID score, so we take a 3,662 subset of each downstream dataset. Note that 3,662 is the size of APTOS, which is the smallest dataset.",
2301.04644v1,What is the relationship between the FID scores and the source of the datasets?,The web-scraped datasets tend to have lower FID scores than the datasets used in this study.,2301.04644v1-Figure3-1.png," FID scores vs ImageNet for the datasets we study in this work (red), and the web-scraped datasets studied by Kornblith et al. (2019) (blue).",
2301.04644v1,Which model and learning rate/weight decay combination resulted in the highest accuracy on the Kaggle leaderboard?,Inception-Resnet-v2 with a learning rate of 1.00E-04 and weight decay of 1.00E-05 resulted in the highest accuracy on the Kaggle leaderboard with an accuracy of 0.8960.,2301.04644v1-Table5-1.png, Comparing various models with additional interventions by evaluating on the Kaggle leaderboard.,
2301.04644v1,Which model performs best on CIFAR-10?,ConvNeXt-tiny,2301.04644v1-Figure6-1.png," Transfer performance across models from ImageNet to CIFAR-10. Green linear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson.",
2301.04644v1,Which dataset has the most classes?,Human Protein Atlas.,2301.04644v1-Table1-1.png, We examine a variety of real-world datasets that cover different types of tasks.,
2301.04644v1,Which dataset contains images that could be used to develop a model to diagnose eye diseases?,APTOS 2019,2301.04644v1-Figure2-1.png, Sample images from each of the datasets.,
2302.00378v2,Which fine-tuning strategy achieved the best average performance across all tasks?,"The Full-FT strategy achieved the best average performance across all tasks, with an average score of 82.8.",2302.00378v2-Table1-1.png," The performance of BERT on the GLUE benchmark with different fine-tuning strategies. We report Matthew’s correlation for CoLA, F1 score for MRPC and QQP, Spearman’s correlation for STS-B, and accuracy for the rest. LayerNormsA (LayerNormsF ) stands for the scenario in which only LayerNorms of Attention (Feedforward) modules are set to be trainable. The best and the second-best results are highlighted for each task. † Results from Ben Zaken et al. (2022).",
2302.00378v2,Which fine-tuning strategy achieves the best performance on the CoNLL-2003 dataset according to the table?,Full-FT,2302.00378v2-Table2-1.png, The performance of BERT on Penn Treebank (PTB) and CoNLL-2003 (CoNLL) datasets with five different fine-tuning strategies. We report accuracy for PTB and F1 score (macro/micro) for CoNLL.,
2302.00378v2,Which layer of BERT has the highest transferability for the MRPC task?,Layer #4,2302.00378v2-Table3-1.png, The performance of layer-wise fine-tuning of LayerNorms on the selected downstream tasks for BERT. The LayerNorms in the middle layers tend to have the highest transferability.,
2302.00378v2,Which modules of BERT have the most consistent weight distributions?,The Feed-Forward and Multi-Head modules have the most consistent weight distributions.,2302.00378v2-Figure1-1.png," The empirical distribution of a fixed random subset of weights in different modules of BERT. For better visualization, we have discarded the outliers. The weights of LayerNorms appear to have a bimodal distribution with significantly higher overall averages and standard deviations.",
2302.00378v2,Which tuning method consistently outperforms the other across all tasks and sample sizes?,Outlier tuning consistently outperforms random selection across all tasks and sample sizes.,2302.00378v2-Table4-1.png," The performance of the fine-tuned BERT with n trainable parameters in every LayerNorm module on four different target tasks. Selecting the n parameters from the outliers leads to better performance in most cases, compared to the random selection. For n = 256, the results of outlier tuning are comparable with the Full-FT scenario.",
2302.00993v2,What is the difference between the two graphs shown in the figure?,"The two graphs differ in whether they have a shared latent parent. In graph (a), there is a shared latent parent, while in graph (b), there is not.",2302.00993v2-Figure5-1.png," Compact versions of two 2-domain graphs. In both graphs, both domains have two latent parents. In setup (a) there is a shared latent parent while in setup (b) there is not.",
2302.00993v2,How does the average value of l* change with sample size for 2 domains and 3 domains?,"For both 2 and 3 domains, the average value of l* initially increases with increasing sample size, then plateaus at a higher sample size.",2302.00993v2-Figure3-1.png, Results. Logarithmic scale on the x-axis. Error bars in (a) are one standard deviation of the mean and in (b) and (c) they are the interquartile range.,
2302.00993v2,What does the figure represent?,"The figure represents a latent causal representation where multiple domains provide different ""views"" on subsets of the latent variables.",2302.00993v2-Figure1-1.png," Setup. A latent causal representation where multiple domains Xe provide different “views” on subsets of the latent variables Zi. The domains may correspond to different data modalities such as images, text or numerical data. Crucially, the observations across domains are unpaired, i.e., they arise from different states of the latent causal model.",
2302.00993v2,"What is the effect of sample size on the median score for the ""Same errors"" condition?","The median score for the ""Same errors"" condition decreases as the sample size increases.",2302.00993v2-Figure8-1.png, Simulation results where assumptions are not satisfied. Logarithmic scale on the x-axis. Error bars in (a) are one standard deviation of the mean and in (b) and (c) they are the interquartile range.,
2302.00993v2,What is the difference between the latent nodes and the observed nodes in the figure?,"The latent nodes are the nodes that are not directly observed, while the observed nodes are the nodes that are directly observed. In the figure, the latent nodes are represented by the circles with numbers, while the observed nodes are represented by the circles with the letter ""v"" and a superscript.",2302.00993v2-Figure4-1.png," A 2-domain graph with 5 latent nodes and dimensions of the observed domains given by |V1| = d1 = 4 and |V2| = d2 = 5. We denote Ve = {ve1, . . . , vede }, that is, the superscript indicates the domain a node belongs to.",
2302.00993v2, Which of the random variables has the largest variance?, All of the random variables have the same variance.,2302.00993v2-Figure6-1.png, Histograms showing the frequency of 25000 values sampled from the random variables εi with distribution as specified in Appendix E. Each distribution has mean zero and variance one. The first row shows the empirical distributions from ε1 to ε5 and the second row from ε6 to ε9.,
2302.00993v2,How does the average number of ℓ change as the sample size increases for different numbers of domains?,The average number of ℓ increases as the sample size increases for all numbers of domains.,2302.00993v2-Figure7-1.png, Simulation results for ℓ = 5 shared latent nodes. Logarithmic scale on the x-axis. Error bars in (a) are one standard deviation of the mean and in (b) and (c) they are the interquartile range.,
2302.00993v2,How many observed nodes are in domain V1?,There are three observed nodes in domain V1.,2302.00993v2-Figure2-1.png," Compact version of a 2-domain graph G2 with five latent nodes and two domains V1 and V2. All observed nodes in each domain are represented by a single grey node. We draw a dashed blue edge from latent node h ∈ H to domain Ve ⊆ V if h ∈ Se = pa(Ve). The random vectors associated to the two domains are uncoupled, that is, we do not observe their joint distribution.",
2303.06806v1,Which dataset has the shortest average duration?,CH-adapt and CH-test,2303.06806v1-Table1-1.png, Statistics of training/adaptation/test datasets.,
2303.06806v1,What is the difference between the conventional EDA and the proposed non-autoregressive attractor extraction?,"The conventional EDA uses an LSTM autoregressive process to extract attractors, while the proposed non-autoregressive attractor extraction uses an attention mechanism.",2303.06806v1-Figure2-1.png, System diagrams of the conventional EEND-EDA and the proposed non-autoregressive attractor extraction.,
2303.06806v1,Which layer has the highest diarization error rate (DER)?,Layer 1.,2303.06806v1-Table3-1.png, Diarization error rates (%) with intermediate speaker labels. The results are obtained using the EEND-NA-deep+SelfCond model after adaptation.,
2303.06806v1,Which model has the highest training throughput?,EEND-EDA.,2303.06806v1-Table2-1.png," Diarization error rates (%) on CH-test, the number of parameters and training throughput (Tp; #batches/sec) for each model.",
2303.06806v1,What is the role of the EDA block in the EEND-EDA model?,The EDA block is responsible for estimating the data distribution and generating new samples.,2303.06806v1-Figure1-1.png, The overview of the proposed method with the four-layer EEND-EDA model.,
2303.13659v1,What is the main difference between the local upsampling method and the proposed PGCU method?,"The local upsampling method only utilizes the local information of LRMS for upsampling, while the proposed PGCU method can fully exploit the rich global information of LRMS and the cross-modal global information of PAN.",2303.13659v1-Figure1-1.png," Comparison between local upsampling methods and our proposed PGCU method. The local method has limited receptive field and thus only utilizes the local information of LRMS for upsampling, while our proposed PGCU method can fully exploit the rich global information of LRMS and the cross-modal global information of PAN.",
2303.13659v1,Which method achieved the best performance on the WorldView3 dataset in terms of PSNR?,SFIIHN* achieved the best performance on the WorldView3 dataset in terms of PSNR.,2303.13659v1-Table2-1.png, The average results of component replacement experiments. Methods with * represent the method whose upsampling method is replaced by our PGCU method without any further changes. The best results in each column are in bold.,
2303.13659v1,Which method and parameter setting achieved the highest PSNR on the WorldView3 dataset?,GPPNN* with a parameter setting of 0.17M.,2303.13659v1-Table3-1.png, The results of equal parameter experiment. The parameter quantity of methods with is incremented to be the same as methods with *. The best results in each column are in bold.,
2303.13659v1,Which method produces the image with the highest PSNR value?,SFIIN,2303.13659v1-Figure4-1.png, Visualize comparison of one sample image from the WV2 dataset. PSNR value is shown under the corresponding method.,
2303.13659v1,How does the performance of GaoFen2 change with increasing feature vector length?,"The performance of GaoFen2 generally improves with increasing feature vector length, as evidenced by the increasing values of SSIM, SCC, and PSNR. However, there are some fluctuations in the performance metrics, suggesting that the relationship between feature vector length and performance is not strictly monotonic.",2303.13659v1-Table5-1.png, Experimental results with different feature vector lengths.,
2303.13659v1,Which combination of PAN information and channel specificity yielded the highest PSNR value?,Using both PAN information and channel specificity yielded the highest PSNR value of 46.715.,2303.13659v1-Table6-1.png, Ablation study on PAN information and channel specificity.,
2303.13659v1,How does the PGCU module extract information from the input images?,"The PGCU module extracts information from the input images by using three different branches. The first branch extracts value information (G) from the LRMS image, the second branch extracts pixel information (F) from the PAN image, and the third branch extracts channel distribution information (V) from the MS image.",2303.13659v1-Figure2-1.png, How our proposed PGCU module can be used in the existing pansharpening networks. (a) PGCU module is embedded into the residual backbone; (b) PGCU module is embedded into the two-branch backbone; (c) The overall flow of the PGCU module.,
2303.13659v1,Which dataset has the most training samples?,GaoFen2,2303.13659v1-Table1-1.png, The basic information for each dataset.,
2303.13659v1,What is the purpose of the PGCU module?,The PGCU module is used to generate a high-resolution multispectral image from a low-resolution multispectral image and a high-resolution panchromatic image.,2303.13659v1-Figure3-1.png," The detailed structure of each block in the PGCU module, where D represents the vector dimension in F and G.",
2304.04959v2,How does the AdaTTT model differ from the other MTL models shown in the figure?,"The AdaTTT model uses a combination of linear fusion and expert modules, while the other models use either gate fusion or linear fusion.",2304.04959v2-Figure2-1.png," MTL models used in our experiments. In multi-level MTL models, two fusion levels are used to illustrate their design. Modules are represented using different colors: shared modules are in blue, task A’s specific modules are in yellow, and task B’s specific modules are in purple.",
2304.04959v2,Which model performs the best on both tasks?,AdaTT-sp (single task expert),2304.04959v2-Table3-1.png, Performance on the consumption task group,
2304.04959v2,Which model performed the best on the consumption task in the consumption + engagement task group?,AdaTT-sp (single task expert),2304.04959v2-Table1-1.png, Performance on the consumption task in consumption + engagement task group,
2304.04959v2,Which model performs the best on the engagement task in the consumption + engagement task group?,AdaTT-sp (single task expert),2304.04959v2-Table2-1.png, Performance on the engagement task in consumption + engagement task group,
2304.04959v2,Which model performed the best on the consumption task 1?,AdaTT,2304.04959v2-Table4-1.png, Model performance on the group of 5 tasks,
2304.04959v2,Which model performs best on all three tasks?,AdaTT,2304.04959v2-Table5-1.png," Performance on 3 tasks of UCI Census income dataset. We compare PLE, ML-MMoE and AdaTT using 2-level fusion. Expert and task tower networks are single-layer MLPs and their hidden dimensions are listed. The AdaTT-sp setup, which solely utilizes task-specific experts, enables AdaTT to achieve its optimal results.",
2304.04959v2,How does the general AdaTT model differ from the AdaTT-sp model?,"The general AdaTT model includes an optional shared fusion unit, which is not present in the AdaTT-sp model.",2304.04959v2-Figure1-1.png," AdaTT-sp and general AdaTT with 2 fusion levels. Task A and B’s specific and shared modules are differentiated by color: yellow for A, purple for B, and blue for shared. For the purpose of illustration, we use 2 experts for each task-specific unit. In general AdaTT, we add a shared fusion unit which has a single expert as an example. Note that the shared modules in general AdaTT are not necessary and are therefore depicted using dotted lines. When there are no shared modules present, general AdaTT falls back to AdaTT-sp.",
2304.04959v2,How does the performance of AdaTT change as the number of experts per task increases?,The performance of AdaTT generally improves as the number of experts per task increases.,2304.04959v2-Table7-1.png, AdaTT’s performance with different number of experts per task.,
2304.04959v2,What is the general trend of AdaTT’s performance as the fusion level increases?,The performance of AdaTT generally decreases as the fusion level increases.,2304.04959v2-Table8-1.png, Results of AdaTT’s performance as the fusion level increases. We denote each fusion level’s expert hidden dimensions in the first column.,
2304.04959v2,Which task is most affected by the ablation of the 𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹 module?,Engagement task 1.,2304.04959v2-Table6-1.png, Ablation study on the𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹 module. The performance degradation on every task demonstrates the importance of residual mechanism with separate fusion.,
2304.04959v2," Based on the figure, which expert was assigned the highest weight for the engagement task at level 1?", Expert 3,2304.04959v2-Figure3-1.png," Visualization of the distribution of expert weights learned at each fusion level in a two-level AdaTT-sp. Tasks and experts are arranged in the order of consumption regression task, consumption classification task and engagement task. Note this figure shows the sum of weights from the 𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹 and 𝐴𝑙𝑙𝐸𝑥𝑝𝑒𝑟𝑡𝐺𝐹 modules. As there is only one native expert per task, 𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹 module assigns unit weights to them (mapped to diagonal grids in the figure).",
2304.07413v1,Which method has the lowest variance in its norm estimate?,Our method has the lowest variance in its norm estimate.,2304.07413v1-Figure1-1.png, Figures for our experiments.,
2304.14514v1,Which encoder produces representations that are more tightly clustered by duration?,The LS Maestro text encoder.,2304.14514v1-Figure4-1.png," T-SNE of LS Maestro text encoder outputs (left), and shared encoder text output (right), color coded by duration",
2304.14514v1,Which Maestro method performs the best on the AMI dataset?,AMI Maestro.,2304.14514v1-Table4-1.png, Shared space cosine retrieval probe accuracy (%),
2304.14514v1,Which method has the highest accuracy on the SWBD test set?,TED Maestro (Direct),2304.14514v1-Table5-1.png, Text and speech encoder retrieval probe accuracy (%),
2304.14514v1,What is the relationship between the different types of data shown in the figure?,"The figure shows that the different types of data are clustered together, suggesting that they are related. For example, the LS test-clean and LS test-other data are clustered together, as are the CV test and TED test data. This suggests that these data types are similar to each other.",2304.14514v1-Figure3-1.png, T-SNE of SLAM text (crosses) and speech (dots) shared encoder outputs,
2304.14514v1,Which method performs the best in terms of AMI?,"The AMI Duration, LS Refiner method performs the best in terms of AMI, with a score of 27.20 for ihm and 52.13 for sdm1.",2304.14514v1-Table3-1.png, Ablation of importance for in-domain text encoder components with AMI Text adaptation.,
2304.14514v1,Which adaptation method performs best on the LibriSpeech test-clean dataset?,The LS Maestro init method performs best on the LibriSpeech test-clean dataset with a WER of 2.22.,2304.14514v1-Table1-1.png, LibriSpeech Maestro adaptation results.,
2304.14514v1,Which text encoder training corpus resulted in the lowest SWBD score?,TED Duration Training,2304.14514v1-Table2-1.png," Ablation of text encoder trained on different corpora with Switchboard (SWBD) Text Adaptation, using text encoders trained on different corpora.",
2304.14514v1,What is the difference between the two figures?,"The left figure shows the t-SNE of LibriSpeech Maestro text and speech encoder outputs, while the right figure shows the t-SNE of the shared encoder output.",2304.14514v1-Figure2-1.png," T-SNE of LibriSpeech Maestro text (crosses) and speech (dots) encoder outputs (left), and shared encoder output (right)",
2304.14514v1,What is the role of the shared encoder in the Maestro architecture?,The shared encoder takes the speech encoder's output and the text encoder's output and combines them to create a representation of the input data that is shared by both the ASR decoder and the auxiliary phone decoder.,2304.14514v1-Figure1-1.png, Maestro architecture.,
2305.04582v2,Which language has the highest median micro-F1 score on the test split of the target language?,English (en),2305.04582v2-Table2-1.png," Micro-F1 scores on the TACREV dataset for the monolingual setting. The table shows the median microF1 score across 5 runs, on the test split of the target language (testL), and on the intersection of test instances available in all languages (test∩).",
2305.04582v2,What is the best learning rate for the Finnish language model?,7e-6,2305.04582v2-Table7-1.png," Best learning rate and model identifiers per language for the monolingual settings, and for the cross- and multilingual scenarios. The table also lists the model license, if it was available.",
2305.04582v2,What are some of the common error types that can be automatically detected in translated TACRED examples?,Some of the common error types that can be automatically detected in translated TACRED examples are missing or additional aligned spans in the translation.,2305.04582v2-Table8-1.png," Common error types of translated TACRED examples. The first half of the table shows alignment errors that can be automatically detected, such as missing or additional aligned spans in the translation. The second half shows error types identified by human judges.",
2305.04582v2,Which language benefits the most from adding in-language training data?,"The language that benefits the most from adding in-language training data is ""zh"".",2305.04582v2-Table4-1.png," Micro-F1 scores on the TACREV dataset for the mixed/multilingual setting. The table shows the median micro-F1 score across 5 runs, on the translated test split of the target language, when training mBERT on the full English train split and various portions, from 5% to 100%, of the translated target language train split. The last column shows the mean improvement across languages, compared to the cross-lingual baseline. Micro-F1 scores improve when adding in-language training data for languages not well represented in mBERT, while other languages mainly benefit when using all of the English and in-language data, i.e. essentially doubling the amount of training data (last row).",
2305.04582v2,Which language has the highest median F1 score on the test∩ split?,Japanese,2305.04582v2-Table3-1.png," Micro-Precision, Recall and F1 scores on the TACREV dataset for the cross-lingual setting. The table shows the median scores across 5 runs, on the translated test split of the target language (testL) and on the intersection of test instances available in all languages (test∩), when training mBERT on the English train split. For reference, the table also shows the size of mBERT’s training data in a given language (Wikisize, as log2(MegaBytes), taken from Wu and Dredze (2020)). Languages with less pretraining data in mBERT suffer a larger performance loss.",
2305.04582v2,What does the XML markup in the example translations indicate?,The XML markup indicates the head and tail entities of the relation argument annotations.,2305.04582v2-Figure1-1.png," Example translations from English to German, Polish, Turkish and Chinese with XML markup for the head and tail entities to project relation argument annotations.",
2305.04582v2,Which language translation engine produced the most errors during translation?,Google,2305.04582v2-Table6-1.png," MultiTACRED instances per language and split, and for the back-translation (BT) of the test split. The ‘en’ row shows the statistics of the original TACRED. (G) and (D) refer to Google and DeepL, respectively. The error columns list the number of instances discarded after translation due to missing / erroneous entity tag markup. On average, 2.3% of the instances were discarded due to invalid entity markup after translation. The last row shows the intersection of valid instances available in all languages.",
2305.04582v2,What are the two questions that the judges are asked to answer?,"The judges are asked to answer two questions: 
1. Does the sentence express the relation ""as"" in the EN original?
2. Is the translation in general linguistically acceptable for a native speaker?",2305.04582v2-Figure2-1.png, Task description given to human judges for translation quality analysis.,
2305.04582v2,Which language has the highest median micro-F1 score?,English.,2305.04582v2-Table5-1.png," Median micro-F1 scores across 5 runs of the English BERT model evaluated on the back-translated test splits of all languages. Compared to the micro-F1 score of 77.1 on the untranslated English test set, backtranslation results are somewhat lower, due to MT system quality and the linguistic variance introduced by the back-translation.",
2305.04582v2,Which language had the highest translation quality in Q1?,Turkish (tr),2305.04582v2-Table1-1.png," Translation quality, as judged by native speakers. (Q1) Does the translated text meaningfully express the semantic relation of the English original, regardless of minor translation errors? (Q2) Is the overall translation linguistically acceptable for a native speaker?",
2305.08328v1,What is the role of the non-label party in the vertical federated learning framework?,"The non-label party, in this case, the online publisher, provides the collected user feedback data, such as browsing behavior and clicks, to collaboratively train the conversion rate estimation model.",2305.08328v1-Figure1-1.png, (a) The feedback behaviors after a user browses an ad. (b) Vertical federated learning framework for training conversion rate estimation model. Online publisher and advertising platform provide the collected user feedback data and collaboratively train the model.,
2305.08328v1," What is the difference between the ""Projection"" and ""Mixup and Projection"" operations shown in the figure?"," The ""Projection"" operation projects the mixed gradient onto the unit sphere, while the ""Mixup and Projection"" operation first mixes the gradient with a random vector and then projects the result onto the unit sphere.",2305.08328v1-Figure4-1.png, Illustration of gradient mixup and projection operations inMixPro for a training sample’s gradient w.r.t. the federated embedding.,
2305.08328v1,Which vFL algorithm achieves the best ranking performance according to the AUC metric?,ORALE,2305.08328v1-Table3-1.png, Effectiveness evaluation of comparative vFL algorithms for CVR estimation.,
2305.08328v1,How many samples are in the test set?,1.3 million samples.,2305.08328v1-Table1-1.png, Statistics of the dataset.,
2305.08328v1,Which dataset has the largest number of samples?,FedAds.,2305.08328v1-Table2-1.png, Comparison of datasets.,
2305.08328v1,What are the three main components of the ad delivery procedure?,"The three main components of the ad delivery procedure are the ranking system, the user, and the publisher.",2305.08328v1-Figure2-1.png, Brief illustration of the ranking stage in ad delivery procedure and user behaviors. The proposed dataset is built upon the click log collected from this procedure.,
2305.08328v1,Which defense approach provides the best trade-off between privacy and utility?,Marvell.,2305.08328v1-Table4-1.png, Privacy evaluation of comparative defense approaches to label inference attack.,
2305.08328v1, How does the Diffu-AT framework exploit unaligned samples in vFL training?," The Diffu-AT framework exploits unaligned samples in vFL training by learning a conditional diffusion model for generating federated embeddings of the label party's unaligned samples. This is done by first training a denoising diffusion probabilistic model (DDPM) on the label party's aligned samples. Then, the DDPM is used to generate federated embeddings of the unaligned samples. These embeddings are then used to train the vFL model.",2305.08328v1-Figure3-1.png, (a) Learning a conditional diffusion model for generating federated embeddings of label party’s unaligned samples. (b) The Diffu-AT framework for exploiting unaligned samples in vFL training.,
2305.10727v1,What is the purpose of the weight factors in the sparse floating-point model?,The weight factors indicate the probability that a layer has a real influence on the final accuracy of the sparse quantized model.,2305.10727v1-Figure5-1.png," GPUSQ-ViT scheme with two sub-workflows. For the 2:4 structured sparse pruning workflow, the dense floating-point model MDF is compressed as the sparse floating-point model MSF . Hard label, soft logits and feature-based distillation losses are accumulated as the overall sparse pruning loss. The sparse floating-point model MSF is quantized as the sparse quantized model MSQ for the sparsedistillation-aware QAT workflow. Hard label and soft logits calibration losses are obtained in a similar manner. Each feature maps calibration result is multiplied with a weight factor to indicate this layer’s probability of having a real influence on MSQ model’s final accuracy. Three calibration losses are accumulated as the overall quantization calibration loss.",
2305.10727v1,How does the sparse GEMM operation on the Tensor Core differ from the dense GEMM operation?,"The sparse GEMM operation only multiplies the non-zero elements of matrix A with the corresponding elements of matrix B, while the dense GEMM operation multiplies all elements of matrix A with the corresponding elements of matrix B.",2305.10727v1-Figure1-1.png," Comparison of computing a M ×N ×K GEMM onto a Tensor Core. Dense matrix A of size M ×K in left side becomes M × K 2 in right side after compressing with 2:4 fine-grained structured sparse pattern. Sparse Tensor Core automatically picks only the elements from B according to the nonzero elements in A. Comparing the dense and sparse GEMMs, B and C are the same dense K ×N and M ×N matrices, respectively. By skipping the unnecessary multiplications of redundant zeros, sparse GEMM accelerate the dense GEMM with 2×.",
2305.10727v1,Which model seems to be focusing on the most relevant parts of the image for each example?,The Swin Transformer baseline dense models seem to be focusing on the most relevant parts of the image for each example.,2305.10727v1-Figure6-1.png, CAM visualization for Swin Transformer baseline dense models and GPUSQ-ViT compressed INT8 and INT4 models.,
2305.10727v1,How does the performance of GPUSQ-ViT compare to other state-of-the-art compression methods on the classification task?,"GPUSQ-ViT achieves similar or slightly better accuracy than other state-of-the-art compression methods, while requiring significantly fewer parameters and FLOPs.",2305.10727v1-Table1-1.png, Compare the model size and FLOPs of GPUSQ-ViT with state-of-the-art compression methods on classification task.,
2305.10727v1,Which model and method combination achieves the highest throughput on the NVIDIA A100 GPU with a batch size of 256?,The Swin-Base model with the GPUSQ-ViT method achieves the highest throughput on the NVIDIA A100 GPU with a batch size of 256.,2305.10727v1-Table2-1.png, Deployment efficiency of GPUSQ-ViT compressed DeiT and Swin Transformer models on NVIDIA GPUs. The latency is measured with batch size 1 on a single A100 GPU and AGX Orin. The throughput is measured with batch size fixed to 256 on a single A100 GPU and with batch size fixed to 64 on a single AGX Orin.,
2305.10727v1,How many bits are used to store the non-zero elements in the Sparse INT4 data format?,4 bits,2305.10727v1-Figure2-1.png," Storage formats for 2:4 fine-grained structured sparse pattern and metadata with FP16, INT8 and INT4 operators. (w,x,y,z denote the non-zero elements.)",
2305.10727v1,How does the performance of the GPUSQ-ViT models compare to the baseline models in terms of accuracy?,"The GPUSQ-ViT models generally perform slightly worse than the baseline models in terms of accuracy, as measured by both Mean IoU and Pixel Acc. However, the difference in accuracy is small, typically less than 0.5%.",2305.10727v1-Table4-1.png, Effectiveness of GPUSQ-ViT on semantic segmentation. GPUSQ-ViT provides good compression effects on detection and segmentation tasks in Table 3 and 4 with small accuracy gap to the dense baseline models.,
2305.10727v1,Which model has the highest Top-1 accuracy for GPUSQ-ViT (INT8)?,DeiT-Base with 384^2 input size.,2305.10727v1-Table5-1.png, Effectiveness of GPUSQ-ViT in unsupervised learning.,
2305.10727v1,What is the effect of enabling QAT on the Top-1 and Top-5 accuracy of GPUSQ-ViT (INT8) and GPUSQ-ViT (INT4) models?,Enabling QAT generally improves the Top-1 and Top-5 accuracy of both GPUSQ-ViT (INT8) and GPUSQ-ViT (INT4) models.,2305.10727v1-Table6-1.png, Ablation study of the loss adjustment factors and sparsedistillation-aware weight factors of GPUSQ-ViT method.,
2305.10727v1,How does the Cascade Mask R-CNN model with Swin-Tiny backbone compare to the DETR model with ResNet50 backbone in terms of bbox mAP and FLOPs?,"The Cascade Mask R-CNN model with Swin-Tiny backbone achieves a higher bbox mAP of 51.9 compared to 42.0 for the DETR model with ResNet50 backbone. However, the Cascade Mask R-CNN model also requires more FLOPs, 27.5 G compared to 2.8 G for the DETR model.",2305.10727v1-Table3-1.png, Effectiveness of GPUSQ-ViT on object detection task.,
2305.10727v1, Which layers of the vision transformer are targeted by the 2:4 fine-grained structured sparsity?," The patch embedding, final linear projection, as well as the feed forward and linear projection inside each transformer block.",2305.10727v1-Figure3-1.png," Illustration about applying the 2:4 fine-grained structured sparsity in vision transformer. The target layers include the patch embedding, final linear projection, as well as the feed forward and linear projection inside each transformer block.",
2305.10727v1,What is the purpose of the figure?,The figure shows the attention maps for Swin Transformer ImageNet-1K pretrained models.,2305.10727v1-Figure4-1.png," Attention map visualization for Swin Transformer ImageNet-1K pretrained models. (a-1) and (b-1) Swin-V1-Tiny [2, 2, 6, 2]. (a-2) and (b-2) Swin-V1-Base [2, 2, 18, 2]. Numbers in square brackets indicate how many Swin Transformer blocks in each stage. We choose the output of last Swin Transformer block in each stage, to generate the CAM visualization results.",
2305.14152v2,Which method resulted in the lowest perplexity for GPT-Neo 2.7B when quantized into 4 bits?,PEQA,2305.14152v2-Table2-1.png," To empirically confirm the validity of PEQA’s approach, we compare the perplexity (PPL) of fine-tuned LLMs through QAT, PEFT+PTQ, and PEQA on Wikitext2 [51] for GPT-Neo 2.7B, GPT-J 6B, LLaMA 7B, and LLaMA 13B. Weights are quantized into either 3-bit or 4-bit per channel, without a group size [28, 49]. LoRA configuration is set to QV4. The lower PPL, the better.",
2305.14152v2,Which quantization method resulted in the lowest perplexity for LLaMA 7B?,Both zero-points and quantization scales,2305.14152v2-Table17-1.png, Perplexity (PPL) of PEQA on the Wikitext2 dataset for LLaMA 7B and LLaMA 13B with weights quantized into 4-bit.,
2305.14152v2,What is the purpose of the decomposition (quantization) step in the PEQA scheme?,"The decomposition (quantization) step is used to convert the full-precision weights of the model into sub-4-bit integers. This reduces the memory footprint of the model, which makes it more efficient to fine-tune and deploy.",2305.14152v2-Figure1-1.png, Illustration of our proposed PEQA scheme where A ·B indicates the element-wise product of A and B. PEQA is memory-efficient fine-tuning method for quantized large language models that updates only the quantization scale while keeping the integer matrix frozen. Notice a significant reduction in memory footprint when full-precision weights are converted into sub-4-bit integers.,
2305.14152v2,How does the performance of PEQA compare to LoRA on the Wikitext2 and PTB datasets?,PEQA performs better than LoRA on both the Wikitext2 and PTB datasets.,2305.14152v2-Table3-1.png," To show scalability of PEQA, the perplexity (PPL) on Wikitext2 and PennTreeBank (PTB) was compared with LoRA and PEQA. In this comparison, only the weights were quantized into 3-bit and 4-bit per-channel without group size. LoRA configuration is set to QV4. A lower PPL value indicates better performance.",
2305.14152v2,Which model and quantization method combination achieved the highest average MMLU score?,LLaMA2 with PEQA quantization achieved the highest average MMLU score of 67.5.,2305.14152v2-Table7-1.png," Massive Multitask Language Understanding (MMLU) benchmark performance of PEQAtuned LLaMAs using Alpaca datasets. Five-shot accuracy is reported for the MMLU. Quantization precision of PEQA is set to 4-bit. When we quantize LLaMA [6] into 4-bit precision using the RTN method, no group size is applied. For LLaMA2 [7], a group size of 256 is used with the RTN method. Note that RTN stands for round-to-nearest in the table.",
2305.14152v2,Which model has the best performance on Wikitext2 according to the table?,LLaMA 65B,2305.14152v2-Table11-1.png, The perplexity (PPL) on Wikitext2 was compared with LoRA QV4 and QKVO16. A lower PPL value indicates better performance.,
2305.14152v2,Which method performs better according to the perplexity (PPL) values in the table?,LoRA(QV4) performs better than PEQA(Ours) according to the PPL values in the table.,2305.14152v2-Table10-1.png," The perplexity (PPL) on Wikitext2 for OPT 1.3B to 66B. In this comparison, only the weights were quantized into 4-bit. A lower PPL value indicates better performance.",
2305.14152v2,Which quantization method is more efficient in terms of model size and perplexity trade-off for 3-bit quantization?,PEQA (Ours) is more efficient.,2305.14152v2-Figure3-1.png, The perplexity over model size of 3/4-bit performance of PEQA and LoRA+OPTQ,
2305.14152v2,How does the number of learnable parameters in LLaMA models change as the model size increases?,"The number of learnable parameters in LLaMA models increases as the model size increases. This is shown in the table, where the number of learnable parameters for LLaMA models ranges from 2.10 million for the 7B model to 10.49 million for the 65B model.",2305.14152v2-Table4-1.png," Number of learnable parameters and model size of GPT-Neo, GPT-J and LLaMAs. PEQA configuration is set to 4-bit or 3-bit channel-wise quantization.",
2305.14152v2,Which model performs better in terms of perplexity when W Bits is set to 3?,LLaMA 13B performs better in terms of perplexity when W Bits is set to 3.,2305.14152v2-Table5-1.png, Multi-scale (grouping) performance with PEQA-tuned LLaMA 7B and 13B on Wikitext2 where g indicates the group size [49]. The perplexity consistently increases as PEQA take on more learnable parameters.,
2305.14152v2,How does the number of trainable parameters in the LLaMA-65B model with LoRA compare to the number of trainable parameters in the LLaMA-65B model with 4-bit PEQA?,"The LLaMA-65B model with LoRA has 10.49M trainable parameters, while the LLaMA-65B model with 4-bit PEQA has 6.8M trainable parameters. This means that the LLaMA-65B model with LoRA has almost 4 million more trainable parameters than the LLaMA-65B model with 4-bit PEQA.",2305.14152v2-Figure2-1.png," (a) DRAM usage comparison of LLaMA-65B on various tuning methods and (b) perplexity over model size when tuning LLaMA models with LoRA and PEQA on Wikitext2 dataset. The size of a circle indicates the number of trainable parameters. For instance, the LLaMA-65B model with LoRA has a size of 131GB and 10.49M trainable parameters. Otherwise, LLaMA-65B with 4-bit PEQA has a model size of 33GB and 6.8M trainable parameters.",
2305.14152v2,Which LLaMA model has the highest epoch value?,LLaMA 30B,2305.14152v2-Table13-1.png," The learning rate, epoch, quantization group size [49] for experiments on Section 4.3. the weights were quantized into 4-bit.",
2305.14152v2,Which model and quantization configuration achieves the lowest learning rate?,LLaMA 7B with W Bits = 4 and g = 1.,2305.14152v2-Table12-1.png, Learning rate for Table 5.,
2305.14152v2,What is the learning rate of QAT with 4 W bits for GPT-Neo 2.7B?,4e-5,2305.14152v2-Table8-1.png, Learning rates of QAT in Table 2.,
2305.14152v2,Which method and model combination achieves the lowest learning rate on the Wikitext2 dataset?,PEQA (Ours) with 3 W bits and LLaMA 65B,2305.14152v2-Table9-1.png, Learning rate of LoRA and PEQA in Table 3 on Wikitext2 and PTB datasets.,
2305.14152v2,Which method performs the best on the PIQA dataset in the zero-shot setting?,LLaMA + PEQA with 30 billion parameters.,2305.14152v2-Table6-1.png," Common-sense reasoning and in-context learning performance of parameter-efficient instruction-tuned LLaMAs [6] using Alpaca datasets. LoRA configuration is set to QKVO16. Quantization precision of PEQA is set to 4-bit per-channel without group size. Note that ARC-C, ARC-E and OBQA stands for ARC-Challenge, ARC-Easy, and OpenBookQA respectively.",
2305.14152v2,What is the learning rate of AlphaTuning when W Bits is 4 and GPT-Neo 1.3B is used?,5e-4,2305.14152v2-Table16-1.png, Learning rate of AlphaTuning in Table 15.,
2305.14152v2,Which method achieves lower perplexity on Wikitext2 with OPT 1.3B: AlphaTuning or PEQA?,PEQA achieves lower perplexity on Wikitext2 with OPT 1.3B.,2305.14152v2-Table15-1.png," The perplexity (PPL) of AlphaTuning and PEQA on Wikitext2 with OPT and GPT-Neo 1.3B. The lower PPL, the better.",
2305.14152v2,Which model configuration has the highest zero-shot ROUGE-L score on the Natural Instruction benchmark?,+PEQA with 13B parameters.,2305.14152v2-Table14-1.png, Natural Instruction benchmark performance of parameter-efficient instruction-tuned LLaMAs using Alpaca datasets. Zero-shot performance (ROUGE-L) is reported for the NI. LoRA configuration is set to QKVO16. Quantization precisions of LoRA w/ OPTQ and PEQA are set to 4-bit.,
2305.14152v2,Which method requires the least amount of DRAM for deployment?,PEQA and PTQ+PEFT both require the least amount of DRAM for deployment.,2305.14152v2-Table1-1.png," Comparison of PEQA with other methods using LLaMA 65B on the DRAM usage and training time during fine-tuning, the DRAM storage for deployment, the inference acceleration, and task-switching efficiency. The DRAM usage estimation for PEFT is based on LoRA. PEFT+PTQ denotes PTQ after PEFT and PTQ+PEFT denotes PTQ before PEFT.",
2305.17225v2,Which intervention target results in the same outcome for both latent CBN models in all datasets?,Wet grass (W),2305.17225v2-Figure5-1.png," Typical cases in the four definitions of CauCA in Defn. E.2. Each row corresponds to a dataset in which there is one perfect intervention target: Cloud (C), Sprinkler (S), Rain (R), and Wet grass (W). We consider two latent CBN models with their nodes corresponding to the same nodes in V (G), with intervention targets (τi)i∈[5] and (τ ′i)i∈[5]. Suppose we are given G and (τi)i∈[5]. The mapping V (G′) −→ V (G) are unknown in all settings except with known intervention targets. We give one example of mapping V (G′) −→ V (G) for each of the 4 settings in Defn. E.2.",
2305.17225v2,Which dataset in the figure is generated from the causal structure where Z1 and Z3 are intervened upon?,Dataset D3.,2305.17225v2-Figure1-1.png," Causal Component Analysis (CauCA). We posit that observed variables X are generated through a nonlinear mapping f , applied to unobserved latent variables Z which are causally related. The causal structure G of the latent variables is assumed to be known, while the causal mechanisms Pi(Zi | Zpa(i)) and the nonlinear mixing function are unknown and to be estimated. (Known or observed quantities are highlighted in red.) CauCA assumes access to multiple datasets Dk that result from stochastic interventions on the latent variables.",
2305.17225v2,"How many interventions are required per node for the representation [h_1(z_1), h_2(z_2), h_3(z_2, z_3)] to be identifiable according to Proposition D.1?","One intervention per node for z_1 and z_2, plus 2 x 3 = 6 imperfect interventions on z_3.",2305.17225v2-Table1-1.png," Overview of identifiability results. For the DAG Z1 −→ Z2 −→ Z3 from Fig. 1, we summarise the guarantees provided by our theoretical analysis in § 4.1 for representations learnt by maximizing the likelihoods Pk θ(X) for different sets of interventional regimes.",
2305.17225v2,What is the difference between the two assumptions shown in the figure?,"The left assumption (Asm. 4.4) requires fewer interventions to achieve block-identification of zτk than the right assumption (Prop. 4.9), which requires twice as many interventions to achieve elementwise identification up to scaling and permutation.",2305.17225v2-Figure3-1.png," We use the “ ” symbol together with a “times” symbol to represent how many interventions are required by the two assumptions. (Left) (Thm. 4.5) For Asm. 4.4, we need nk interventions to get block-identification of zτk . (Right) (Prop. 4.9) For the block-variability assumption, we need 2nk to get to elementwise identification up to scaling and permutation.",
2305.17225v2,"What is the minimum number of interventions needed to identify the learned representation  if we assume the ""variability"" assumption?",6 fat-hand interventions.,2305.17225v2-Table2-1.png," Overview of ICA identifiability results. For the trivial DAG over Z1, Z2, Z3 (i.e., no edges), we summarise our identifiability results (§ 4.2) for representations learnt by maximizing the likelihoods Pk θ(X) based on multiple interventional regimes. π[·] denotes a permutation.",
2305.17225v2,Which model performed the best in terms of the mean correlation coefficient (MCC)?,CauCA with a local scale model.,2305.17225v2-Figure6-1.png, Experimental results with nonparametric model. The figure shows the mean correlation coefficients (MCC) between true and learned latents for Causal Component Analysis (CauCA) experiments. The first two violin plots show the fully nonparametric model when the ground truth latent CBN is generated by a locationscale model and for the case with a linear SCM generating the ground truth CBN. Misspecified models assuming a linear encoder function class and a naive (single-environment) unidentifiable normalizing flow with an independent Gaussian base distribution (labelled i.i.d.) are compared. The misspecified models are trained on a location-scale CBN. All violin plots show the distribution of outcomes for 10 pairs of CBNs and mixing functions.,
2305.17225v2,"Which method performs better when the signal-to-noise ratio in the structural causal model is low, CauCA or the misspecified baseline?",CauCA performs better when the signal-to-noise ratio in the structural causal model is low.,2305.17225v2-Figure4-1.png," Experimental results. Figures (a) and (e) present the mean correlation coefficients (MCC) between true and learned latents and log-probability differences between the model and ground truth (∆ log prob.) for CauCA experiments. Misspecified models assuming a trivial graph (E(G)=∅) and a linear encoder function class are compared. All violin plots show the distribution of outcomes for 10 pairs of CBNs and mixing functions. Figures (c) and (d) display CauCA results with varying numbers of nonlinearities in the mixing function and latent dimension. For the ICA setting, MCC values and log probability differences are illustrated in (b) and (f). Baselines include a misspecified model (linear mixing) and a naive (single-environment) unidentifiable normalizing flow with an independent Gaussian base distribution (labelled i.i.d.). The naive baseline is trained on pooled data without using information about interventions and their targets. Figure (g) shows the median MCC for CauCA and the misspecified baseline (E(G)=∅) as the strength of the linear parameters relative to the exogenous noise in the structural causal model generating the CBN increases. The shaded areas show the range between minimum and maximum values.",
2305.17225v2,How does the figure illustrate the violation of the Interventional Discrepancy Assumption?,"The figure shows that it is possible for two different latent variable models to have the same observed distribution, even after interventions are performed. This violates the Interventional Discrepancy Assumption, which states that different latent variable models should have different observed distributions after interventions.",2305.17225v2-Figure2-1.png," Violation of the Interventional Discrepancy Assumption. The shown distributions constitute a counterexample to identifiability that violates Asm. 4.1 and thus allows for spurious solutions, see App. C for technical details. (Left) Visualisation of the joint distributions of two independent latent components z1 and z2 after no intervention (red), and interventions on z1 (green) and z2 (blue). As can be seen, each distribution reaches the same plateau on some rectangular interval of the domain, coinciding within the red square. (Center/Right) Within the red square where all distributions agree, it is possible to apply a measure preserving automorphism which leaves all distributions unchanged, but non-trivially mixes the latents. The right plot shows a distance-dependent rotation around the centre of the black circle, whereas the middle plot show a reference identity transformation.",
2305.17721v1,Why did the model over-correct the second test input?,"The model over-corrected the second test input because it was trained on data that included the correction of ""stiff"" to ""sound"".",2305.17721v1-Figure1-1.png, Mistakes made by regularly fine-tuned BERT.,
2305.17721v1,How did adding MFT affect the performance of the different models on the CAR dataset?,"Adding MFT improved the performance of all the models on the CAR dataset. For example, the F1 score for BERT increased from 47.6 to 52.3, and the F1 score for Soft-Masked increased from 47.7 to 52.0.",2305.17721v1-Table6-1.png, Performances on LEMON. We report the F1 scores and also include SIGHAN as the 8th domain (SIG).,
2305.17721v1,Which data set has the highest number of positive examples?,"The Encyclopedia data set has the highest number of positive examples, with 1712.",2305.17721v1-Table14-1.png," Data statistics for LEMON (NE: number of examples, NPE: number of positive examples, SL: sentence length, NEC: number of error characters per example, NEP: number of edit pairs). SIGHAN-15 refers to the SIGHAN-15 test set.",
2305.17721v1,Which transfer method resulted in the highest F1 score for the NEW + MEC model?,MLM Loss,2305.17721v1-Table7-1.png," Domain transfer results (F1 score). All models are trained with Masked-FT, unless specified as FT, referring to regular fine-tuning.",
2305.17721v1,How does the performance of ECSpell-LAW change as the mask rate increases?,"The performance of ECSpell-LAW generally improves as the mask rate increases, as indicated by the increasing F1, I-F1, and E-F1 scores.",2305.17721v1-Table8-1.png, Impact of mask ratio on ECSpell-LAW.,
2305.17721v1,Which mask strategy performs the best on average across all three LEMON domains?,mask any,2305.17721v1-Table9-1.png, Comparison of mask strategies on three LEMON domains (F1 score). The mask rate is 0.2.,
2305.17721v1,What does the gradient tell us about the importance of each token?,The gradient indicates how much the model's prediction changes when the embedding of a token is changed. A higher gradient indicates that the token is more important for the model's prediction.,2305.17721v1-Table3-1.png, Gradient on each token embedding. We choose a model checkpoint at the early stage of training (two epochs). The sentence is “(年级→年纪)轻轻就惨遭 谢顶。” (Shedding of hair at a young (grade→age).).,
2305.17721v1,Which model recalled the most relevant keywords for the first sentence?,The Masked-FT model.,2305.17721v1-Table1-1.png, Top-k results each model recalls on the same sentence. The models here are trained on SIGHAN. FT refers to regular fine-tuning.,
2305.17721v1,How does the performance of the fine-tuned model compare to that of the vanilla BERT model on unseen edit pairs?,The fine-tuned model performs better than the vanilla BERT model on unseen edit pairs.,2305.17721v1-Table2-1.png, CSC performance crash on unseen edit pairs.,
2305.17721v1,Which fine-tuning method leads to faster convergence of the model?,Masked fine-tuning.,2305.17721v1-Figure2-1.png," Gradient and model convergence. In (a), we compute the L2-norm of gradients over all model parameters. In (b), we evaluate the model each 15 steps.",
2305.17721v1,Which method has the highest F1 score for CAR?,Masked-FT,2305.17721v1-Table11-1.png, Masked-FT vs. confusion set (F1 score).,
2305.17721v1,"What are the differences between the outputs generated by the source, target, and Masked-FT models?","The source and target models generate outputs that contain the phrase ""可以转换成"", while the Masked-FT model generates outputs that do not contain this phrase.",2305.17721v1-Table13-1.png, Error analysis selected from ECSpell-MED.,
2305.17721v1,Which method has the highest F1 score?,Masked-FT,2305.17721v1-Table10-1.png, Masked-FT vs. confusion set (F1 score).,
2305.17721v1,How does the masked-FT compare to the FT in terms of preserving the meaning of the source sentence?,The masked-FT preserves the meaning of the source sentence better than the FT.,2305.17721v1-Table12-1.png, Case study selected from LEMON.,
2305.17721v1,Which of the datasets is more diverse in terms of the number of characters used?,LEMON.,2305.17721v1-Figure3-1.png, A snapshot of LEMON. We also include the SIGHAN-15 test set here for comparison.,
2305.17721v1,Which model performs the best on the SIGHAN-15 dataset?,PHMOSpell,2305.17721v1-Table4-1.png," Fine-tuning results on SIGHAN-15. The results in the bottom part requires additional pre-training. † indicates the result we quote (DCN (Wang et al., 2021), PLOME (Liu et al., 2021), REALISE (Xu et al., 2021), PHOMOSpell (Huang et al., 2021)).",
2305.17721v1,Which method performed the best on the LAW dataset?,"The method that performed the best on the LAW dataset was ""MDCSpell w/ Masked-FT"".",2305.17721v1-Table5-1.png, Fine-tuning results on ECSpell.,
2306.06480v1,Which model performs the best on the Level 1 4-way task according to the Ji metric?,The model by Long and Webbber (2022) performs the best on the Level 1 4-way task according to the Ji metric with an accuracy of 72.18.,2306.06480v1-Table1-1.png, Results on PDTB 2.0. Subscripts are the standard deviation of the mean performance.,
2306.06480v1,Which model performed the best on the task of predicting implicit relations and connectives?,Our Model.,2306.06480v1-Figure6-1.png," Examples on 11-way prediction of PDTB 2.0 from different models. RoBERTa and Adversarial can only predict relations, while Multi-Task, Pipeline, and Our Model make predictions on both connectives and relations. Therefore, we show both Connective (Conn) and relation (Rel) prediction results of the latter three models. ""Text"" denotes input arguments and the annotated implicit connective, and ""Label"" means ground truth implicit relations. Correct predictions are marked in gray background.",
2306.06480v1,Which model performed the best on the Ji dataset for Level 1 (4-way) of PDTB 3.0?,Our Model,2306.06480v1-Table2-1.png, Results on PDTB 3.0 and PCC. Subscripts are the standard deviation of the mean performance.,
2306.06480v1,Which model achieves the highest accuracy in the level 1 classification task?,Our Model achieves the highest accuracy in the level 1 classification task.,2306.06480v1-Figure2-1.png," Level1 classification results on PDTB 2.0 (Ji split) when annotated connectives are fed to connectiveenhanced models. ""Increase"" denotes performance gain compared to the model with default settings (""Base"").",
2306.06480v1,Which model has the highest F1-macro score?,RoBERTaConn.,2306.06480v1-Figure4-1.png," Level1 classification results (F1) on PDTB 2.0 (Ji split) when annotated connectives are fed to models. ""Increase"" denotes performance gain compared to the model with default settings (""Base"").",
2306.06480v1,Which model performs better in terms of F1-macro on PDTB 2.0 (Ji split) when the generated connectives are removed?,"The ""Remove"" model performs better than the ""Base"" model.",2306.06480v1-Figure5-1.png," Level1 classification results (F1) on PDTB 2.0 (Ji split). ""Remove"" denotes the generated connectives are removed from the original model (""Base"").",
2306.06480v1,Which model performed the best on the PDTB 2.0 dataset?,"The RoBERTaConn model performed the best on the PDTB 2.0 dataset, with an accuracy of 96.69% and an F1 score of 95.58%.",2306.06480v1-Table11-1.png, Level1 classification results on PDTB 2.0 (Ji split) when manually-annotated connectives are fed to connectives enhanced models. The numbers in brackets are performance gains compared to the default settings.,
2306.06480v1,What is the learning rate used for training the model?,The learning rate used for training the model is 1e-5.,2306.06480v1-Table10-1.png, Hyperparameters for training our model.,
2306.06480v1,How does the performance of Our Model compare to the Pipeline model on PDTB 2.0?,"Our Model outperforms the Pipeline model on PDTB 2.0, achieving an accuracy of 72.27% and an F1 score of 65.49%, compared to the Pipeline model's accuracy of 58.15% and F1 score of 46.68%.",2306.06480v1-Table12-1.png, Level1 classification results on PDTB 2.0 (Ji split) when generated connectives are removed from Pipeline and Our Method. The numbers in brackets are performance drops compared to the default settings.,
2306.06480v1,What is the purpose of the MASK token in the connective generation module?,The MASK token is used to indicate the position of the connective that the model should generate.,2306.06480v1-Figure1-1.png," An overview of the proposed approach. The left part is the connective generation module which generates a connective at the masked position between arguments (Arg1, Arg2). The right part is the relation classification module which predicts the relation based on both arguments and the generated connective. We share the embedding layer and transformer blocks between two modules and train the whole model in an end-to-end manner.",
2306.06480v1,Which version of PDTB has more data samples in the training set?,PDTB 3.0,2306.06480v1-Table7-1.png," Dataset statistics for the ""Ji"" split.",
2306.06480v1,What is the purpose of the table?,The table shows the number of residues in each fold of the PDB for the different versions of the PDB and the different splits used in cross-validation.,2306.06480v1-Table8-1.png, Dataset statistics in cross-validation (Xval). Numbers are arranged in Dev/Train/Test order. Sections 6-1 denote sections 6-24 and sections 0-1.,
2306.06480v1,What are the second-level relations of PCC?,"The second-level relations of PCC are: Comparison.Concession, Contingency.Cause, Expansion.Equivalence, Expansion.Level-of-detail, Comparison.Contrast, Expansion.Conjunction, Expansion.Instantiation, and Temporal.Asynchronous.",2306.06480v1-Table9-1.png, Second-level (L2) relations of PCC used in our experiments.,
2306.06480v1,What are the different types of relations in PDTB 2.0 and PDTB 3.0?,"The different types of relations in PDTB 2.0 and PDTB 3.0 are Comparison, Contingency, Expansion, and Temporal.",2306.06480v1-Table6-1.png, Top-level (L1) and second-level (L2) relations of PDTB 2.0 and PDTB 3.0 used in our experiments.,
2306.06480v1,Which model performs best on the PDTB 2.0 (Ji split) dataset when connectives are correctly generated?,Our Model,2306.06480v1-Table3-1.png," Level1 classification results on PDTB 2.0 (Ji split) when connectives are correctly and incorrectly generated (or predicted). ""+"" and ""-"" denote the increase and decrease compared to the RoBERTa baseline (Base).",
2306.06480v1,"How does removing the generated connectives affect the performance of the ""Our Model"" pipeline?","Removing the generated connectives decreases the accuracy of the ""Our Model"" pipeline by about 5%.",2306.06480v1-Figure3-1.png," Level1 classification results on PDTB 2.0 (Ji split). ""Remove"" denotes the generated connectives are removed from the original model (""Base"").",
2306.06480v1,Which model performs best on the Expansion.List relation?,Our Model,2306.06480v1-Table4-1.png, F1 results for each second-level relation of PDTB 2.0.,
2306.06480v1,Which model performs the best on PDTB 3.0 in terms of accuracy and F1 score?,Our Model performs the best on PDTB 3.0 in terms of both accuracy and F1 score.,2306.06480v1-Table5-1.png, Ablation study for Scheduled Sampling and connective generation loss LConn.,
2306.11592v1,What is the general trend of the influence of γ on ACC for all four datasets?,The general trend is that ACC decreases as γ increases.,2306.11592v1-Figure4-1.png, The influence of γ on ACC of benchmark datasets.,
2306.11592v1,What is the SRE of the affinity matrix constructed by C on the ORL dataset?,88.92,2306.11592v1-Table4-1.png," The SRE, CONN of affinity matrix constructed by C, θ, C + θ, respectively.",
2306.11592v1,Which module performs the best on the ORL dataset in terms of ACC?,L1 + L2 + L3,2306.11592v1-Table3-1.png, Results of ablation experiments on ORL and COIL20 datasets.,
2306.11592v1, Which dataset uses the most convolutional filters in the encoder?," The Extended Yale B dataset uses the most convolutional filters in the encoder, with a total of 30 filters.",2306.11592v1-Table1-1.png, Convolution layer parameters for each dataset.,
2306.11592v1, What are the differences between the four benchmark datasets shown in the figure?," The four benchmark datasets shown in the figure are Extended Yale B, ORL, Umist, and COIL20. They differ in terms of the number of images, the number of subjects, and the lighting conditions. For example, Extended Yale B has more images and subjects than ORL, and the images in Extended Yale B are taken under more varied lighting conditions than the images in ORL.",2306.11592v1-Figure2-1.png, Example images of the four benchmark datasets.,
2306.11592v1,What is the purpose of the self-supervised module in the proposed network?,The self-supervised module is used to learn a representation of the input data that is useful for downstream tasks.,2306.11592v1-Figure1-1.png," Architecture of the proposed network. It including three section: 1) The input X is mapped to Z through an encoder, and Z is self-expressed by ZC. Then, it was reconstructed as X̂ through a decoder. 2) The C is self-expressed by Cθ. 3) A self-supervised module is proposed according to Z and ZC.",
2306.16201v1,"What is the relationship between the size of a bounding box and its IoU score, as shown in the figure?",The figure shows that the IoU score generally increases with the size of the bounding box.,2306.16201v1-Figure1-1.png," (a) Illustration of the mean IoU for pseudo-labels with three different size intervals (small:[0, 32×32], medium:[32×32, 96×96], large:[96×96, +∞]). (b) IoU versus the box area for pseudo-labels.",
2306.16201v1,Which model performs better in terms of recall and precision on 5% COCO-standard?,LSM(Ours) performs better in terms of both recall and precision.,2306.16201v1-Figure4-1.png, Comparison of the average recall and average precision curves for UBTeacher and LSM(ours) on 5% COCO-standard.,
2306.16201v1,Which model achieves the highest AP50:95 score on the COCO-ImageNet dataset?,PIM (in UBTeacher),2306.16201v1-Table4-1.png, Comparison with Multi-scale Label Consistency (MLC). “∆” indicates that the model is trained using MLC.,
2306.16201v1,Which model performs the best on the COCO-ImageNet benchmark?,Deformable-DETR* (LSM),2306.16201v1-Table3-1.png," Comparison with the state-of-the-arts on COCO-ImageNet. Where “*” represents that the LSM is applied on the corresponding model and “Ω” means that the model is only trained on train2017 in a fully-supervised manner. “Φ” indicates that the model is pretrained with 20% ImageNet, and then finetuned with train2017. The last row represents that our method is solely applied in the pretraining stage.",
2306.16201v1,Which method performs best on the COCO-additional dataset when 100% of the data is labeled?,PseCo + LSM,2306.16201v1-Table1-1.png, Comparison with the state-of-the-arts from different percentages of labeled MS-COCO. The margins of error are reported under 5 different random seeds. Where“-” means the corresponding result is not available.,
2306.16201v1,Which method performs the best on the VOC dataset according to the AP50 metric?,UBTeacher + LSM performs the best on the VOC dataset according to the AP50 metric.,2306.16201v1-Table2-1.png, Comparison with the state-of-the-arts on VOC.,
2306.16201v1,Which of the two pseudo-labels in the image is more likely to be incorrect?,The pseudo-label for the toilet in the bottom left corner of image (a) is more likely to be incorrect.,2306.16201v1-Figure2-1.png," Illustration of pseudo-labels with boundary box, category and classification score. The incorrect pseudo-label (the toilet box in bottom) in (a) has a higher classification score than the correct pseudo-label (bus box) in (b).",
2306.16201v1,What is the effect of adding large objects to the training of UBteacher baseline?,"Adding large objects to the training of UBteacher baseline increases the AP score for all object sizes (small, medium, and large).",2306.16201v1-Table5-1.png," Ablation study on PIM under 1% COCO-standard. The row 1 represents the UBteacher baseline without using P d 2 , P d 3 , P d 4 . The row 2 indicates that large objects exceeding a lower threshold (t > 0.5, area > 96 × 96) are added to the training of UBteacher baseline.",
2306.16201v1,Which model performs better at a threshold of 0.5?,UBteacher performs better at a threshold of 0.5.,2306.16201v1-Figure5-1.png, Ablation study on filtering threshold,
2306.16201v1,How does the performance of UB and DDETR compare when using PIM and PIM+SD methods?,"When using the PIM method, DDETR outperforms UB by 10.35 AP50:95. When using the PIM+SD method, DDETR outperforms UB by 10.6 AP50:95.",2306.16201v1-Table6-1.png, Ablation study on self-distillation.,
2306.16201v1,What are the two main branches of the training method depicted in the image?,The main branch and the pseudo information mining (PIM) branch.,2306.16201v1-Figure3-1.png," An overview of our training method. (a) represents the generation of pseudo-labels. (b) illustrates the pipeline of LSM. The yellow line is the main branch following mean teacher framework and the green line is the forward procedure of PIM. The purple line is original information reused by PIM from the main branch. It refers to the proposals generated by the region proposal network (RPN) in Faster-RCNN, and indexes of bipartite graph matching results from in DDETR. The detector is equipped with self-distillation (SD) to learn complementary predictions from two branches.",
2307.07512v1,What is the difference between the monotonic and unconstrained models when extrapolating beyond the region occupied by the training data?,"The monotonic model is still monotonic and smooth, while the unconstrained model is non-monotonic and exhibits overfitting of the noise.",2307.07512v1-Figure2-1.png," Our monotonic architecture (green) and an unconstrained network (red) trained on two realizations (purple data points) of a one dimensional dataset. The shaded regions are where training data were absent. Each model is trained using 10 random initialization seeds. The dark lines are averages over the seeds, which are each shown as light lines. The unconstrained models exhibit overfitting of the noise, non-monotonic behavior, and highly undesirable and unpredictable results when extrapolating beyond the region occupied by the training data. Conversely, the monotonic Lipschitz models are always monotonic, even in scenarios where the noise is strongly suggestive of non-monotonic behavior. In addition, the Lipschitz constraint produces much smoother models.",
2307.07512v1,How does the efficiency of the monotonic BDT and Lipschitz NN models compare to the unconstrained NN model?,"The monotonic BDT and Lipschitz NN models have a more uniform efficiency than the unconstrained NN model, but at the expense of a few percent lifetime-integrated efficiency.",2307.07512v1-Figure4-1.png, From Kitouni et al. (2021): True positive rate (efficiency) of each model shown in Figure 3 versus the proper lifetime of the decaying heavy-quark particle selected. The monotonic models produce a nearly uniform efficiency above a few picoseconds at the expense of a few percent lifetime-integrated efficiency. Such a trade off is desirable as explained in the text.,
2307.07512v1,Which of the three models has the most distinct decision boundary?,The monotonic BDT.,2307.07512v1-Figure3-1.png," From Kitouni et al. (2021): Simplified version of the heavy-quark selection problem using only two inputs, which permits displaying the response everywhere in the feature space; shown here as a heat map with more signal-like (background-like) regions colored blue (red). The dark solid line shows the decision boundary (upper right regions are selected). Shown are (left) a standard fully connected neural network, (middle) a monotonic BDT, and (right) our architecture. The quantities shown on the horiztonal and vertical axes are related to how long the particle lived before decaying and how massive the particle was, respectively.",
2307.07512v1,Explain how the figure shows that the p-norm constraint with p = 1 allows for more possible gradient configurations than the p-norm constraint with p = 2.,"The figure shows that the p-norm constraint with p = 1 allows for more possible gradient configurations than the p-norm constraint with p = 2 because the green square, which represents the p = 1 constraint, encompasses the red circle, which represents the p = 2 constraint. This means that all of the possible gradient configurations allowed by the p = 2 constraint are also allowed by the p = 1 constraint, but not vice versa.",2307.07512v1-Figure1-1.png," p-norm constrained gradients showing (red) p = 2 and (green) p = 1. The gradient of a function g(x) that is Lipp=2 resides within the dashed red line. For a Lipp=1 function, the boundary is the green dashed line. Note that x is taken to be a row vector. The residual connection (in blue) effectively shifts the possible gradients to strictly positive values and thus enforces monotonicity. Note how the red solid circle does not include all possible gradient configurations. For instance, it does not allow for very small gradients in both inputs, whereas the green square includes all configurations, up to an element-wise maximum of 2λ.",
2307.07512v1,What is the learning rate used to train the MNIST dataset?,10^-5,2307.07512v1-Table2-1.png, Training MNIST and CIFAR10/100 to 100% training accuracy with Lipschitz networks.,
2307.07512v1,Which method has the best test accuracy on the LoanDefaulter dataset?,LMN,2307.07512v1-Table1-1.png," We compare our method (in bold) against state-of-the-art monotonic models (we only show the best) on a variety of benchmarks. The performance numbers for other techniques were taken from Liu et al. (2020) and Sivaraman et al. (2020). In the ChestXRay experiment, we train one model with frozen ResNet18 weights (second to last) and another with end-to-end training (last). While our models can generally get quite small, we can achieve even smaller models when only taking a subset of all the features. These models are denoted with “mini”.",
2307.11494v3,Which value of κ results in the highest probability density for the unnormalized Asymmetric Laplace Distribution?,The highest probability density occurs when κ is close to 0.5.,2307.11494v3-Figure4-1.png, Probability density function (PDF) and negative log-likelihood (NLL) of the unnormalized Asymmetric Laplace Distribution for different quantile levels (κ) with m = 0 and b = 1.,
2307.11494v3, What is the key difference between TSDiff and TSDiff-Cond? ," TSDiff-Cond incorporates a conditioning input, c, through Conv1x1 layers, while TSDiff does not.",2307.11494v3-Figure9-1.png," A schematic of the architecture of the conditional model, TSDiff-Cond. The key difference from TSDiff is the incorporation of the conditioning input, c, through Conv1x1 layers.",
2307.11494v3,"What is the purpose of the ""Refine"" stage in TSDiff?","The ""Refine"" stage in TSDiff is used to improve the predictions of base forecasters by leveraging the implicit probability density of TSDiff.",2307.11494v3-Figure1-1.png," An overview of TSDiff’s use cases. Predict: By utilizing observation self-guidance, TSDiff can be conditioned during inference to perform predictive tasks such as forecasting (see Sec. 3.1). Refine: Predictions of base forecasters can be improved by leveraging the implicit probability density of TSDiff (see Sec. 3.2). Synthesize: Realistic samples generated by TSDiff can be used to train downstream forecasters achieving good performance on real test data (see Sec. 4.3).",
2307.11494v3,Which method performed the best on the UberTLC dataset?,"The TSDiff-q method performed the best on the UberTLC dataset, with a score of 0.161 ± 0.002.",2307.11494v3-Table1-1.png," Forecasting results on eight benchmark datasets. The best and second best models have been shown as bold and underlined, respectively.",
2307.11494v3,How does the forecast horizon affect the accuracy of the forecast?,"The forecast horizon is the length of time into the future that the forecast is made. In the figure, the forecast horizon is indicated by the shaded gray area. The accuracy of the forecast generally decreases as the forecast horizon increases. This is because there is more uncertainty about the future the further out you try to predict.",2307.11494v3-Figure3-1.png," Example forecasts generated by TSDiff-Q for time series in Electricity, KDDCup, and Exchange — three datasets with different frequencies and/or prediction lengths.",
2307.11494v3," Which of the synthetic samples generated by TSDiff, TimeGAN, and TimeVAE is most similar to the real samples? ", The synthetic samples generated by TimeGAN are most similar to the real samples. ,2307.11494v3-Figure10-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the Solar dataset.",
2307.11494v3,Which of the three time series shows the most variation in the ground truth data?,KDDCup,2307.11494v3-Figure5-1.png," Predictions generated by TSDiff-Q for time series in Electricity, KDDCup, and Exchange.",
2307.11494v3,Which dataset requires the least amount of quantile self-guidance?,KDDCup.,2307.11494v3-Table7-1.png," Guidance scales, s, for quantile self-guidance on each dataset.",
2307.11494v3,Which of the synthetic datasets is most similar to the real dataset?,TimeGAN,2307.11494v3-Figure12-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the Traffic dataset.",
2307.11494v3,"How well do the synthetic samples generated by TSDiff, TimeGAN, and TimeVAE match the real samples?","The synthetic samples generated by TSDiff, TimeGAN, and TimeVAE appear to match the real samples reasonably well, although there are some minor differences. For example, the synthetic samples generated by TSDiff appear to be slightly smoother than the real samples, while the synthetic samples generated by TimeGAN and TimeVAE appear to be slightly more noisy.",2307.11494v3-Figure14-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the M4 dataset.",
2307.11494v3,Which of the synthetic datasets most closely resembles the real dataset?,TimeGAN,2307.11494v3-Figure13-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the Exchange dataset.",
2307.11494v3,Which of the synthetic datasets most closely resembles the real dataset?,TSDiff.,2307.11494v3-Figure11-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the Electricity dataset.",
2307.11494v3,Which setting performed the best on the Wikipedia dataset?,The best performing setting on the Wikipedia dataset was Transformer-MS.,2307.11494v3-Table3-1.png," Refinement results on eight benchmark datasets. The best and second best settings have been shown as bold and underlined, respectively.",
2307.11494v3,Which method performs the best on the KDD Cup dataset?,BM-E TSDiff - Cond,2307.11494v3-Table2-1.png, Forecasting with missing values results on six benchmark datasets.,
2307.11494v3, What is the role of the observation self-guidance module in the TSDiff architecture?, The observation self-guidance module helps to guide the denoising process by using the log-likelihood of the observation under a distribution parameterized by the predicted output. ,2307.11494v3-Figure2-1.png," An overview of observation self-guidance. The predicted noise, ϵθ(xt, t), first denoises xt unconditionally as x̂t−1 and approximates y as ŷ. The reverse diffusion step then guides x̂t−1 via the log-likelihood of the observation yobs under a distribution parameterized by ŷ.",
2307.11494v3,Which model has the shortest inference time?,TSDiff-Cond,2307.11494v3-Table8-1.png, Comparison of inference times of TSDiff-Cond and observation self-guidance on the Exchange dataset.,
2307.11494v3,What are the three missing value scenarios considered in the experiments?,"The three missing value scenarios are:
1. Missing values at the beginning of the time series.
2. Missing values in the middle of the time series.
3. Missing values at the end of the time series.",2307.11494v3-Figure6-1.png, Illustration of the three missing value scenarios considered in our experiments.,
2307.11494v3,Which hyperparameter controls the number of steps used in the diffusion process?,Diffusion steps T,2307.11494v3-Table6-1.png, Hyperparameters of TSDiff.,
2307.11494v3,Which model generated the most realistic-looking synthetic samples?,TimeGAN.,2307.11494v3-Figure15-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the UberTLC dataset.",
2307.11494v3,Which of the synthetic data generation methods is the most successful at capturing the overall trends of the real data?,TimeGAN,2307.11494v3-Figure16-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the KDDCup dataset.",
2307.11494v3,"Which of the three generative models, TSDiff, TimeGAN, or TimeVAE, appears to generate the most realistic samples?",TimeGAN appears to generate the most realistic samples.,2307.11494v3-Figure17-1.png," Real samples and synthetic samples generated by TSDiff, TimeGAN, and TimeVAE for the Wikipedia dataset.",
2307.11494v3,Which generative model achieved the best score for forecasting the Wikipedia dataset?,TimeGAN achieved the best score for forecasting the Wikipedia dataset.,2307.11494v3-Table4-1.png, Results of forecasters trained on synthetic samples from different generative models on eight benchmark datasets. Best scores are shown in bold.,
2307.11494v3,Which model performs best on the Solar dataset?,DeepAR and Transformer,2307.11494v3-Figure8-1.png, Variation of the relative CRPS with the number of refinement iterations on the Solar dataset.,
2307.11494v3,Which dataset has the highest average loss per diffusion step?,Wikipedia,2307.11494v3-Figure7-1.png," Loss per diffusion step, t, and representative diffusion step, τ , for the eight datasets used in our experiments.",
2307.11494v3,Which dataset has the longest median sequence length?,The KDD Cup dataset has the longest median sequence length of 10850.,2307.11494v3-Table5-1.png, Overview of the benchmark datasets used in our experiments.,
2307.15934v1,Which method performed the best in terms of AUC?,Our method performed the best in terms of AUC.,2307.15934v1-Table1-1.png," Performance on CMV dataset. Other methods’ results are from DeepRC. Acc. stands for Accuracy. Best results are in bold. Note that for the calculation of the F1 score and accuracy, we select a threshold value based on the validation set.",
2307.15934v1,How well does the model perform on unseen repertoires compared to seen repertoires?,The model performs slightly worse on unseen repertoires compared to seen repertoires.,2307.15934v1-Figure3-1.png," The ROC of associated-sequence identification. Only trained by repertoire-level labels, our model identifies the most associated sequences. In the second row, we let the model make predictions on repertoires that are outside of the training dataset.",
2307.15934v1,What is the difference in the distribution of total frequency between positive and negative subjects?,"The distribution of total frequency is skewed to the right for both positive and negative subjects, but the distribution for positive subjects is more skewed than the distribution for negative subjects. This means that there are more positive subjects with a higher total frequency than negative subjects.",2307.15934v1-Figure2-1.png, The sum of occurrence frequency of the discovered CMVassociated sequences appearing in positive and negative subjects.,
2307.15934v1,What is the effect of using label smoothing on the performance of the proposed method?,Label smoothing generally improves the performance of the proposed method.,2307.15934v1-Figure4-1.png," Our method’s performance (on the validation set) varies with the (a). label correction technique, (b). α in EMA, and (c). β in label smoothing.",
2307.15934v1,Which dataset shows a higher confidence for the positive class?,The Cancer dataset shows a higher confidence for the positive class.,2307.15934v1-Figure5-1.png, The average confidence on two classes’ samples.,
2307.15934v1,Which method performed best on the CMV dataset?,Our method.,2307.15934v1-Table3-1.png, Ablation study. Results (AUC (%) ± std) on CMV and DeepCAT are reported. Best results are in bold.,
2307.15934v1,Which method has the highest average AUC across all sequence lengths?,Our method.,2307.15934v1-Table2-1.png," AUC (%) on Cancer dataset. DeepCAT’s results are from its original paper. Note the last column is the AUC of all sequences, which includes other lengths. Best results are in bold.",
2307.15934v1,What is the purpose of the co-training process in the model training?,The co-training process is used to improve the performance of the model by training two models with different initializations and then using the predictions of each model to help train the other.,2307.15934v1-Figure1-1.png," (a) The robust training process with co-training and label correction. ASA stands for asymmetric self-adaptive label correction, which combines historical predictions and repertoire-level labels to supervise training. Dash lines indicate stop-gradient. (b). The inference process of our method, which can naturally perform disease-associated sequence identification and is ready for repertoire classification (c). Model architecture. Dash lines indicate ""optional"".",
2308.13369v3,Which method has the fastest inference speed?,Method [6] has the fastest inference speed.,2308.13369v3-Table6-1.png, Comparison of inference speed.,
2308.13369v3,What is the purpose of the figure?,"The figure shows the results of a method for recovering body and hand meshes from images. The method is effective even under ambiguity, such as heavy occlusions, and produces high-quality results.",2308.13369v3-Figure2-1.png," Visualization of body and hand mesh outputs using our method. Our method effectively recovers the mesh even under ambiguity (e.g., due to heavy occlusions), and produces high-quality results. (More visualization results are shown in Supplementary.)",
2308.13369v3,Which method produced the most accurate mesh distribution?,Our method.,2308.13369v3-Figure3-1.png, Visualization of mesh distributions generated by different variations of the reverse diffusion process. We observe that the mesh distribution produced by our method (bottom) smoothly converges towards the target.,
2308.13369v3,Which method required the least number of steps?,Ours (with DAT),2308.13369v3-Table4-1.png, Evaluation of impact of DAT.,
2308.13369v3,Which method performed the best in terms of MPVE?,Ours w/ $H_U$,2308.13369v3-Table5-1.png, Evaluation of DAT components.,
2308.13369v3,Which method performs the best according to the table?,PA-MPJPE,2308.13369v3-Table3-1.png, Evaluation of diffusion pipeline.,
2308.13369v3,Which method performs the best on the Human3.6M dataset in terms of MPJPE?,Ours.,2308.13369v3-Table1-1.png, Comparison results on 3DPW and Human3.6M.,
2308.13369v3,Which method achieved the best F@15 mm score?,Our method achieved the best F@15 mm score.,2308.13369v3-Table2-1.png, Comparison results on FreiHAND. The results with an asterisk (*) are reported by [7].,
2308.13369v3,What is the purpose of the Distribution Alignment Technique (DAT) in the Human Mesh Diffusion (HMDiff) framework?,"The Distribution Alignment Technique (DAT) is used to guide the diffusion process by computing a Distribution Alignment Gradient, which effectively infuses prior distribution information to guide the denoising steps.",2308.13369v3-Figure1-1.png," Illustration of the proposed Human Mesh Diffusion (HMDiff) framework with the Distribution Alignment Technique (DAT). Given an RGB image I , we first extract an image feature fI and a prior distribution U , by using pre-trained models ϕI and ϕP . Then, a Transformer-based diffusion model g takes in U , fI and noise from a Gaussian distribution HK , and iteratively performs K denoising diffusion steps to eventually obtain a denoised distribution H0. We use our DAT technique to guide the diffusion process by computing a Distribution Alignment Gradient, which effectively infuses prior distribution information to guide the denoising steps. Lastly, after obtaining the high-quality mesh distribution H0, we take the center of H0 and feed it into an MLP to obtain the final prediction hm.",
2309.00284v1,What are the differences between the multi-singer pre-training step and the single-singer fine-tuning step in the SVS system?,"The main differences are that the fine-tuning step includes additional modules for predicting energy, pitch, and duration, as well as a semantic extraction module. These modules are not present in the pre-training step. Additionally, the prior encoder is only used in the fine-tuning step.",2309.00284v1-Figure1-1.png," The structure of the proposed SVS system, where (a) represents the pre-training stage, while (b) represents the fine-tuning stage. Modules with blue boxes and highlighted backgrounds only exist in the fine-tuning step.",
2309.00284v1,Which component of the proposed model has the greatest impact on the CMOS metric?,Pretrain,2309.00284v1-Table2-1.png, Ablation study results.,
2309.00284v1,Which model performed better in terms of F0 MAE and F0 Correlation?,The proposed model performed better in terms of F0 MAE and F0 Correlation.,2309.00284v1-Table1-1.png, Experimental results in terms of subjective mean opinion score (MOS) with 95% confidence intervals and four objective metrics.,
2309.00284v1,How do the pitch contours of the three systems compare?,"The pitch contours of the three systems are similar, but there are some differences. The Proposed system's pitch contour is closer to the Ground truth than the Baseline system's pitch contour.",2309.00284v1-Figure2-1.png," Visualizations of spectrograms and pitch contours in three systems: Ground truth, Proposed, and Baseline. The blue line represents the pitch contour. The numbers on the right represent the range of pitches (Hz) and the pitch value at the red line.",
2309.08957v2,What is the difference between the images in (a) and (b)?,"The images in (a) are blurry, while the image in (b) is sharp and clear.",2309.08957v2-Figure1-1.png," Given a set of extremely blurred multi-view images (a), our method restores sharp radiance fields and renders clearly deblurred novel views (b).",
2309.08957v2,Which method produces the sharpest image?,Ours,2309.08957v2-Figure5-1.png, Qualitative comparison of deblurring on ExBlur dataset.,
2309.08957v2,How does ExBluRF compare to DeblurNeRF in terms of training time and memory cost?,ExBluRF is more efficient than DeblurNeRF in terms of both training time and memory cost.,2309.08957v2-Figure2-1.png," Training time and GPU memory consumption on “Camellia” shown in Fig 1. Our method, ExBluRF, significantly improves efficiency on both the training time and memory cost with better deblurring performance. N is the number of samples (kernel size) to reconstruct blurry color.",
2309.08957v2,What is the purpose of the figure?,The figure shows the results of different deblurring methods on synthetic data.,2309.08957v2-Figure6-1.png, Qualitative comparison of deblurring on synthetic dataset.,
2309.08957v2,What are the parameters that are specific to ExBlurRF?,"The parameters specific to ExBlurRF are the voxel grid parameters, denoted by ϕ_voxel.",2309.08957v2-Table3-1.png, Notations used for the analysis of memory cost.,
2309.08957v2,How does the memory cost of DeblurNeRF and ExBluRF change as the number of images increases?,"The memory cost of DeblurNeRF increases significantly as the number of images increases, while the memory cost of ExBluRF remains relatively stable.",2309.08957v2-Table2-1.png, Comparison of memory cost of DeblurNeRF [26] and our ExBluRF.,
2309.08957v2,What is the role of the voxel grid in the ExBluRF pipeline?,The voxel grid is used to represent the scene in a way that is efficient for volumetric rendering.,2309.08957v2-Figure3-1.png," The overview of ExBluRF. We incorporate the physical operation that generates camera motion blur in the volume rendering of radiance fields. The blurry RGB color is reproduced by accumulating the rays along the estimated camera trajectory. By minimizing the photo-consistency loss between the accumulated color and input blurry RGB, we obtain sharp radiance fields and the camera trajectories that explain the motion blur of training views. We adopt voxel-based radiance fields to deal with explosive computation when optimizing extremely motion blurred scenes.",
2309.08957v2,What is the effect of using a 2D kernel blur model on the PSNR and GPU memory usage compared to not using any blur model?,Using a 2D kernel blur model increases the PSNR by 4.8 and the GPU memory usage by 10.34 GB.,2309.08957v2-Table4-1.png, Ablation test on the main components in our method. We report ”Camellia” scene of ExBlur dataset.,
2309.08957v2,How does the number of sampling on the camera trajectory affect the deblurring performance?,The deblurring performance increases with the number of sampling on the camera trajectory.,2309.08957v2-Table7-1.png, Effectiveness of the number of sampling on the camera trajectory for deblurring performance.,
2309.08957v2,Which method performed better on the ExBlur dataset?,ExBlueRF performed better on the ExBlur dataset.,2309.08957v2-Table5-1.png, Evaluation of absolute trajectory error on ExBlur and synthetic datasets. We report 2 scenes for each dataset.,
2309.08957v2,How does the performance of the Bézier curve change as its order increases?,"The performance of the Bézier curve generally improves as its order increases, but there is a point of diminishing returns.",2309.08957v2-Table6-1.png," Sensitivity to order of Bézier curve. We uniformly increase the order by {1, 3, 5, 7, 9}. The first-order curve means linear motion in 6-DOF pose space.",
2309.08957v2,What is the purpose of the figure?,The figure shows the results of a blur model trained on synthetic data.,2309.08957v2-Figure4-1.png, (a) Blurry training view of synthetic data. (b) Visualization of the GT (green) and estimated (red) trajectory of (a) by increasing the training iterations. Our blur model converges well to the GT trajectory without sophisticated initialization.,
2309.08957v2,Which method performs best on the ExBlur dataset?,ExBluRF,2309.08957v2-Table1-1.png, Quantitative comparison of novel view synthesis.,
2310.08854v3,Which of the DETR-based detectors achieves the highest AP on COCO val2017 with R50?,Rank-DETR achieves the highest AP (64.0) on COCO val2017 with R50.,2310.08854v3-Table1-1.png, Comparison with previous highly performant DETR-based detectors on COCO val2017 with R50.,
2310.08854v3," 
Which method and backbone combination achieves the highest average precision (AP) for object detection?"," 
Our method with the Swin-L backbone achieves the highest AP of 57.6.",2310.08854v3-Table3-1.png, Improving object detection results based on DINO-DETR.,
2310.08854v3,How does the performance of H-Deformable-DETR compare to the proposed method when using a Swin-L backbone and training for 36 epochs?,The proposed method outperforms H-Deformable-DETR in all metrics.,2310.08854v3-Table2-1.png, Improving object detection results based on H-DETR.,
2310.08854v3,What is the difference between H-DETR and Rank-DETR in terms of their detection results?,"H-DETR produces more false positives (red boxes) than Rank-DETR, as evidenced by the greater number of red boxes in the top row of the figure compared to the bottom row. This indicates that H-DETR is less accurate in identifying objects than Rank-DETR.",2310.08854v3-Figure5-1.png," Illustration of positive boxes and negative box of H-DETR (row1) and Rank-DETR (row2). The green, red and yellow boxes (scores) refer to the positive, negative, and ground truth boxes (scores), respectively.",
2310.08854v3,Which module has the biggest impact on AP@50 when added to the baseline?,HMC (high-order matching cost),2310.08854v3-Table4-1.png, Ablation experiments based on H-DETR + R50. RCH: rank-adaptive classification head. QRL: query rank layer. GCL: GIoU-aware classification loss. HMC: high-order matching cost.,
2310.08854v3,Which classification loss target and matching cost function combination results in the highest average precision (AP) across all IoU thresholds?,The combination of the GloU-aware classification loss target with an exponent of 2 and the matching cost function with an exponent of 4 results in the highest AP across all IoU thresholds.,2310.08854v3-Table5-1.png, The influence of classification loss target and matching cost function choice.,
2310.08854v3,What is the purpose of the query rank layer in the rank-oriented architecture?,The query rank layer exploits the latest ranking information to recreate the content queries and position queries used as the input to the following Transformer decoder layer.,2310.08854v3-Figure1-1.png, Illustrating the rank-oriented architecture design. (a) The rank-oriented architecture consists of a query rank layer before each of the last L− 1 Transformer decoder layer and a rank-adaptive classification head after each Transformer decoder layer. (b) The rank-adaptive classification head learns to adjust the classification scores accordingly. (c) The query rank layer exploits the latest ranking information to recreate the content queries and position queries used as the input to the following Transformer decoder layer.,
2310.08854v3,Which layer has the highest percentage of queries with an IoU score of at least 0.5?,Layer #6.,2310.08854v3-Figure4-1.png, Cumulative distribution of the IoU scores on the unmatched queries with or without HMC.,
2310.08854v3,Which method is more efficient in terms of training cost?,H-DETR,2310.08854v3-Table6-1.png, Computational efficiency analysis for our method.,
2310.08854v3,What is the effect of using QRL on the classification score of matched queries?,Using QRL increases the classification score of matched queries.,2310.08854v3-Figure3-1.png, (a) Comparing the PR-curves between baseline and our method under different IoU thresholds. (b) Density distribution of the classification scores on the matched queries with or without QRL. (c) Density distribution of the classification scores on the unmatched queries with or without QRL.,
2310.08854v3, What is the difference between the original matching cost and loss design and the rank-oriented matching cost and loss design?," The original matching cost and loss design uses a linear combination of the classification scores and bounding box overlap scores to determine the best match between predicted and ground-truth bounding boxes. In contrast, the rank-oriented matching cost and loss design uses a GIoU-aware classification head and high-order matching cost function to prioritize predictions with more accurate localization accuracy.",2310.08854v3-Figure2-1.png, Illustrating the rank-oriented matching cost and loss design. (a) The original DETR and its variants use a classification head and a bounding box regression head to perform predictions. The matching cost function is a linear combination of the classification scores and bounding box overlap scores. (b) The rank-oriented matching cost and loss scheme uses a GIoU-aware classification head and high-order matching cost function to prioritize predictions of more accurate localization accuracy.,
2310.17594v2,Which method performs the best on the R->P task?,SPA (Ours),2310.17594v2-Table1-1.png," Classification Accuracy (%) on DomainNet for unsupervised domain adaptation (inductive), using ResNet101 as backbone. The best accuracy is indicated in bold and the second best one is underlined. Note that we compare methods on the original DomainNet dataset with train/test splits in target dataset, leading to an inductive scenario.",
2310.17594v2,Which setting (A → P or R → A) achieves higher accuracy?,The A → P setting achieves higher accuracy.,2310.17594v2-Figure8-1.png," Convergence. The red curve is the changes of accuracy during the training process and the blue curve is the changes of training loss. The experiments are conducted on OfficeHome dataset, where (a) and (b) are in A → P setting, (c) and (d) are in R → A setting.",
2310.17594v2,Which method performed the best on the Office31 dataset?,SPA (Ours),2310.17594v2-Table3-1.png," Classification Accuracy (%) on (a) Office31 and (b) VisDA2017 for unsupervised domain adaptation, using ResNet50 and ResNet101 as backbone respectively. For VisDA2017, all the results are based on ResNet101 except those with mark †, which are based on ResNet50. The best accuracy is indicated in bold and the second best one is underlined. See supplementary material for more details.",
2310.17594v2,Which method achieves the highest average accuracy?,SPA (Ours),2310.17594v2-Table2-1.png," Classification Accuracy (%) on OfficeHome for unsupervised domain adaptation, using ResNet50 as backbone. The best accuracy is indicated in bold and the second best one is underlined.",
2310.17594v2,What is the purpose of the memory module in the architecture?,"The memory module stores information about past inputs and outputs, which can be used to improve the performance of the model on future inputs.",2310.17594v2-Figure1-1.png," The overall architecture. The final objective integrates supervised classification loss Lcls, domain adversarial loss Ladv, neighbor-aware propagation loss Lnap, and graph spectral alignment loss Lspa.",
2310.17594v2,Which method performs the best for 1-shot and 3-shot semi-supervised domain adaptation on OfficeHome using ResNet34 as backbone?,SPA (Ours) performs the best for both 1-shot and 3-shot semi-supervised domain adaptation on OfficeHome using ResNet34 as backbone.,2310.17594v2-Table8-1.png," Classification Accuracy (%) on OfficeHome for 1-shot and 3-shot semi-supervised domain adaptation, using ResNet34 as backbone. The best accuracy is indicated in bold and the second best one is underlined.",
2310.17594v2,Which method has the highest average accuracy across all six domain adaptation tasks?,The L_sym w/ gauss method has the highest average accuracy of 91.2%.,2310.17594v2-Table9-1.png," Classification Accuracy (%) on OfficeHome for unsupervised domain adaptation. The table is divided into three sections corresponding to robustness analysis, and parameter sensitivity, each separated by a double horizontal line.",
2310.17594v2,Which method achieves the highest average classification accuracy?,L_rwk w/ k = 5,2310.17594v2-Table4-1.png," Classification Accuracy (%) on OfficeHome for unsupervised domain adaptation. The table is divided into three sections corresponding to the three analysis of ablation study, robustness analysis, and parameter sensitivity, each separated by a double horizontal line. More studies on other datasets are in supplementary material.",
2310.17594v2,Which method achieves the highest overall accuracy in the C → R setting?,Our method.,2310.17594v2-Figure4-1.png," Transferability and Discriminability. We compare SPA with DANN [18], BSP [10] and NPL [39] on OfficeHome dataset, where (a) is A-distance in A → C and C → R setting, (b) is accuracy results in A → C setting and (c) is accuracy results in C → R setting.",
2310.17594v2,Which coefficient has a greater impact on accuracy in the C → R setting?,The coefficient of Lgas has a greater impact on accuracy in the C → R setting.,2310.17594v2-Figure3-1.png," Parameter Sensitivity. The line plot of the coefficient of Lnap and the coefficient of Lgsa change from 0.1 to 0.9, leading to different accuracy results. The experiments are conducted on OfficeHome dataset, where (a) is A → C setting and (b) is C → R setting.",
2310.17594v2,Which method has the highest accuracy for 3-shot semi-supervised domain adaptation on DomainNet126?,SPA (Ours),2310.17594v2-Table7-1.png," Classification Accuracy (%) on DomainNet126 for 1-shot and 3-shot semi-supervised domain adaptation, using ResNet34 as backbone. The best accuracy is indicated in bold and the second best one is underlined.",
2310.17594v2,Which method performs best on average across all categories?,SPA (Ours),2310.17594v2-Table5-1.png," Per-category Accuracy (%) on VisDA2017 for unsupervised domain adaptation, using ResNet101 as backbone. All the results are based on ResNet101 except those with mark †, which are based on ResNet50. The best accuracy is indicated in bold and the second best one is underlined.",
2310.17594v2,Which method performs the best on average across all tasks?,SPA (Ours),2310.17594v2-Table6-1.png," Classification Accuracy (%) on DomainNet126 for unsupervised domain adaptation, using ResNet101 as backbone. The best accuracy is indicated in bold and the second best one is underlined.",
2310.17594v2,"Which method appears to have the best domain alignment, based on the t-SNE visualization?",SPA,2310.17594v2-Figure2-1.png," Feature Visualization. the t-SNE plot of DANN [18], BSP [10], NPL [39], and SPA features on OfficeHome dataset in the C → R setting. We use red markers for source domain features and blue markers for target domain features.",
2310.17594v2,Which method seems to have the best domain alignment?,SPA,2310.17594v2-Figure5-1.png," Feature Visualization. the t-SNE plot of DANN [18], BSP [13], NPL [39], and SPA features on office31 dataset in the A → D setting. We use red markers for source domain features and blue markers for target domain features.",
2310.17594v2,"Which of the four methods, DANN, BSP, NPL, or SPA, appears to be the most successful at aligning the source and target domain features?",SPA,2310.17594v2-Figure6-1.png," Feature Visualization. the t-SNE plot of DANN [18], BSP [13], NPL [39], and SPA features on Office31 dataset in the A → W setting. We use red markers for source domain features and blue markers for target domain features.",
2310.17594v2,Which method seems to best align the source and target domain features?,"Based on the t-SNE plot, DANN seems to best align the source and target domain features.",2310.17594v2-Figure7-1.png," Feature Visualization. the t-SNE plot of DANN [18], BSP [13], NPL [39], and SPA features on OfficeHome dataset in the A → C setting. We use red markers for source domain features and blue markers for target domain features.",
