{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f22e47c16bf74becb9ee8c66533f6544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_950bc48484bc4599bd336c38835cf6c8",
              "IPY_MODEL_32b920f7af654a969f4f563602c474b4",
              "IPY_MODEL_0ad1c38e6b564cce8f7a2ec50134b6c5"
            ],
            "layout": "IPY_MODEL_27d28715c98e4efe979cf95689c837f6"
          }
        },
        "950bc48484bc4599bd336c38835cf6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2668f38abe14ace8b0f3a8eac477fc4",
            "placeholder": "​",
            "style": "IPY_MODEL_950a8b4f76f347389579845b34e3f308",
            "value": "test-A%2FSPIQA_testA.json: 100%"
          }
        },
        "32b920f7af654a969f4f563602c474b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d782855eea458ca2aeed027b54424c",
            "max": 777983,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9c798cccefc4aeb8071423a85799322",
            "value": 777983
          }
        },
        "0ad1c38e6b564cce8f7a2ec50134b6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_026a6f6e5fcf4a518a7b2daa1fb1e3d5",
            "placeholder": "​",
            "style": "IPY_MODEL_d15ce05598544a2883a2391c7bd7e497",
            "value": " 778k/778k [00:00&lt;00:00, 28.4MB/s]"
          }
        },
        "27d28715c98e4efe979cf95689c837f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2668f38abe14ace8b0f3a8eac477fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950a8b4f76f347389579845b34e3f308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2d782855eea458ca2aeed027b54424c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9c798cccefc4aeb8071423a85799322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "026a6f6e5fcf4a518a7b2daa1fb1e3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d15ce05598544a2883a2391c7bd7e497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61cb383003494b88982e4858bee8cb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e988d44d385d43c88630303024aa350b",
              "IPY_MODEL_d6478444fca04d4f993b95421e69d9f0",
              "IPY_MODEL_4bf333a6e927440294227c745ee3c2aa"
            ],
            "layout": "IPY_MODEL_803d8237b429475ebe32db08d6ac207e"
          }
        },
        "e988d44d385d43c88630303024aa350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbfa0201c1da4756804871b87bf775c5",
            "placeholder": "​",
            "style": "IPY_MODEL_0bd143d42e3c41dda8fc80b1bd24b714",
            "value": "SPIQA_testA_Images_224px.zip: 100%"
          }
        },
        "d6478444fca04d4f993b95421e69d9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8976eb3c7abe4a9fb85abd9f2b050d0b",
            "max": 47111250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84135932c7b0453abbf9676d9960fba1",
            "value": 47111250
          }
        },
        "4bf333a6e927440294227c745ee3c2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e65129237534cb9bf373bd422c6ff74",
            "placeholder": "​",
            "style": "IPY_MODEL_2f3c0fd0ae7a4a0681986b260a856cfd",
            "value": " 47.1M/47.1M [00:01&lt;00:00, 42.8MB/s]"
          }
        },
        "803d8237b429475ebe32db08d6ac207e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbfa0201c1da4756804871b87bf775c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd143d42e3c41dda8fc80b1bd24b714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8976eb3c7abe4a9fb85abd9f2b050d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84135932c7b0453abbf9676d9960fba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e65129237534cb9bf373bd422c6ff74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3c0fd0ae7a4a0681986b260a856cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcEkSjUj1ZPE",
        "outputId": "697b39e9-e5f7-47c0-cab6-b5b576ab4ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfpvlhH-23y2",
        "outputId": "cb3121c5-4994-4eb4-ecee-5bad9d37a545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.48.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert-score\n",
            "Successfully installed bert-score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_zoZuQVoruF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "f22e47c16bf74becb9ee8c66533f6544",
            "950bc48484bc4599bd336c38835cf6c8",
            "32b920f7af654a969f4f563602c474b4",
            "0ad1c38e6b564cce8f7a2ec50134b6c5",
            "27d28715c98e4efe979cf95689c837f6",
            "a2668f38abe14ace8b0f3a8eac477fc4",
            "950a8b4f76f347389579845b34e3f308",
            "e2d782855eea458ca2aeed027b54424c",
            "a9c798cccefc4aeb8071423a85799322",
            "026a6f6e5fcf4a518a7b2daa1fb1e3d5",
            "d15ce05598544a2883a2391c7bd7e497",
            "61cb383003494b88982e4858bee8cb78",
            "e988d44d385d43c88630303024aa350b",
            "d6478444fca04d4f993b95421e69d9f0",
            "4bf333a6e927440294227c745ee3c2aa",
            "803d8237b429475ebe32db08d6ac207e",
            "cbfa0201c1da4756804871b87bf775c5",
            "0bd143d42e3c41dda8fc80b1bd24b714",
            "8976eb3c7abe4a9fb85abd9f2b050d0b",
            "84135932c7b0453abbf9676d9960fba1",
            "3e65129237534cb9bf373bd422c6ff74",
            "2f3c0fd0ae7a4a0681986b260a856cfd"
          ]
        },
        "outputId": "0249588c-7a9c-4401-d5be-20bb6458fd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-A%2FSPIQA_testA.json:   0%|          | 0.00/778k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22e47c16bf74becb9ee8c66533f6544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SPIQA_testA_Images_224px.zip:   0%|          | 0.00/47.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61cb383003494b88982e4858bee8cb78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'test-A/SPIQA_testA_Images_224px.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(repo_id=\"google/spiqa\", filename=\"test-A/SPIQA_testA.json\", repo_type=\"dataset\", local_dir='.') ### Mention the local directory path\n",
        "hf_hub_download(repo_id=\"google/spiqa\", filename=\"test-A/SPIQA_testA_Images_224px.zip\", repo_type=\"dataset\", local_dir='.') ### Mention the local directory path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the tar file below and put it in test-A directory\n",
        "!unzip -q ./test-A/SPIQA_testA_Images_224px.zip"
      ],
      "metadata": {
        "id": "WL1KiEbVwzSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "testA_metadata = json.load(open('test-A/SPIQA_testA.json', 'r'))\n",
        "paper_id = '1702.03584v3'\n",
        "print(testA_metadata[paper_id]['qa'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcXPPzz_rQ1f",
        "outputId": "c4c90506-69c4-409f-d2f2-4747921a249e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'question': 'How does the observed error compare to the underlying true error as CPU time increases?', 'answer': 'The observed error is initially higher than the underlying true error, but it quickly decreases and converges to the true error as CPU time increases.', 'explanation': 'The figure shows two lines, one representing the observed error and the other representing the underlying true error. Both lines decrease as CPU time increases, but the observed error line decreases more quickly and eventually converges to the true error line.', 'reference': '1702.03584v3-Figure1-1.png'}, {'question': 'How does the performance of SPIRAL-DTW-kMeans compare to k-Shape and CLDS?', 'answer': 'SPIRAL-DTW-kMeans performs better than k-Shape and CLDS on most datasets.', 'explanation': 'In Figure (a) and (b), the circles below the diagonal line indicate datasets where SPIRAL-DTW-kMeans yields better clustering performance in terms of NMI than k-Shape and CLDS, respectively.', 'reference': '1702.03584v3-Figure2-1.png'}, {'question': 'Which method performs the best in terms of NMI and what percentage of datasets does it outperform the other methods on?', 'answer': 'The SPIRAL-MSM-kMeans method performs the best in terms of NMI with a score of 0.365. It outperforms the other methods on 89.4% of the datasets.', 'explanation': 'The table shows the NMI scores and the percentage of datasets on which each method outperforms the other methods. The SPIRAL-MSM-kMeans method has the highest NMI score and the highest percentage of datasets on which it outperforms the other methods.', 'reference': '1702.03584v3-Table1-1.png'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the provided python file\n",
        "!python llava-1.5_qa_test-a_evaluation_image+caption.py --response_root ./responses --image_resolution 224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJraJe7yXhP",
        "outputId": "3bc1ebef-f1e9-4d3c-facc-afa2ce03ff19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-23 15:15:46.389883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740323746.625725    1819 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740323746.692627    1819 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-23 15:15:47.186182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.15MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 4.15MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 3.19MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 100% 1.45k/1.45k [00:00<00:00, 9.22MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 45.5MB/s]\n",
            "tokenizer.json: 100% 3.62M/3.62M [00:00<00:00, 15.0MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 293kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 3.37MB/s]\n",
            "config.json: 100% 950/950 [00:00<00:00, 7.10MB/s]\n",
            "model.safetensors.index.json: 100% 70.1k/70.1k [00:00<00:00, 125MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 10.5M/4.99G [00:00<02:16, 36.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 21.0M/4.99G [00:00<02:12, 37.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 31.5M/4.99G [00:00<02:10, 38.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 41.9M/4.99G [00:01<02:13, 37.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 52.4M/4.99G [00:01<02:15, 36.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 62.9M/4.99G [00:01<02:16, 36.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 73.4M/4.99G [00:01<02:13, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 83.9M/4.99G [00:02<02:12, 37.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 94.4M/4.99G [00:02<02:13, 36.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 105M/4.99G [00:02<02:18, 35.4MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 115M/4.99G [00:03<02:17, 35.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 126M/4.99G [00:03<02:22, 34.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 136M/4.99G [00:03<02:19, 34.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 147M/4.99G [00:04<02:15, 35.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 157M/4.99G [00:04<02:10, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 168M/4.99G [00:04<02:06, 38.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 178M/4.99G [00:04<02:02, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 189M/4.99G [00:05<01:58, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 199M/4.99G [00:05<01:57, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 210M/4.99G [00:05<01:56, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 220M/4.99G [00:05<01:57, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 231M/4.99G [00:06<01:59, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 241M/4.99G [00:06<02:02, 38.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 252M/4.99G [00:06<02:15, 35.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 262M/4.99G [00:07<02:13, 35.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 273M/4.99G [00:07<02:07, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 283M/4.99G [00:07<02:05, 37.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 294M/4.99G [00:07<02:01, 38.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 304M/4.99G [00:08<01:58, 39.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 315M/4.99G [00:08<01:55, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 325M/4.99G [00:08<01:54, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 336M/4.99G [00:08<01:55, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 346M/4.99G [00:09<01:56, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 357M/4.99G [00:09<01:55, 40.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 367M/4.99G [00:09<01:57, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 377M/4.99G [00:09<01:58, 39.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 388M/4.99G [00:10<01:56, 39.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 398M/4.99G [00:10<01:58, 38.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 409M/4.99G [00:10<02:19, 32.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 419M/4.99G [00:11<02:10, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 430M/4.99G [00:11<02:04, 36.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 440M/4.99G [00:11<02:03, 36.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 451M/4.99G [00:11<01:59, 37.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 461M/4.99G [00:12<01:59, 37.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 472M/4.99G [00:12<01:56, 38.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 482M/4.99G [00:12<01:54, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 493M/4.99G [00:12<01:52, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 503M/4.99G [00:13<01:52, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 514M/4.99G [00:13<01:52, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 524M/4.99G [00:13<01:52, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 535M/4.99G [00:14<01:55, 38.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 545M/4.99G [00:14<01:54, 38.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 556M/4.99G [00:14<02:00, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 566M/4.99G [00:14<01:59, 37.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 577M/4.99G [00:15<01:59, 36.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 587M/4.99G [00:15<02:02, 35.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 598M/4.99G [00:15<02:03, 35.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 608M/4.99G [00:16<02:00, 36.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 619M/4.99G [00:16<01:58, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 629M/4.99G [00:16<01:56, 37.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 640M/4.99G [00:16<01:55, 37.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 650M/4.99G [00:17<01:54, 38.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 661M/4.99G [00:17<01:50, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 671M/4.99G [00:17<01:48, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 682M/4.99G [00:17<01:45, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 692M/4.99G [00:18<01:43, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 703M/4.99G [00:18<01:42, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 713M/4.99G [00:18<01:41, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 724M/4.99G [00:18<01:40, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 734M/4.99G [00:19<01:40, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 744M/4.99G [00:19<01:39, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 755M/4.99G [00:19<01:39, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 765M/4.99G [00:19<01:38, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 776M/4.99G [00:20<01:38, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 786M/4.99G [00:20<01:38, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 797M/4.99G [00:20<01:37, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 807M/4.99G [00:20<01:37, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 818M/4.99G [00:21<01:37, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 828M/4.99G [00:21<01:37, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 839M/4.99G [00:21<01:36, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 849M/4.99G [00:21<01:36, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 860M/4.99G [00:22<01:36, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 870M/4.99G [00:22<01:35, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 881M/4.99G [00:22<01:54, 36.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 891M/4.99G [00:22<01:48, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 902M/4.99G [00:23<01:44, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 912M/4.99G [00:23<01:41, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 923M/4.99G [00:23<01:38, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 933M/4.99G [00:24<01:43, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 944M/4.99G [00:24<01:40, 40.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 954M/4.99G [00:24<01:38, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 965M/4.99G [00:24<01:37, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 975M/4.99G [00:25<01:36, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 986M/4.99G [00:25<01:36, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 996M/4.99G [00:25<01:36, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:25<01:37, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.02G/4.99G [00:26<01:35, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.03G/4.99G [00:26<01:35, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.04G/4.99G [00:26<01:34, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.05G/4.99G [00:26<01:34, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.06G/4.99G [00:27<01:33, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.07G/4.99G [00:27<01:35, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.08G/4.99G [00:27<01:36, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.09G/4.99G [00:27<01:34, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.10G/4.99G [00:28<01:33, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.11G/4.99G [00:28<01:32, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.12G/4.99G [00:28<01:35, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.13G/4.99G [00:28<01:41, 38.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:29<01:37, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.15G/4.99G [00:29<01:35, 40.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.16G/4.99G [00:29<01:33, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.17G/4.99G [00:29<01:31, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.18G/4.99G [00:30<01:30, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.20G/4.99G [00:30<01:32, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:30<01:30, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.22G/4.99G [00:30<01:30, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.23G/4.99G [00:31<01:29, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.24G/4.99G [00:31<01:28, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.25G/4.99G [00:31<01:27, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.26G/4.99G [00:31<01:27, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.27G/4.99G [00:32<01:27, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.99G [00:32<01:26, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.29G/4.99G [00:32<01:27, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.30G/4.99G [00:32<01:27, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.31G/4.99G [00:33<01:26, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.32G/4.99G [00:33<01:26, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.33G/4.99G [00:33<01:25, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:33<01:25, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.35G/4.99G [00:34<01:25, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.36G/4.99G [00:34<01:24, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.37G/4.99G [00:34<01:24, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.38G/4.99G [00:34<01:24, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.39G/4.99G [00:35<01:24, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:35<01:24, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.42G/4.99G [00:35<01:24, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.43G/4.99G [00:35<01:42, 34.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.44G/4.99G [00:36<01:32, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.45G/4.99G [00:36<01:29, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.46G/4.99G [00:36<01:26, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.47G/4.99G [00:36<01:25, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:37<01:24, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.49G/4.99G [00:37<01:23, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.50G/4.99G [00:37<01:22, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.51G/4.99G [00:37<01:21, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.52G/4.99G [00:38<01:21, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.53G/4.99G [00:38<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:38<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.55G/4.99G [00:38<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.56G/4.99G [00:39<01:19, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.57G/4.99G [00:39<01:19, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.58G/4.99G [00:39<01:19, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.59G/4.99G [00:39<01:18, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.60G/4.99G [00:40<01:18, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:40<01:18, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.63G/4.99G [00:40<01:18, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.64G/4.99G [00:40<01:18, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.65G/4.99G [00:41<01:17, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.66G/4.99G [00:41<01:17, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.67G/4.99G [00:41<01:17, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:41<01:17, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.69G/4.99G [00:42<01:16, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.70G/4.99G [00:42<01:16, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.71G/4.99G [00:42<01:16, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.72G/4.99G [00:42<01:16, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.73G/4.99G [00:42<01:15, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:43<01:15, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.75G/4.99G [00:43<01:15, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.76G/4.99G [00:43<01:14, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.77G/4.99G [00:43<01:16, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.78G/4.99G [00:44<01:15, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.79G/4.99G [00:44<01:14, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.80G/4.99G [00:44<01:14, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.81G/4.99G [00:44<01:14, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.82G/4.99G [00:45<01:13, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.84G/4.99G [00:45<01:13, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.85G/4.99G [00:45<01:12, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.86G/4.99G [00:45<01:12, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.87G/4.99G [00:46<01:26, 36.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.88G/4.99G [00:46<01:21, 38.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.89G/4.99G [00:46<01:18, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.90G/4.99G [00:47<01:16, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.91G/4.99G [00:47<01:14, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.92G/4.99G [00:47<01:13, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.93G/4.99G [00:47<01:12, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.94G/4.99G [00:48<01:12, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.99G [00:48<01:11, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.96G/4.99G [00:48<01:11, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.97G/4.99G [00:48<01:10, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.98G/4.99G [00:48<01:10, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.99G/4.99G [00:49<01:10, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 2.00G/4.99G [00:49<01:09, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [00:49<01:09, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.02G/4.99G [00:49<01:08, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.03G/4.99G [00:50<01:08, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.04G/4.99G [00:50<01:08, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.06G/4.99G [00:50<01:08, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.07G/4.99G [00:50<01:07, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [00:51<01:08, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.09G/4.99G [00:51<01:07, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.10G/4.99G [00:51<01:07, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.11G/4.99G [00:51<01:07, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.12G/4.99G [00:52<01:07, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.13G/4.99G [00:52<01:07, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.14G/4.99G [00:52<01:08, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [00:52<01:07, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.16G/4.99G [00:53<01:08, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.17G/4.99G [00:53<01:07, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.18G/4.99G [00:53<01:06, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.19G/4.99G [00:53<01:05, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.20G/4.99G [00:54<01:05, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [00:54<01:04, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.22G/4.99G [00:54<01:04, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.23G/4.99G [00:54<01:04, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.24G/4.99G [00:55<01:03, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.25G/4.99G [00:55<01:03, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.26G/4.99G [00:55<01:03, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.99G [00:55<01:03, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.29G/4.99G [00:56<01:03, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.30G/4.99G [00:56<01:03, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.31G/4.99G [00:56<01:02, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.32G/4.99G [00:56<01:02, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.33G/4.99G [00:57<01:02, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.34G/4.99G [00:57<01:02, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [00:57<01:01, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.36G/4.99G [00:57<01:01, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.37G/4.99G [00:58<01:01, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.38G/4.99G [00:58<01:00, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.39G/4.99G [00:58<01:00, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.40G/4.99G [00:58<00:59, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.41G/4.99G [00:59<00:59, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.42G/4.99G [00:59<00:59, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.43G/4.99G [00:59<00:59, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.44G/4.99G [00:59<00:59, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.45G/4.99G [01:00<00:59, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.46G/4.99G [01:00<01:01, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.47G/4.99G [01:00<01:00, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.49G/4.99G [01:00<01:00, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.50G/4.99G [01:01<00:59, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.51G/4.99G [01:01<00:59, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.52G/4.99G [01:01<00:59, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.53G/4.99G [01:01<00:58, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.54G/4.99G [01:02<00:57, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [01:02<00:57, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.56G/4.99G [01:02<00:57, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.57G/4.99G [01:02<00:57, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.58G/4.99G [01:03<00:56, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.59G/4.99G [01:03<00:56, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.60G/4.99G [01:03<00:55, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.61G/4.99G [01:03<00:55, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.99G [01:04<00:55, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.63G/4.99G [01:04<00:54, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.64G/4.99G [01:04<00:54, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.65G/4.99G [01:04<00:54, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.66G/4.99G [01:04<00:54, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.67G/4.99G [01:05<00:53, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [01:05<00:53, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.69G/4.99G [01:05<00:53, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.71G/4.99G [01:06<01:03, 36.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.72G/4.99G [01:06<01:00, 37.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.73G/4.99G [01:06<00:57, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.74G/4.99G [01:06<00:55, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.75G/4.99G [01:07<00:54, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.76G/4.99G [01:07<00:53, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.77G/4.99G [01:07<00:52, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.78G/4.99G [01:07<00:52, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.79G/4.99G [01:08<00:52, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.80G/4.99G [01:08<00:51, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.81G/4.99G [01:08<00:51, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.82G/4.99G [01:08<00:50, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.83G/4.99G [01:09<00:51, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.84G/4.99G [01:09<00:50, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.85G/4.99G [01:09<00:50, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.86G/4.99G [01:09<00:50, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.87G/4.99G [01:10<00:49, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.99G [01:10<00:49, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.89G/4.99G [01:10<00:49, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.90G/4.99G [01:10<00:48, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.92G/4.99G [01:11<00:48, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.93G/4.99G [01:11<00:49, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.94G/4.99G [01:11<00:48, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.95G/4.99G [01:11<00:48, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.96G/4.99G [01:12<00:47, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.97G/4.99G [01:12<00:47, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.98G/4.99G [01:12<00:47, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.99G/4.99G [01:12<00:47, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.00G/4.99G [01:12<00:46, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.01G/4.99G [01:13<00:46, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.02G/4.99G [01:13<00:46, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.03G/4.99G [01:13<00:45, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.04G/4.99G [01:13<00:45, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.05G/4.99G [01:14<00:45, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.06G/4.99G [01:14<00:44, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.07G/4.99G [01:14<00:44, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.99G [01:14<00:44, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.09G/4.99G [01:15<00:43, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.10G/4.99G [01:15<00:43, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.11G/4.99G [01:15<00:43, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.12G/4.99G [01:15<00:43, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.14G/4.99G [01:16<00:42, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.15G/4.99G [01:16<00:42, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.16G/4.99G [01:16<00:42, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.17G/4.99G [01:16<00:42, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.18G/4.99G [01:17<00:42, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.19G/4.99G [01:17<00:42, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.20G/4.99G [01:17<00:41, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.21G/4.99G [01:17<00:41, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.22G/4.99G [01:18<00:41, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.23G/4.99G [01:18<00:41, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.24G/4.99G [01:18<00:40, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.25G/4.99G [01:18<00:40, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.26G/4.99G [01:19<00:40, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.27G/4.99G [01:19<00:39, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.28G/4.99G [01:19<00:39, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.99G [01:19<00:39, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.30G/4.99G [01:20<00:39, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.31G/4.99G [01:20<00:38, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.32G/4.99G [01:20<00:38, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.33G/4.99G [01:20<00:38, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.34G/4.99G [01:21<00:38, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.36G/4.99G [01:21<00:37, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.37G/4.99G [01:21<00:37, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.38G/4.99G [01:21<00:37, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.39G/4.99G [01:21<00:37, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.40G/4.99G [01:22<00:36, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.41G/4.99G [01:22<00:36, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.42G/4.99G [01:22<00:36, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.43G/4.99G [01:22<00:36, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.44G/4.99G [01:23<00:35, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.45G/4.99G [01:23<00:35, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.46G/4.99G [01:23<00:35, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.47G/4.99G [01:23<00:35, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.48G/4.99G [01:24<00:35, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [01:24<00:34, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.50G/4.99G [01:24<00:34, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.51G/4.99G [01:24<00:34, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.52G/4.99G [01:25<00:34, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.53G/4.99G [01:25<00:34, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.54G/4.99G [01:25<00:33, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.55G/4.99G [01:25<00:33, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.57G/4.99G [01:26<00:33, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.58G/4.99G [01:26<00:33, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.59G/4.99G [01:26<00:32, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.60G/4.99G [01:26<00:32, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.61G/4.99G [01:27<00:32, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.62G/4.99G [01:27<00:31, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.63G/4.99G [01:27<00:38, 35.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.64G/4.99G [01:28<00:35, 37.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.65G/4.99G [01:28<00:34, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.66G/4.99G [01:28<00:33, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.67G/4.99G [01:28<00:32, 40.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.68G/4.99G [01:29<00:31, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [01:29<00:31, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.70G/4.99G [01:29<00:30, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.71G/4.99G [01:29<00:30, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.72G/4.99G [01:29<00:30, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.73G/4.99G [01:30<00:29, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.74G/4.99G [01:30<00:29, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.75G/4.99G [01:30<00:29, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:30<00:28, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.77G/4.99G [01:31<00:28, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.79G/4.99G [01:31<00:28, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.80G/4.99G [01:31<00:28, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.81G/4.99G [01:31<00:27, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.82G/4.99G [01:32<00:27, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.83G/4.99G [01:32<00:27, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.84G/4.99G [01:32<00:27, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.85G/4.99G [01:32<00:26, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.86G/4.99G [01:33<00:26, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.87G/4.99G [01:33<00:26, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.88G/4.99G [01:33<00:26, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [01:33<00:25, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.90G/4.99G [01:34<00:25, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.91G/4.99G [01:34<00:25, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.92G/4.99G [01:34<00:24, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.93G/4.99G [01:34<00:24, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.94G/4.99G [01:35<00:24, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.95G/4.99G [01:35<00:24, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:35<00:23, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.97G/4.99G [01:35<00:23, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.98G/4.99G [01:36<00:23, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 4.00G/4.99G [01:36<00:23, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 4.01G/4.99G [01:36<00:22, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 4.02G/4.99G [01:36<00:22, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.99G [01:37<00:22, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.04G/4.99G [01:37<00:22, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.05G/4.99G [01:37<00:21, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.06G/4.99G [01:37<00:21, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.07G/4.99G [01:38<00:21, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.08G/4.99G [01:38<00:21, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:38<00:20, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.10G/4.99G [01:38<00:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.11G/4.99G [01:39<00:20, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.12G/4.99G [01:39<00:20, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.13G/4.99G [01:39<00:20, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.14G/4.99G [01:39<00:19, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.15G/4.99G [01:40<00:19, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:40<00:19, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.17G/4.99G [01:40<00:19, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.18G/4.99G [01:40<00:22, 36.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.19G/4.99G [01:41<00:20, 38.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.20G/4.99G [01:41<00:20, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.22G/4.99G [01:41<00:19, 40.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.23G/4.99G [01:41<00:18, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.24G/4.99G [01:42<00:18, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.25G/4.99G [01:42<00:17, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.26G/4.99G [01:42<00:17, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.27G/4.99G [01:42<00:17, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.28G/4.99G [01:43<00:16, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.29G/4.99G [01:43<00:16, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.30G/4.99G [01:43<00:16, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.31G/4.99G [01:43<00:15, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.32G/4.99G [01:44<00:15, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.33G/4.99G [01:44<00:15, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.34G/4.99G [01:44<00:15, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.35G/4.99G [01:44<00:15, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.36G/4.99G [01:45<00:14, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.37G/4.99G [01:45<00:14, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.38G/4.99G [01:45<00:14, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.39G/4.99G [01:45<00:13, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.40G/4.99G [01:46<00:13, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.41G/4.99G [01:46<00:13, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.42G/4.99G [01:46<00:13, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.44G/4.99G [01:46<00:12, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.45G/4.99G [01:47<00:12, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.46G/4.99G [01:47<00:12, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.47G/4.99G [01:47<00:12, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.48G/4.99G [01:47<00:11, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.49G/4.99G [01:48<00:11, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.50G/4.99G [01:48<00:11, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.51G/4.99G [01:48<00:11, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.99G [01:48<00:11, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.53G/4.99G [01:48<00:10, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.54G/4.99G [01:49<00:10, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.55G/4.99G [01:49<00:10, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.56G/4.99G [01:49<00:10, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.57G/4.99G [01:49<00:09, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.58G/4.99G [01:50<00:09, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [01:50<00:09, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.60G/4.99G [01:50<00:09, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.61G/4.99G [01:50<00:08, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.62G/4.99G [01:51<00:08, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [01:51<00:08, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.65G/4.99G [01:51<00:08, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [01:51<00:07, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.67G/4.99G [01:52<00:07, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.68G/4.99G [01:52<00:07, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.69G/4.99G [01:52<00:07, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.70G/4.99G [01:52<00:06, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.71G/4.99G [01:53<00:06, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:53<00:06, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.73G/4.99G [01:53<00:06, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.74G/4.99G [01:53<00:05, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.75G/4.99G [01:54<00:05, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.76G/4.99G [01:54<00:05, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.77G/4.99G [01:54<00:05, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.78G/4.99G [01:54<00:04, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:55<00:04, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.80G/4.99G [01:55<00:04, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.81G/4.99G [01:55<00:04, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.82G/4.99G [01:55<00:03, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.83G/4.99G [01:56<00:03, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.84G/4.99G [01:56<00:03, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.85G/4.99G [01:56<00:03, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.87G/4.99G [01:56<00:02, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.88G/4.99G [01:57<00:02, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.89G/4.99G [01:57<00:02, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.90G/4.99G [01:57<00:02, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.91G/4.99G [01:57<00:01, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.92G/4.99G [01:58<00:01, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:58<00:01, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.94G/4.99G [01:58<00:01, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.95G/4.99G [01:58<00:01, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.96G/4.99G [01:58<00:00, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.97G/4.99G [01:59<00:00, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.98G/4.99G [01:59<00:00, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:59<00:00, 41.7MB/s]\n",
            "Downloading shards:  33% 1/3 [02:00<04:00, 120.24s/it]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 10.5M/4.96G [00:00<01:58, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 21.0M/4.96G [00:00<01:55, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 31.5M/4.96G [00:00<01:54, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 41.9M/4.96G [00:00<01:54, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 52.4M/4.96G [00:01<01:54, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 62.9M/4.96G [00:01<01:54, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 73.4M/4.96G [00:01<01:54, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 83.9M/4.96G [00:01<01:53, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 94.4M/4.96G [00:02<01:53, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 105M/4.96G [00:02<01:52, 43.0MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 115M/4.96G [00:02<01:52, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 126M/4.96G [00:02<01:51, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 136M/4.96G [00:03<01:51, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 147M/4.96G [00:03<01:52, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 157M/4.96G [00:03<01:52, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 168M/4.96G [00:03<01:51, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 178M/4.96G [00:04<01:50, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 189M/4.96G [00:04<01:50, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 199M/4.96G [00:04<01:51, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 210M/4.96G [00:04<01:50, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 220M/4.96G [00:05<01:50, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 231M/4.96G [00:05<01:50, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 241M/4.96G [00:05<01:50, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 252M/4.96G [00:05<01:49, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 262M/4.96G [00:06<01:49, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 273M/4.96G [00:06<01:49, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 283M/4.96G [00:06<01:48, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 294M/4.96G [00:06<01:48, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 304M/4.96G [00:07<01:48, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 315M/4.96G [00:07<01:48, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 325M/4.96G [00:07<01:48, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 336M/4.96G [00:07<01:47, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 346M/4.96G [00:08<01:47, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 357M/4.96G [00:08<01:47, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 367M/4.96G [00:08<01:47, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 377M/4.96G [00:08<01:47, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 388M/4.96G [00:09<01:47, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 398M/4.96G [00:09<01:46, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 409M/4.96G [00:09<01:46, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 419M/4.96G [00:09<01:45, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 430M/4.96G [00:10<01:45, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 440M/4.96G [00:10<01:44, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 451M/4.96G [00:10<01:44, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 461M/4.96G [00:10<01:44, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 472M/4.96G [00:10<01:44, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 482M/4.96G [00:11<01:43, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 493M/4.96G [00:11<01:43, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 503M/4.96G [00:11<01:43, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 514M/4.96G [00:11<01:43, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 524M/4.96G [00:12<01:42, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 535M/4.96G [00:12<01:42, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 545M/4.96G [00:12<01:42, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 556M/4.96G [00:12<01:42, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 566M/4.96G [00:13<01:41, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 577M/4.96G [00:13<01:42, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 587M/4.96G [00:13<01:41, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 598M/4.96G [00:13<01:41, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 608M/4.96G [00:14<01:41, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 619M/4.96G [00:14<01:41, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 629M/4.96G [00:14<01:40, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 640M/4.96G [00:14<01:42, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 650M/4.96G [00:15<01:41, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 661M/4.96G [00:15<01:41, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 671M/4.96G [00:15<01:40, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 682M/4.96G [00:15<01:39, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 692M/4.96G [00:16<01:38, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 703M/4.96G [00:16<01:38, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 713M/4.96G [00:16<01:38, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 724M/4.96G [00:16<01:39, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 734M/4.96G [00:17<01:38, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 744M/4.96G [00:17<01:38, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 755M/4.96G [00:17<01:37, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 765M/4.96G [00:17<01:37, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 776M/4.96G [00:18<01:37, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 786M/4.96G [00:18<01:37, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 797M/4.96G [00:18<01:37, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 807M/4.96G [00:18<01:36, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 818M/4.96G [00:19<01:36, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 828M/4.96G [00:19<01:36, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 839M/4.96G [00:19<01:36, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 849M/4.96G [00:19<01:35, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 860M/4.96G [00:20<01:35, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 870M/4.96G [00:20<01:35, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 881M/4.96G [00:20<01:34, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 891M/4.96G [00:20<01:34, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 902M/4.96G [00:21<01:34, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 912M/4.96G [00:21<01:34, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 923M/4.96G [00:21<01:34, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 933M/4.96G [00:21<01:34, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 944M/4.96G [00:22<01:33, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 954M/4.96G [00:22<01:33, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 965M/4.96G [00:22<01:32, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 975M/4.96G [00:22<01:32, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 986M/4.96G [00:22<01:32, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 996M/4.96G [00:23<01:31, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.96G [00:23<01:31, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.02G/4.96G [00:23<01:31, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.03G/4.96G [00:23<01:31, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.04G/4.96G [00:24<01:31, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.05G/4.96G [00:24<01:30, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.06G/4.96G [00:24<01:30, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:24<01:30, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.08G/4.96G [00:25<01:30, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.09G/4.96G [00:25<01:30, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.10G/4.96G [00:25<01:31, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.11G/4.96G [00:25<01:30, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.12G/4.96G [00:26<01:29, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.13G/4.96G [00:26<01:29, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:26<01:28, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.15G/4.96G [00:26<01:28, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.16G/4.96G [00:27<01:28, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.17G/4.96G [00:27<01:28, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.18G/4.96G [00:27<01:27, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.20G/4.96G [00:27<01:27, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.96G [00:28<01:27, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.22G/4.96G [00:28<01:27, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.23G/4.96G [00:28<01:26, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.24G/4.96G [00:28<01:26, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.25G/4.96G [00:29<01:26, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.26G/4.96G [00:29<01:25, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.27G/4.96G [00:29<01:25, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:29<01:25, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.29G/4.96G [00:30<01:24, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.30G/4.96G [00:30<01:24, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.31G/4.96G [00:30<01:24, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.32G/4.96G [00:30<01:39, 36.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.33G/4.96G [00:31<01:34, 38.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:31<01:30, 39.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.35G/4.96G [00:31<01:28, 40.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.36G/4.96G [00:31<01:27, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.37G/4.96G [00:32<01:25, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.38G/4.96G [00:32<01:24, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.39G/4.96G [00:32<01:24, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:32<01:23, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.42G/4.96G [00:33<01:23, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.43G/4.96G [00:33<01:23, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.44G/4.96G [00:33<01:23, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.45G/4.96G [00:33<01:21, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.46G/4.96G [00:34<01:21, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.47G/4.96G [00:34<01:21, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.96G [00:34<01:21, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.49G/4.96G [00:34<01:20, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.50G/4.96G [00:35<01:20, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.51G/4.96G [00:35<01:20, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.52G/4.96G [00:35<01:20, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.53G/4.96G [00:35<01:20, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:36<01:20, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.55G/4.96G [00:36<01:19, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.56G/4.96G [00:36<01:19, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.57G/4.96G [00:36<01:18, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.58G/4.96G [00:37<01:18, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.59G/4.96G [00:37<01:18, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.60G/4.96G [00:37<01:18, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.61G/4.96G [00:37<01:18, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.63G/4.96G [00:38<01:17, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.64G/4.96G [00:38<01:17, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.65G/4.96G [00:38<01:17, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.66G/4.96G [00:38<01:17, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.67G/4.96G [00:39<01:16, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:39<01:17, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.69G/4.96G [00:39<01:16, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.70G/4.96G [00:39<01:16, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.71G/4.96G [00:39<01:16, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.72G/4.96G [00:40<01:15, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.73G/4.96G [00:40<01:15, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.74G/4.96G [00:40<01:15, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.96G [00:40<01:14, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.76G/4.96G [00:41<01:14, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.77G/4.96G [00:41<01:14, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.78G/4.96G [00:41<01:13, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.79G/4.96G [00:41<01:13, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.80G/4.96G [00:42<01:13, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:42<01:12, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.82G/4.96G [00:42<01:12, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.84G/4.96G [00:42<01:12, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.85G/4.96G [00:43<01:12, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.86G/4.96G [00:43<01:12, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.87G/4.96G [00:43<01:11, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:43<01:11, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.89G/4.96G [00:44<01:11, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.90G/4.96G [00:44<01:11, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.91G/4.96G [00:44<01:22, 37.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.92G/4.96G [00:45<01:18, 38.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.93G/4.96G [00:45<01:16, 39.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.94G/4.96G [00:45<01:14, 40.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:45<01:13, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.96G/4.96G [00:45<01:12, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.97G/4.96G [00:46<01:11, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.98G/4.96G [00:46<01:10, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.99G/4.96G [00:46<01:10, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 2.00G/4.96G [00:46<01:09, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:47<01:09, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.02G/4.96G [00:47<01:08, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.03G/4.96G [00:47<01:08, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.04G/4.96G [00:47<01:07, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.06G/4.96G [00:48<01:07, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.07G/4.96G [00:48<01:07, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:48<01:06, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.09G/4.96G [00:48<01:06, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.10G/4.96G [00:49<01:06, 43.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.11G/4.96G [00:49<01:06, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.12G/4.96G [00:49<01:05, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.13G/4.96G [00:49<01:05, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.14G/4.96G [00:50<01:05, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/4.96G [00:50<01:05, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.16G/4.96G [00:50<01:04, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.17G/4.96G [00:50<01:04, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/4.96G [00:51<01:04, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.19G/4.96G [00:51<01:03, 43.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.20G/4.96G [00:51<01:03, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:51<01:03, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.22G/4.96G [00:52<01:03, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.23G/4.96G [00:52<01:03, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.24G/4.96G [00:52<01:03, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.25G/4.96G [00:52<01:03, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.26G/4.96G [00:53<01:02, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:53<01:02, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.29G/4.96G [00:53<01:01, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.30G/4.96G [00:53<01:01, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.31G/4.96G [00:54<01:01, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.32G/4.96G [00:54<01:01, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.33G/4.96G [00:54<01:01, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.34G/4.96G [00:54<01:00, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:55<01:02, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.36G/4.96G [00:55<01:02, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.37G/4.96G [00:55<01:01, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.38G/4.96G [00:55<01:00, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.39G/4.96G [00:56<01:01, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.40G/4.96G [00:56<01:00, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.41G/4.96G [00:56<00:59, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:56<00:59, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.43G/4.96G [00:57<01:01, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.44G/4.96G [00:57<00:59, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.45G/4.96G [00:57<00:59, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.46G/4.96G [00:57<00:58, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.47G/4.96G [00:58<01:00, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.49G/4.96G [00:58<00:58, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.50G/4.96G [00:58<00:58, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.51G/4.96G [00:58<00:57, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.52G/4.96G [00:59<00:59, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.53G/4.96G [00:59<00:58, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.54G/4.96G [00:59<00:57, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:59<00:56, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.56G/4.96G [01:00<00:58, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.57G/4.96G [01:00<00:57, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.58G/4.96G [01:00<00:56, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.59G/4.96G [01:00<00:55, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.60G/4.96G [01:01<00:57, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.61G/4.96G [01:01<00:56, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [01:01<00:55, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.63G/4.96G [01:01<00:54, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.64G/4.96G [01:02<00:56, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.65G/4.96G [01:02<00:55, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.66G/4.96G [01:02<00:54, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.67G/4.96G [01:02<00:53, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [01:03<00:55, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/4.96G [01:03<00:53, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.71G/4.96G [01:03<00:53, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.72G/4.96G [01:03<00:52, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.73G/4.96G [01:04<00:54, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.74G/4.96G [01:04<00:52, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [01:04<00:52, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.76G/4.96G [01:04<00:51, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.77G/4.96G [01:05<00:53, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.78G/4.96G [01:05<00:52, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.79G/4.96G [01:05<00:51, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.80G/4.96G [01:05<00:50, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.81G/4.96G [01:06<00:52, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [01:06<00:51, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.83G/4.96G [01:06<00:51, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.84G/4.96G [01:06<00:50, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.85G/4.96G [01:07<00:50, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.86G/4.96G [01:07<00:49, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.87G/4.96G [01:07<00:49, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.88G/4.96G [01:07<00:48, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [01:08<00:49, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.90G/4.96G [01:08<00:49, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.92G/4.96G [01:08<00:48, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.93G/4.96G [01:08<00:47, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.94G/4.96G [01:09<00:48, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.95G/4.96G [01:09<00:47, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.96G/4.96G [01:09<00:47, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.97G/4.96G [01:09<00:46, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.98G/4.96G [01:10<00:47, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.99G/4.96G [01:10<00:46, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 3.00G/4.96G [01:10<00:46, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.01G/4.96G [01:10<00:45, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [01:11<00:47, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.03G/4.96G [01:11<00:45, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.04G/4.96G [01:11<00:39, 48.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.06G/4.96G [01:12<00:46, 40.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.07G/4.96G [01:12<00:45, 41.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.08G/4.96G [01:12<00:45, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.96G [01:12<00:44, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.10G/4.96G [01:13<00:45, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.11G/4.96G [01:13<00:44, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.12G/4.96G [01:13<00:43, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.14G/4.96G [01:13<00:42, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.15G/4.96G [01:14<00:44, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.16G/4.96G [01:14<00:43, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.17G/4.96G [01:14<00:42, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.18G/4.96G [01:14<00:42, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.19G/4.96G [01:15<00:42, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.20G/4.96G [01:15<00:42, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.21G/4.96G [01:15<00:41, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [01:15<00:41, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.23G/4.96G [01:16<00:41, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.24G/4.96G [01:16<00:41, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.25G/4.96G [01:16<00:40, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.26G/4.96G [01:16<00:39, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.27G/4.96G [01:17<00:40, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.28G/4.96G [01:17<00:39, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [01:17<00:39, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.30G/4.96G [01:17<00:38, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.31G/4.96G [01:18<00:39, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.32G/4.96G [01:18<00:39, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.33G/4.96G [01:18<00:38, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.34G/4.96G [01:18<00:37, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.36G/4.96G [01:19<00:38, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.37G/4.96G [01:19<00:38, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.38G/4.96G [01:19<00:37, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.39G/4.96G [01:19<00:37, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.40G/4.96G [01:20<00:37, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.41G/4.96G [01:20<00:37, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [01:20<00:36, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.43G/4.96G [01:20<00:36, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/4.96G [01:21<00:36, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.45G/4.96G [01:21<00:35, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.46G/4.96G [01:21<00:35, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.47G/4.96G [01:21<00:34, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.48G/4.96G [01:22<00:35, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [01:22<00:35, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.50G/4.96G [01:22<00:35, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.51G/4.96G [01:22<00:34, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.52G/4.96G [01:23<00:34, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.53G/4.96G [01:23<00:33, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.54G/4.96G [01:23<00:33, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.55G/4.96G [01:23<00:32, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.57G/4.96G [01:24<00:33, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.58G/4.96G [01:24<00:32, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.59G/4.96G [01:24<00:32, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.60G/4.96G [01:24<00:32, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.61G/4.96G [01:25<00:32, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [01:25<00:31, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/4.96G [01:25<00:31, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.64G/4.96G [01:25<00:31, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.65G/4.96G [01:26<00:31, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.66G/4.96G [01:26<00:31, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.67G/4.96G [01:26<00:30, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.68G/4.96G [01:26<00:30, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [01:27<00:30, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.70G/4.96G [01:27<00:30, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.71G/4.96G [01:27<00:29, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.72G/4.96G [01:27<00:29, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.73G/4.96G [01:28<00:29, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.74G/4.96G [01:28<00:28, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.75G/4.96G [01:28<00:28, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [01:28<00:28, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.77G/4.96G [01:29<00:28, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.79G/4.96G [01:29<00:28, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.80G/4.96G [01:29<00:27, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.81G/4.96G [01:29<00:27, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [01:30<00:27, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.83G/4.96G [01:30<00:26, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.84G/4.96G [01:30<00:26, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.85G/4.96G [01:30<00:26, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.86G/4.96G [01:31<00:26, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.87G/4.96G [01:31<00:25, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.88G/4.96G [01:31<00:25, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/4.96G [01:31<00:25, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.90G/4.96G [01:32<00:25, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.91G/4.96G [01:32<00:25, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.92G/4.96G [01:32<00:24, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.93G/4.96G [01:32<00:24, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.94G/4.96G [01:33<00:24, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.95G/4.96G [01:33<00:24, 41.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:33<00:23, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.97G/4.96G [01:33<00:23, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.98G/4.96G [01:34<00:23, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.00G/4.96G [01:34<00:22, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.01G/4.96G [01:34<00:22, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.02G/4.96G [01:34<00:22, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:35<00:22, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.04G/4.96G [01:35<00:21, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.05G/4.96G [01:35<00:21, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.06G/4.96G [01:35<00:21, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.07G/4.96G [01:36<00:21, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.08G/4.96G [01:36<00:20, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.09G/4.96G [01:36<00:20, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.10G/4.96G [01:36<00:20, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.11G/4.96G [01:37<00:20, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.12G/4.96G [01:37<00:19, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.13G/4.96G [01:37<00:19, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.14G/4.96G [01:37<00:19, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.15G/4.96G [01:38<00:19, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [01:38<00:18, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.17G/4.96G [01:38<00:18, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/4.96G [01:38<00:18, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.19G/4.96G [01:39<00:18, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.20G/4.96G [01:39<00:18, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.22G/4.96G [01:39<00:17, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:39<00:17, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.24G/4.96G [01:40<00:17, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.25G/4.96G [01:40<00:17, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.26G/4.96G [01:40<00:16, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.27G/4.96G [01:40<00:16, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.28G/4.96G [01:41<00:16, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [01:41<00:15, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.30G/4.96G [01:41<00:15, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.31G/4.96G [01:41<00:15, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.32G/4.96G [01:42<00:15, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.33G/4.96G [01:42<00:14, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.34G/4.96G [01:42<00:14, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.35G/4.96G [01:42<00:14, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:43<00:14, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.37G/4.96G [01:43<00:13, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.38G/4.96G [01:43<00:13, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.39G/4.96G [01:43<00:13, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.40G/4.96G [01:44<00:13, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.41G/4.96G [01:44<00:12, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.42G/4.96G [01:44<00:12, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.44G/4.96G [01:44<00:12, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.45G/4.96G [01:45<00:12, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.46G/4.96G [01:45<00:11, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.47G/4.96G [01:45<00:11, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.48G/4.96G [01:45<00:11, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [01:46<00:11, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.50G/4.96G [01:46<00:13, 35.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.51G/4.96G [01:46<00:12, 37.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.52G/4.96G [01:46<00:11, 38.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.53G/4.96G [01:47<00:10, 39.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.54G/4.96G [01:47<00:10, 40.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.55G/4.96G [01:47<00:09, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [01:47<00:09, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.57G/4.96G [01:48<00:09, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.58G/4.96G [01:48<00:08, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.59G/4.96G [01:48<00:08, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.60G/4.96G [01:48<00:08, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.61G/4.96G [01:49<00:08, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.62G/4.96G [01:49<00:07, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [01:49<00:07, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.65G/4.96G [01:49<00:07, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.66G/4.96G [01:50<00:07, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.67G/4.96G [01:50<00:06, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.68G/4.96G [01:50<00:06, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:50<00:06, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.70G/4.96G [01:51<00:06, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.71G/4.96G [01:51<00:05, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.72G/4.96G [01:51<00:05, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.73G/4.96G [01:51<00:05, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.74G/4.96G [01:52<00:05, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.75G/4.96G [01:52<00:04, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:52<00:03, 51.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.77G/4.96G [01:52<00:03, 48.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.78G/4.96G [01:53<00:04, 40.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.79G/4.96G [01:53<00:04, 41.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.80G/4.96G [01:53<00:03, 41.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.81G/4.96G [01:53<00:03, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:54<00:03, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.83G/4.96G [01:54<00:02, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.84G/4.96G [01:54<00:02, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.85G/4.96G [01:54<00:02, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.87G/4.96G [01:55<00:02, 41.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.88G/4.96G [01:55<00:01, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:55<00:01, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.90G/4.96G [01:55<00:01, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.91G/4.96G [01:56<00:01, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.92G/4.96G [01:56<00:00, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.93G/4.96G [01:56<00:00, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.94G/4.96G [01:56<00:00, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.95G/4.96G [01:57<00:00, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:57<00:00, 42.3MB/s]\n",
            "Downloading shards:  67% 2/3 [03:57<01:58, 118.76s/it]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 10.5M/4.18G [00:00<02:30, 27.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 21.0M/4.18G [00:00<02:00, 34.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 31.5M/4.18G [00:00<01:48, 38.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 41.9M/4.18G [00:01<01:42, 40.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 52.4M/4.18G [00:01<01:40, 41.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 62.9M/4.18G [00:01<01:38, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 73.4M/4.18G [00:01<01:37, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 83.9M/4.18G [00:02<01:36, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 94.4M/4.18G [00:02<01:35, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 105M/4.18G [00:02<01:35, 42.7MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 115M/4.18G [00:02<01:35, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 126M/4.18G [00:03<01:34, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 136M/4.18G [00:03<01:34, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 147M/4.18G [00:03<01:34, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 157M/4.18G [00:03<01:34, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 168M/4.18G [00:04<01:33, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 178M/4.18G [00:04<01:34, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 189M/4.18G [00:04<01:33, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 199M/4.18G [00:04<01:33, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 210M/4.18G [00:05<01:33, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 220M/4.18G [00:05<01:32, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 231M/4.18G [00:05<01:32, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 241M/4.18G [00:05<01:31, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 252M/4.18G [00:06<01:31, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 262M/4.18G [00:06<01:30, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 273M/4.18G [00:06<01:30, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 283M/4.18G [00:06<01:30, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 294M/4.18G [00:06<01:30, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 304M/4.18G [00:07<01:30, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 315M/4.18G [00:07<01:30, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 325M/4.18G [00:07<01:30, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 336M/4.18G [00:07<01:29, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 346M/4.18G [00:08<01:29, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 357M/4.18G [00:08<01:29, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 367M/4.18G [00:08<01:28, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 377M/4.18G [00:08<01:28, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 388M/4.18G [00:09<01:28, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 398M/4.18G [00:09<01:27, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 409M/4.18G [00:09<01:27, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 419M/4.18G [00:09<01:27, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 430M/4.18G [00:10<01:26, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 440M/4.18G [00:10<01:26, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 451M/4.18G [00:10<01:26, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 461M/4.18G [00:10<01:26, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 472M/4.18G [00:11<01:26, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 482M/4.18G [00:11<01:26, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 493M/4.18G [00:11<01:26, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 503M/4.18G [00:11<01:26, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 514M/4.18G [00:12<01:25, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 524M/4.18G [00:12<01:25, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 535M/4.18G [00:12<01:25, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 545M/4.18G [00:12<01:25, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 556M/4.18G [00:13<01:25, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 566M/4.18G [00:13<01:24, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 577M/4.18G [00:13<01:23, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 587M/4.18G [00:13<01:37, 36.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 598M/4.18G [00:14<01:32, 38.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 608M/4.18G [00:14<01:29, 39.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 619M/4.18G [00:14<01:27, 40.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 629M/4.18G [00:14<01:25, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 640M/4.18G [00:15<01:24, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 650M/4.18G [00:15<01:23, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 661M/4.18G [00:15<01:22, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 671M/4.18G [00:15<01:22, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 682M/4.18G [00:16<01:21, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 692M/4.18G [00:16<01:21, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 703M/4.18G [00:16<01:20, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 713M/4.18G [00:16<01:20, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 724M/4.18G [00:17<01:20, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 734M/4.18G [00:17<01:20, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 744M/4.18G [00:17<01:19, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 755M/4.18G [00:17<01:19, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 765M/4.18G [00:18<01:19, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 776M/4.18G [00:18<01:19, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 786M/4.18G [00:18<01:18, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 797M/4.18G [00:18<01:18, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 807M/4.18G [00:19<01:18, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 818M/4.18G [00:19<01:18, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 828M/4.18G [00:19<01:18, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 839M/4.18G [00:19<01:17, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 849M/4.18G [00:20<01:17, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 860M/4.18G [00:20<01:17, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 870M/4.18G [00:20<01:17, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 881M/4.18G [00:20<01:17, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 891M/4.18G [00:21<01:16, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 902M/4.18G [00:21<01:16, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 912M/4.18G [00:21<01:16, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 923M/4.18G [00:21<01:15, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 933M/4.18G [00:22<01:15, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 944M/4.18G [00:22<01:14, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 954M/4.18G [00:22<01:14, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 965M/4.18G [00:22<01:15, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 975M/4.18G [00:23<01:14, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 986M/4.18G [00:23<01:14, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 996M/4.18G [00:23<01:14, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:23<01:14, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.02G/4.18G [00:24<01:25, 36.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.03G/4.18G [00:24<01:21, 38.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.04G/4.18G [00:24<01:18, 40.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.05G/4.18G [00:24<01:16, 40.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.06G/4.18G [00:25<01:14, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.07G/4.18G [00:25<01:13, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.08G/4.18G [00:25<01:12, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.09G/4.18G [00:25<01:12, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.10G/4.18G [00:26<01:12, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.11G/4.18G [00:26<01:11, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.12G/4.18G [00:26<01:11, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.13G/4.18G [00:26<01:11, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:27<01:10, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.15G/4.18G [00:27<01:10, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.16G/4.18G [00:27<01:09, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.17G/4.18G [00:27<01:09, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.18G/4.18G [00:28<01:09, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.20G/4.18G [00:28<01:09, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:28<01:09, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.22G/4.18G [00:28<01:09, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.23G/4.18G [00:29<01:09, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.24G/4.18G [00:29<01:09, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.25G/4.18G [00:29<01:08, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.26G/4.18G [00:29<01:08, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.27G/4.18G [00:29<01:08, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.28G/4.18G [00:30<01:07, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.29G/4.18G [00:30<01:07, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.30G/4.18G [00:30<00:58, 49.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.31G/4.18G [00:30<00:55, 51.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.32G/4.18G [00:31<00:59, 48.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.33G/4.18G [00:31<01:01, 46.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:31<01:02, 45.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.35G/4.18G [00:31<01:04, 44.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.36G/4.18G [00:32<01:04, 43.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.37G/4.18G [00:32<01:04, 43.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.38G/4.18G [00:32<01:04, 43.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.39G/4.18G [00:32<01:04, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:33<01:04, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.42G/4.18G [00:33<01:04, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.43G/4.18G [00:33<01:04, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.44G/4.18G [00:33<01:04, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.45G/4.18G [00:33<01:03, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.46G/4.18G [00:34<01:03, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.47G/4.18G [00:34<01:03, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:34<01:03, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.49G/4.18G [00:34<01:03, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.50G/4.18G [00:35<01:02, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.51G/4.18G [00:35<01:02, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.52G/4.18G [00:35<01:02, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.53G/4.18G [00:35<01:02, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.54G/4.18G [00:36<01:03, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.55G/4.18G [00:36<01:02, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.56G/4.18G [00:36<01:01, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.57G/4.18G [00:36<01:01, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.58G/4.18G [00:37<01:00, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.59G/4.18G [00:37<01:00, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.60G/4.18G [00:37<01:00, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:37<01:00, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.63G/4.18G [00:38<00:59, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.64G/4.18G [00:38<00:59, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.65G/4.18G [00:38<00:58, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.66G/4.18G [00:38<00:58, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.67G/4.18G [00:39<00:58, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.68G/4.18G [00:39<00:58, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.69G/4.18G [00:39<00:58, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.70G/4.18G [00:39<00:57, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.71G/4.18G [00:40<00:57, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.72G/4.18G [00:40<00:57, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.73G/4.18G [00:40<00:57, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.74G/4.18G [00:40<00:56, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.75G/4.18G [00:41<00:56, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.76G/4.18G [00:41<00:56, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.77G/4.18G [00:41<00:56, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.78G/4.18G [00:41<00:55, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.79G/4.18G [00:42<00:55, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.80G/4.18G [00:42<00:55, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:42<00:55, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.82G/4.18G [00:42<00:54, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.84G/4.18G [00:43<00:54, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.85G/4.18G [00:43<00:54, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.86G/4.18G [00:43<00:54, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.87G/4.18G [00:43<00:53, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:44<00:53, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.89G/4.18G [00:44<00:53, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.90G/4.18G [00:44<00:53, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 1.91G/4.18G [00:44<01:05, 34.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 1.92G/4.18G [00:45<00:58, 38.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 1.93G/4.18G [00:45<00:56, 39.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 1.94G/4.18G [00:45<00:54, 40.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.95G/4.18G [00:45<00:53, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.96G/4.18G [00:46<00:52, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.97G/4.18G [00:46<00:52, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.98G/4.18G [00:46<00:51, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 1.99G/4.18G [00:46<00:51, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.00G/4.18G [00:47<00:51, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:47<00:50, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.02G/4.18G [00:47<00:50, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.03G/4.18G [00:47<00:50, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.04G/4.18G [00:48<00:50, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.06G/4.18G [00:48<00:49, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.07G/4.18G [00:48<00:49, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.08G/4.18G [00:48<00:48, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.09G/4.18G [00:49<00:49, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.10G/4.18G [00:49<00:48, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.11G/4.18G [00:49<00:48, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.12G/4.18G [00:49<00:48, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.13G/4.18G [00:50<00:47, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.14G/4.18G [00:50<00:47, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:50<00:47, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.16G/4.18G [00:50<00:47, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.17G/4.18G [00:51<00:46, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.18G/4.18G [00:51<00:46, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.19G/4.18G [00:51<00:46, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.20G/4.18G [00:51<00:46, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:52<00:45, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.22G/4.18G [00:52<00:45, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.23G/4.18G [00:52<00:45, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.24G/4.18G [00:52<00:45, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.25G/4.18G [00:53<00:44, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.26G/4.18G [00:53<00:44, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.28G/4.18G [00:53<00:44, 43.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.29G/4.18G [00:53<00:43, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.30G/4.18G [00:53<00:43, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.31G/4.18G [00:54<00:43, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.32G/4.18G [00:54<00:43, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.33G/4.18G [00:54<00:43, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.34G/4.18G [00:54<00:42, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:55<00:43, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.36G/4.18G [00:55<00:42, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.37G/4.18G [00:55<00:42, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.38G/4.18G [00:55<00:41, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.39G/4.18G [00:56<00:43, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.40G/4.18G [00:56<00:42, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.41G/4.18G [00:56<00:41, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:56<00:41, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.43G/4.18G [00:57<00:42, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.44G/4.18G [00:57<00:41, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.45G/4.18G [00:57<00:40, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.46G/4.18G [00:57<00:40, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.47G/4.18G [00:58<00:41, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.49G/4.18G [00:58<00:40, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.50G/4.18G [00:58<00:40, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.51G/4.18G [00:58<00:39, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.52G/4.18G [00:59<00:40, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.53G/4.18G [00:59<00:39, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.54G/4.18G [00:59<00:38, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:59<00:38, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.56G/4.18G [01:00<00:39, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.57G/4.18G [01:00<00:38, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.58G/4.18G [01:00<00:37, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.59G/4.18G [01:00<00:37, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.60G/4.18G [01:01<00:38, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.61G/4.18G [01:01<00:37, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [01:01<00:36, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.63G/4.18G [01:01<00:36, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.64G/4.18G [01:02<00:37, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.65G/4.18G [01:02<00:36, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.66G/4.18G [01:02<00:36, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.67G/4.18G [01:02<00:35, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.68G/4.18G [01:03<00:35, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.69G/4.18G [01:03<00:35, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.71G/4.18G [01:03<00:34, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.72G/4.18G [01:03<00:34, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.73G/4.18G [01:04<00:34, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.74G/4.18G [01:04<00:34, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [01:04<00:33, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.76G/4.18G [01:04<00:33, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.77G/4.18G [01:05<00:34, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.78G/4.18G [01:05<00:33, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.79G/4.18G [01:05<00:32, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.80G/4.18G [01:05<00:32, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.81G/4.18G [01:06<00:33, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 2.82G/4.18G [01:06<00:32, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 2.83G/4.18G [01:06<00:31, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 2.84G/4.18G [01:06<00:31, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 2.85G/4.18G [01:07<00:31, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.86G/4.18G [01:07<00:31, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.87G/4.18G [01:07<00:30, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.88G/4.18G [01:07<00:30, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.89G/4.18G [01:08<00:31, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.90G/4.18G [01:08<00:30, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.92G/4.18G [01:08<00:29, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.93G/4.18G [01:08<00:29, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.94G/4.18G [01:09<00:30, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [01:09<00:29, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.96G/4.18G [01:09<00:28, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.97G/4.18G [01:09<00:28, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.98G/4.18G [01:10<00:29, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 2.99G/4.18G [01:10<00:28, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.00G/4.18G [01:10<00:28, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.01G/4.18G [01:10<00:27, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.02G/4.18G [01:11<00:27, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.03G/4.18G [01:11<00:27, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.04G/4.18G [01:11<00:26, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.05G/4.18G [01:11<00:26, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.06G/4.18G [01:12<00:27, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.07G/4.18G [01:12<00:26, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.08G/4.18G [01:12<00:26, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [01:12<00:25, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.10G/4.18G [01:13<00:25, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.11G/4.18G [01:13<00:25, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.12G/4.18G [01:13<00:24, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.14G/4.18G [01:13<00:24, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.15G/4.18G [01:14<00:25, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.16G/4.18G [01:14<00:24, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.17G/4.18G [01:14<00:23, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.18G/4.18G [01:14<00:23, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.19G/4.18G [01:15<00:23, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.20G/4.18G [01:15<00:23, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.21G/4.18G [01:15<00:22, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [01:15<00:22, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.23G/4.18G [01:16<00:22, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.24G/4.18G [01:16<00:22, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.25G/4.18G [01:16<00:21, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.26G/4.18G [01:16<00:21, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.27G/4.18G [01:17<00:21, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.28G/4.18G [01:17<00:21, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [01:17<00:20, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.30G/4.18G [01:17<00:20, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.31G/4.18G [01:18<00:20, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.32G/4.18G [01:18<00:20, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.33G/4.18G [01:18<00:19, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.34G/4.18G [01:18<00:19, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.36G/4.18G [01:19<00:19, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.37G/4.18G [01:19<00:19, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.38G/4.18G [01:19<00:18, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.39G/4.18G [01:19<00:18, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.40G/4.18G [01:20<00:18, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.41G/4.18G [01:20<00:18, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [01:20<00:17, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.43G/4.18G [01:20<00:17, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.44G/4.18G [01:21<00:17, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.45G/4.18G [01:21<00:17, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.46G/4.18G [01:21<00:16, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.47G/4.18G [01:21<00:16, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.48G/4.18G [01:22<00:16, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [01:22<00:16, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.50G/4.18G [01:22<00:15, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.51G/4.18G [01:22<00:15, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.52G/4.18G [01:23<00:15, 41.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.53G/4.18G [01:23<00:15, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.54G/4.18G [01:23<00:15, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.55G/4.18G [01:23<00:14, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.57G/4.18G [01:24<00:14, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.58G/4.18G [01:24<00:14, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.59G/4.18G [01:24<00:13, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.60G/4.18G [01:24<00:13, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.61G/4.18G [01:25<00:13, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [01:25<00:13, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.63G/4.18G [01:25<00:13, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.64G/4.18G [01:25<00:12, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.65G/4.18G [01:26<00:12, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.66G/4.18G [01:26<00:12, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.67G/4.18G [01:26<00:12, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.68G/4.18G [01:26<00:11, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.69G/4.18G [01:27<00:11, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.70G/4.18G [01:27<00:11, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.71G/4.18G [01:27<00:10, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.72G/4.18G [01:27<00:10, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.73G/4.18G [01:28<00:10, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.74G/4.18G [01:28<00:10, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.75G/4.18G [01:28<00:09, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [01:28<00:09, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.77G/4.18G [01:29<00:09, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 3.79G/4.18G [01:29<00:09, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 3.80G/4.18G [01:29<00:09, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 3.81G/4.18G [01:29<00:08, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 3.82G/4.18G [01:30<00:08, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.83G/4.18G [01:30<00:08, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.84G/4.18G [01:30<00:07, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.85G/4.18G [01:30<00:07, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.86G/4.18G [01:31<00:07, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.87G/4.18G [01:31<00:07, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.88G/4.18G [01:31<00:06, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [01:31<00:06, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.90G/4.18G [01:32<00:07, 34.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.91G/4.18G [01:32<00:06, 39.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.92G/4.18G [01:32<00:06, 40.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.93G/4.18G [01:33<00:05, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.94G/4.18G [01:33<00:05, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.95G/4.18G [01:33<00:05, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.96G/4.18G [01:33<00:05, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.97G/4.18G [01:34<00:04, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.98G/4.18G [01:34<00:04, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.00G/4.18G [01:34<00:04, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.01G/4.18G [01:34<00:03, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.02G/4.18G [01:35<00:03, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.03G/4.18G [01:35<00:03, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:35<00:03, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.05G/4.18G [01:35<00:02, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.06G/4.18G [01:35<00:02, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.07G/4.18G [01:36<00:02, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.08G/4.18G [01:36<00:02, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.09G/4.18G [01:36<00:02, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.10G/4.18G [01:36<00:01, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:37<00:01, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.12G/4.18G [01:37<00:01, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.13G/4.18G [01:37<00:01, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.14G/4.18G [01:37<00:00, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.15G/4.18G [01:38<00:00, 41.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.16G/4.18G [01:38<00:00, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:38<00:00, 42.3MB/s]\n",
            "Downloading shards: 100% 3/3 [05:37<00:00, 112.43s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [01:00<00:00, 20.03s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.07MB/s]\n",
            "You may have used the wrong order for inputs. `images` should be passed before `text`. The `images` and `text` inputs will be swapped. This behavior will be deprecated in transformers v4.47.\n",
            "[' Yes', ' The safest method for autonomous driving on straight lanes with different levels of outlier vehicles is the one that has the lowest collision rate.']\n",
            "-----------------\n",
            "[' No', ' The safest method is the one that has the lowest number of accidents.']\n",
            "-----------------\n",
            "[' Yes', \" The ChoiceNet model performs well on datasets with uniform corruptions, as it is designed to handle such situations. The model is able to accurately predict the outcomes of the corrupted datasets, as shown in the image. This demonstrates the model's ability to handle complex and noisy data, which is a valuable asset in real-world applications.\"]\n",
            "-----------------\n",
            "[' Yes', ' The method that performs the best when there are a lot of outliers in the data is the one with the lowest outlier rate.']\n",
            "-----------------\n",
            "[' Yes', ' The mixture of classifiers is more robust to outliers than density estimation.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the performance of different models on the MNIST dataset with randomly permutated labels. The models include ConvNet, ConvNet + Mixup, and ChoiceNet. The table displays the test accuracies for each model as the corruption level increases. The ChoiceNet model appears to be the most robust to label corruption, as it has the highest accuracy across all levels of corruption.']\n",
            "-----------------\n",
            "[' Yes', ' The method that appears to be most robust to the presence of outliers in the training data is the one with the lowest RMSE.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the accuracy of different methods on the Large Movie Review dataset with varying corruption probabilities. The methods include choice net, MLP, and VAT. The table displays the accuracy percentages for each method at different corruption levels. The best performing method is the one with the highest accuracy at the lowest corruption level. As the corruption level increases, the accuracy of the methods decreases.']\n",
            "-----------------\n",
            "[' Yes', ' The accuracy of the WideResNet model is higher than the ChoiceNet model on the CIFAR-10 dataset with 50% random shuffle.']\n",
            "-----------------\n",
            "[' Yes', ' The best performance in terms of average error for the step function is shown in the image.']\n",
            "-----------------\n",
            "[' Yes', ' ChoiceNet is a deep learning model that performs well in various settings, including symmetric and asymmetric noises. The model is designed to make accurate predictions by learning from a large dataset. However, it may struggle with certain types of noises or when the input data is not well-structured. The strengths of ChoiceNet lie in its ability to handle complex data and make accurate predictions, while its weaknesses may include the need for large amounts of training data and the potential for']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the average returns of compared methods on behavior cloning problems using MuJoCo. The table has two columns, one for ChoiceNet and the other for MDN. The table also has a row for HalfCheetah. The table shows that ChoiceNet generally performed better in the HalfCheetah task compared to MDN. The performance gap between these two methods increases as the percentage of outliers increases.']\n",
            "-----------------\n",
            "[' Yes', \" ChoiceNet is a deep learning model that performs well in various settings, including different noise conditions on the CIFAR-10 dataset. The model's strengths lie in its ability to learn from a large dataset, generalize well to new examples, and achieve high accuracy. However, ChoiceNet may struggle with certain types of noises or when the input data is not well-structured. The model's weaknesses may include overfitting to the training data,\"]\n",
            "-----------------\n",
            "[' Yes', ' The method that performs best at all noise levels is the one with the highest accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The Cholesky block in the ChoiceNet architecture is responsible for computing the covariance matrix, which is used to model the target distribution.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the Cholesky Block in this figure is to transform the target weight matrix W∗ and auxiliary matrix Z into the correlated weight matrix W̃k. This is done by applying the CholeskyTransform, which helps to distinguish abnormal patterns from normal ones.']\n",
            "-----------------\n",
            "[' Yes', \" The table shows the test accuracies on the CIFAR-10 datasets with symmetric noises. The row with the highest accuracy is ConvNet+CN with Mixup. When the corruption probability is 80%, it is more beneficial to use ConvNet+CN with Mixup, as it provides better accuracy in detecting the corruption. This is because Mixup helps to improve the model's ability to generalize and detect the corruption, even when\"]\n",
            "-----------------\n",
            "[' Yes', ' The accuracy of the Mixup method increases as the level of random shuffle increases.']\n",
            "-----------------\n",
            "[' Yes', ' The probability of finding the pattern {head=F, ant=NAM} in the data is 0.00000000000000000000000000000000000000000000000000000000000000000000000000000']\n",
            "-----------------\n",
            "[' Yes', ' The support value of the node \"ana=NAM\" is 0.11.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing coreference model on the CoNLL test set according to the F1 score is \"+EPM\". This performance is statistically significant compared to all other models in the table.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows that the deep-coref model that performed best on the WikiCoref dataset is the one with the letter \"B\" in the top row.']\n",
            "-----------------\n",
            "[' Yes', ' The top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method are:\\n\\n1. History\\n2. Culture\\n3. Sports\\n4. Education\\n5. Transportation']\n",
            "-----------------\n",
            "[' Yes', ' The trend in precision and recall as the number of recommended sections k increases is that it improves.']\n",
            "-----------------\n",
            "[' Yes', ' The percentage of categories that can generate at least 10 recommendations using the section-count-based method is 100%.']\n",
            "-----------------\n",
            "[' Yes', ' Bonaparte and Voltaire schools have different educational philosophies. Bonaparte schools focus on physical activities and outdoor education, while Voltaire schools emphasize intellectual and philosophical development. Bonaparte schools teach kids to become good leaders by concentrating on physical activities and managing their time, while Voltaire schools teach kids to become independent thinkers by encouraging them to think critically and develop their own ideas.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the results of the evaluation of different models for response selection. The best performing model is the one with the highest score. The score is calculated based on the accuracy of the selected response. The table shows the scores for each model, and the one with the highest score is the best performing model.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs best on the Ubuntu dataset for text lengths between 60 and 90 words is the LSTM model.']\n",
            "-----------------\n",
            "[' Yes', ' The knowledge gates in the KEHNN architecture are responsible for processing and storing information. They are designed to handle various types of data, including text, images, and audio, and can be used to create and train machine learning models. The architecture is based on the idea of a \"knowledge engine\" that can learn from its environment and make decisions based on the information it has gathered.']\n",
            "-----------------\n",
            "[' Yes', \" The average number of answers per question in the training set is 3.5, while in the development set, it is 6.5. This difference could be due to the fact that the training set is designed to optimize the model's performance, so it may have a higher number of answers to ensure that the model can make accurate predictions. On the other hand, the development set is designed to evaluate the model's performance, so it may have fewer answers to prevent overfitting\"]\n",
            "-----------------\n",
            "[' Yes', ' The NegPair reduction metric shows that the \"excellent\" relevance label category of documents received the most significant rank promotion.']\n",
            "-----------------\n",
            "[' Yes', ' The role of the GRU in the Deep Listwise Context Model (DLCM) is to process the input data and generate the output data.']\n",
            "-----------------\n",
            "[' Yes', ' The NegPair reduction improves as the number of perfect results in a query increases.']\n",
            "-----------------\n",
            "[' Yes', ' The most suitable dataset for training a learning-to-rank model with limited computational resources would be the one with the lowest number of queries, documents, relevance levels, features, and year of release. This is because the dataset with the lowest number of these attributes would require the least amount of computational resources to process and analyze. Additionally, the dataset with the lowest number of attributes is likely to have the most straightforward and unambiguous data, making it easier to train the model and achieve accurate results']\n",
            "-----------------\n",
            "[' Yes', ' The best performance is shown by the green line.']\n",
            "-----------------\n",
            "[' Yes', ' The combination of initial list, model, and loss function that achieved the best overall performance on the Yahoo! set 1 is LIDNN.']\n",
            "-----------------\n",
            "[' Yes', ' The conventional semantic segmentation pipeline involves multiple steps, such as image preprocessing, feature extraction, and classification. In contrast, the proposed framework for joint image denoising and semantic segmentation combines denoising and segmentation into a single process, aiming to improve the accuracy and efficiency of the overall pipeline. This integration allows for better noise reduction and improved segmentation results.']\n",
            "-----------------\n",
            "[' Yes', ' The denoiser that performs the best on the sheep image is the one that is trained with the reconstruction and segmentation joint loss. This can be determined by comparing the denoised image with the ground truth image, and observing which denoiser produces the most accurate and visually pleasing result.']\n",
            "-----------------\n",
            "[' Yes', ' The skip connections in the proposed denoising network are used to reduce the computational complexity of the network. By connecting the feature encoding and decoding modules directly, the network can efficiently process the input image and produce the desired output. This helps to improve the overall performance of the network and reduce the time and resources required for processing.']\n",
            "-----------------\n",
            "[' Yes', ' The best denoising method that performs the best on average across all noise levels tested on the Kodak dataset is the one that has a PSNR value of 40.']\n",
            "-----------------\n",
            "[' Yes', ' C-Tarone achieves higher precision when the number of features is small and the number of data points is large.']\n",
            "-----------------\n",
            "[' Yes', ' The C-Tarone method compares to the binarization method in terms of precision, recall, F-measure, and running time. The C-Tarone method is shown in red, while the binarization approach is shown in blue. The y-axis is in logarithmic scale. Higher (taller) is better in precision, recall, and F-measure, while lower is better in running time.']\n",
            "-----------------\n",
            "[' Yes', ' The KL divergence and the corresponding p-value change as the value of a increases. The KL divergence is the upper bound of the divergence, and the p-value is the minimum achievable value. As the value of a increases, the KL divergence and the p-value both decrease.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that would likely require the most computational resources for C-Tarone to analyze is the one that contains the most data points. In the image, there are several datasets with varying numbers of data points, such as \"data 1,\" \"data 2,\" and \"data 3.\" The dataset with the highest number of data points would be the most challenging for C-Tarone to analyze.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieves the lowest error rate on ILSVRC at 1/4 of the total cost is AdaLoss.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of increasing the budget in FLOPS on the test Top-1 error rate for the three different training strategies is shown in the image. The strategies include a small ANN (orange) that has non-adaptive weights, a large ANN (green) that has non-adaptive weights, and a large ANN (green) that has adaptive weights. The adaptive weights make the small ANN to outperform the large one that has non']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs the best on CIFAR100 and ILSVRC datasets is EANN.']\n",
            "-----------------\n",
            "[' Yes', ' Testing domain adaptation involves evaluating the performance of a machine learning model when applied to data from a specific domain, such as a particular object or scene. On the other hand, testing sensor fusion involves evaluating the performance of a model when applied to data from multiple sensors or sources, such as images from different cameras or data from different sensors.']\n",
            "-----------------\n",
            "[' Yes', ' The method that performs better in terms of classification accuracy is ZDDA3.']\n",
            "-----------------\n",
            "[' Yes', ' The role of the task-irrelevant data in ZDDA is to help the model learn from the unavailable task-relevant data. This is done by using the task-irrelevant data to adapt the model to the target domain, which is the domain where the task-relevant data is unavailable. This allows the model to perform well in the target domain despite the lack of task-relevant data.']\n",
            "-----------------\n",
            "[' No', ' The key difference between ZDDA and UDA/MVL in terms of the available training data is that ZDDA uses a single view of the data, while UDA/MVL uses multiple views.']\n",
            "-----------------\n",
            "[' Yes', ' The layers that would be considered part of the source CNN are the ones that are closer to the input image, while the layers that would be part of the source classifier are the ones that are closer to the output layer.']\n",
            "-----------------\n",
            "[' Yes', ' (c) ZDDA trains a sensor fusion network in step 1.']\n",
            "-----------------\n",
            "[' Yes', \" From the training curves for the ACGAN, one can infer that the model is being trained on a large dataset, as it is running all approaches. The generator and discriminator losses are plotted in orange and blue lines, which can help in understanding the performance of the model and the effectiveness of the training process. The model's performance can be evaluated based on the training curves, and any improvements can be made to enhance the model's accuracy and efficiency.\"]\n",
            "-----------------\n",
            "[' Yes', ' The model that performs better is the one with the blue line.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets was WGAN. This method also achieved the highest initial IS score.']\n",
            "-----------------\n",
            "[' Yes', ' The Euclidean Bregman cost function is based on the Euclidean distance between the image and the reference image, while the Mahalanobis Bregman cost function is based on the Mahalanobis distance between the image and the reference image. The Mahalanobis distance is a measure of the distance between two multivariate normal distributions, and it is a combination of the Euclidean distance and the cosine of the angle between the two distributions.']\n",
            "-----------------\n",
            "[' Yes', ' The Multi-DPP module increases diversity within the selected time-steps by applying a combination of cross-entropy and representative views. Cross-entropy is used to learn representative view(s) at each time-step, while the representative views are selected to ensure that the diversity of the selected time-steps is maximized. This helps to improve the accuracy of the model and to better represent the data being analyzed.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the model increases as the number of views increases.']\n",
            "-----------------\n",
            "[' Yes', ' The highest F1 score on the Lobby dataset was achieved by the Random Walk method.']\n",
            "-----------------\n",
            "[' Yes', ' The three goals that the proposed algorithm must achieve simultaneously are: 1) to generate a complete image from key local patches without geometric priors, 2) to restore broken pieces of ancient ceramics found in ruins, and 3) to synthesize an image from key local patches without geometric priors.']\n",
            "-----------------\n",
            "[' Yes', \" The network's focus changes as the training epoch increases by generating more realistic and diverse masks and images at different epochs.\"]\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the input patches and the generated images is that the input patches are local patches from the image \"Real,\" and the generated images are the results of the image generation process. The input patches are used as input to the image generation algorithm, which then generates the images based on the input patches.']\n",
            "-----------------\n",
            "[' Yes', \" The inputs to the image generation network are the image, the image's features, and a random noise vector.\"]\n",
            "-----------------\n",
            "[' Yes', ' The presence of noise in the input image can affect the quality of the generated images by introducing distortions and artifacts. When a Gaussian noise is added at each pixel of the input image, the generated images can have a more realistic and diverse appearance. However, the quality of the generated images can also be affected by the complexity of the noise and the specific parameters used in the image generation process. In some cases, the presence of noise can lead to better visual results, while in other']\n",
            "-----------------\n",
            "[' Yes', ' The model that achieved the highest performance score on the Breakout game is the one that uses a combination of DQN and A3C.']\n",
            "-----------------\n",
            "[' No', ' The game with the highest code loss in phase 2 is \"Frozen Fractal Puzzle\".']\n",
            "-----------------\n",
            "[' Yes', ' The difference between the predicted frame and the reconstructed frame for each task domain is that the predicted frame is generated by the prediction model, while the reconstructed frame is generated by the autoencoder trained with both reconstruction loss and code matching loss. The autoencoder generates slightly blurred frames compared to the ground-truth frame seen by the agent.']\n",
            "-----------------\n",
            "[' Yes', ' The encoder network in the action-conditional prediction model is responsible for processing the input data and generating a feature representation. The decoder network then takes this feature representation and generates the predicted output. The main difference between the encoder and decoder networks is that the encoder network is trained to extract relevant features from the input data, while the decoder network is trained to reconstruct the original input data from the feature representation.']\n",
            "-----------------\n",
            "[' Yes', ' The policy that resulted in the highest average cost is RQ-ALOQ.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm that performs the best on the robotic arm joint breakage task is the one that is red.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm that performs the best in the Joint Breakage experiment is AlgoQ.']\n",
            "-----------------\n",
            "[' Yes', ' Reinforce']\n",
            "-----------------\n",
            "[' Yes', ' The most efficient method in terms of runtime for both F-SRE1 and F-SRE2 is WSN.']\n",
            "-----------------\n",
            "[' Yes', ' The method that performs the best on the F-SRE1 test function is the one with the red line.']\n",
            "-----------------\n",
            "[' Yes', ' The predicted return changes as a function of θ for a fixed value of π = 1.5. The image shows a graph with different values of θ, and the return is plotted on the y-axis. The x-axis represents the different values of θ. The graph shows how the return changes as the value of θ increases or decreases. The uncertainty associated with the different values of θ is also shown, providing a better understanding of the predicted return and']\n",
            "-----------------\n",
            "[' Yes', ' The \"True max\" curve represents the maximum value of the function, while the \"ALOQ\" curve represents the true maximum value of the function. The \"ALOQ\" curve is a step function that has been unwrapped from its original form, which is a periodic function. This unwrapping process is done to simplify the function and make it easier to analyze.']\n",
            "-----------------\n",
            "[' Yes', ' The method that performs the best on the Branin function is the one that is labeled as \"flower is better\" in the image.']\n",
            "-----------------\n",
            "[' Yes', ' Increasing the training set size, as shown in the table, can improve the performance of the lane marking and drivable area segmentation tasks. This is because a larger training set provides more diverse and representative examples for the model to learn from, which can lead to better generalization and robustness. However, it is important to balance the trade-off between training set size and computational resources, as larger training sets can also increase the computational cost and time required for training.']\n",
            "-----------------\n",
            "[' Yes', ' Joint training with the object detection set can significantly improve instance segmentation performance. This is likely due to the additional localization supervision provided by the object detection set. The presence of multiple instances in the image can make it more challenging to accurately identify and separate each instance. By incorporating the object detection set into the training process, the model can learn to better distinguish between instances, leading to improved instance segmentation results.']\n",
            "-----------------\n",
            "[' Yes', ' The largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks, is for the category of \"car\".']\n",
            "-----------------\n",
            "[' Yes', ' The segmentation model learns to interpolate in areas with no lane markings, which means it can predict the drivable area even in the absence of clear lane markings.']\n",
            "-----------------\n",
            "[' Yes', ' The most common type of scene in the dataset is a street scene.']\n",
            "-----------------\n",
            "[' Yes', ' The different types of annotations included in the dataset are scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.']\n",
            "-----------------\n",
            "[' Yes', ' The model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP, because the detection set provides diverse instance examples that help the model generalize better. The detection set contains various instances of objects, which the model can learn from and improve its performance in the long run.']\n",
            "-----------------\n",
            "[' Yes', ' The larger impact on object detection performance is between daytime and nighttime.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed dataset has a lower number of persons per image compared to the Cityscapes dataset because it is focused on a specific task, such as detecting pedestrians in a specific location, like a parking lot. This specific task requires a smaller number of images to train the model effectively. On the other hand, the Cityscapes dataset is more diverse, containing various scenes and locations, which might require a larger number of images to train the model accurately.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of lane marking detection changes with different thresholds (τ) for direction, continuity, and category. The table in the image shows the results of the individual lane marking task and the joint training of lane marking and the drivable area detection. The ODS-F scores are reported for different thresholds, including 1, 2, and 10 pixels of direction, continuity, and each category. The table provides a clear comparison']\n",
            "-----------------\n",
            "[' Yes', ' The highest classification accuracy is for the weather condition \"Sunny\".']\n",
            "-----------------\n",
            "[' Yes', ' The most common object in the dataset is a car.']\n",
            "-----------------\n",
            "[' No', ' The table shows that the Lane and Drivable area approaches achieved the highest mean IoU for semantic segmentation. However, their overall accuracy drops compared to the baseline Sem-Seg model. This suggests that while these approaches may be effective in identifying specific road and sidewalk areas, they may not perform as well in terms of overall accuracy compared to a more general model.']\n",
            "-----------------\n",
            "[' Yes', ' The training approach that achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score, is the training set AP + MOTS.']\n",
            "-----------------\n",
            "[' Yes', ' The category with the highest total number of annotations is \"Tracks\".']\n",
            "-----------------\n",
            "[' Yes', ' 10%']\n",
            "-----------------\n",
            "[' Yes', ' MOTS datasets like KITTI MOTS and MOTS Challenge are smaller in size compared to VOS datasets like YouTube VOS because they are designed for specific tasks, such as object detection or tracking, whereas VOS datasets are more general and diverse in nature. The smaller size of MOTS datasets allows for more focused and efficient training of the models, which can lead to better performance in specific tasks.']\n",
            "-----------------\n",
            "[' Yes', ' The three main geographical regions where the data for this study was collected are the United States.']\n",
            "-----------------\n",
            "[' Yes', ' The least common category of object in the dataset is \"truncated\".']\n",
            "-----------------\n",
            "[' Yes', ' The BDD100K dataset is larger than the KITTI and MOT17 datasets, as it has more sequences, frames, identities, and box annotations. This suggests that the BDD100K dataset is more complex and may require more computational resources and expertise to analyze and process.']\n",
            "-----------------\n",
            "[' Yes', ' BD100K']\n",
            "-----------------\n",
            "[' Yes', ' The effect of downsampling on the quality of reconstructed frames is that as the resolution increases, the reconstructed frames become more recognizable.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed method outperforms H.264 in terms of MS-SSIM score at low bitrates.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the second-stage decoder $D\\\\_2$ is to generate reconstructed frames from the highly compressed frames (xG) using a generative model.']\n",
            "-----------------\n",
            "[' Yes', ' The second encoding stage involves the use of a specific encoding algorithm to convert the image data into a compressed format. This stage is crucial in reducing the size of the image file, which can be essential for faster transmission and storage. The specific encoding algorithm used in this stage is not provided in the image, but it is known that the algorithm is designed to minimize the loss of image quality while reducing the file size.']\n",
            "-----------------\n",
            "[' Yes', ' The highest compression gain in the example shown in Figure 1 was achieved by the Huffman coding scheme.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed model demonstrates significantly better visual quality at low bitrates compared to H.264.']\n",
            "-----------------\n",
            "[' Yes', ' The level of quantization affects the output of the soft edge detector by altering the amount of detail and resolution in the image. In the image, there are three different levels of quantization, as indicated by the three different grayscale histograms. The higher the quantization level, the less detail is preserved in the image, resulting in a more simplified and less detailed output. Conversely, the lower the quantization level, the more detail is preserved, resulting in a more']\n",
            "-----------------\n",
            "[' Yes', ' The quality of the reconstructed frames increases as the resolution increases, as shown in the image.']\n",
            "-----------------\n",
            "[' Yes', ' The quality of the reconstructed frames increases as the quantization level of the soft edge detector increases. This is because a higher quantization level allows for more precise and accurate representations of the original frames.']\n",
            "-----------------\n",
            "[' Yes', ' The quality factor that improves the most as k is increased is the video quality assessment of reconstructed frames in Figure 7.']\n",
            "-----------------\n",
            "[' No', ' The model is discouraged because the generated response (RSP) does not incorporate relevant content from the N-best response candidates (C#1 and C#2). The model is designed to learn from the examples it sees, and if the generated response does not include the relevant content from the N-best response candidates, it may not be learning effectively. This could be due to a lack of diverse examples, or the model may not be able to accurately identify the relevant content']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the accuracy of the discriminator in the proposed approach compared to the conventional discriminator in AL. The proposed approach has a higher accuracy rate, which suggests that the discriminator in the proposed approach is performing better than the conventional discriminator in AL.']\n",
            "-----------------\n",
            "[' Yes', \" The discrepancy between the number of messages and responses in each dataset could be due to various factors. For example, in the training dataset, there might be more messages sent than responses received, as the focus is on training the model. In contrast, the validation dataset might have a more balanced ratio of messages and responses, as it is used to evaluate the model's performance. The test dataset could also have a similar ratio of messages and responses, as it is used to assess the\"]\n",
            "-----------------\n",
            "[' Yes', ' The discriminator in the proposed REAT approach uses the N-best response candidates as input to guide the generator.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a series of faces with red dots on them. The faces are displayed in two different ways, one with a red dot and the other without a red dot. The image is captioned with the text \"Qualitative results of ablation.\" This suggests that the image might be related to medical research or a study on the effectiveness of different methods in generating realistic faces. The comparison between the two methods could be based on the quality of the faces, the level of detail']\n",
            "-----------------\n",
            "[' Yes', ' The effect of adding DA to the baseline method is to improve the performance of the model.']\n",
            "-----------------\n",
            "[' Yes', ' The method that performed the best on the GRID dataset is the one that is being evaluated on the GRID dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The frame discriminator is responsible for identifying the frame of the image that contains the face of the man.']\n",
            "-----------------\n",
            "[' Yes', ' The dynamic attention block improves the transition of generated video for arbitrary identities by incorporating multiple attention maps into the video generation process. This allows the model to focus on different aspects of the video, such as facial expressions, body language, and other visual cues, and generate more realistic and engaging video content.']\n",
            "-----------------\n",
            "[' Yes', ' AMIE']\n",
            "-----------------\n",
            "[' Yes', ' The limitations of the Zhou \\\\textit{et al.} and Chen \\\\textit{et al.} methods for generating talking-face videos, as compared to the method proposed in the paper, include the lack of a real-time video input, the use of a single image as the input, and the inability to generate videos with multiple facial expressions. The proposed method, on the other hand, can generate videos with multiple facial expressions in real-time, using a video input as the']\n",
            "-----------------\n",
            "[' Yes', ' The best method according to the LMD metric is the one with the highest score.']\n",
            "-----------------\n",
            "[' No', ' The algorithm with the fastest runtime is the one that uses the correlated Erdoğlu-Rényi model.']\n",
            "-----------------\n",
            "[' Yes', ' The GDU and HFLU modules in the FAKEDETECTOR framework are responsible for detecting and preventing fake data input.']\n",
            "-----------------\n",
            "[' Yes', \" The three main components of the Action Search model architecture are: (i) a visual encoder that transforms the visual observation extracted from the model's current temporal location to a representative feature vector; (ii) an LSTM that consumes this feature vector plus the state and temporal location produced in the previous step; and (iii) the LSTM that outputs its updated state and the next search location.\"]\n",
            "-----------------\n",
            "[' Yes', ' The method that requires the fewest observations to spot an action in a video with 2.5% action coverage is the Direction Baseline.']\n",
            "-----------------\n",
            "[' Yes', ' Action Search uses temporal context to reason about where to search next by analyzing the video frames and identifying the target action location. In the image, the left column shows examples when the model successfully spots the target action location, while the right column shows failure cases where the action location is not spotted exactly. The model often oscillates around actions without spotting frames within the exact temporal location. This information helps the model to adjust its search strategy and improve its performance in identifying the target action']\n",
            "-----------------\n",
            "[' Yes', ' The training size of the Action Search model affects its performance by improving the mAP and S score as it increases.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the most accurate results for simultaneous line and circle fitting is the Pearl method.']\n",
            "-----------------\n",
            "[' Yes', ' The lowest average misclassification error for the cubechips image pair is 1.17%.']\n",
            "-----------------\n",
            "[' Yes', ' AdelaideRMF is a specific example of a Multi-X plane, which is a type of aircraft that uses a specific point system to assign points to different areas of the plane. The colors in the image indicate the Multi-X assigned points to.']\n",
            "-----------------\n",
            "[' Yes', ' The color of the points in the image indicates the motion of the object. The green points represent fast motion, while the blue points represent slow motion. This relationship helps to convey the motion of the object in the scene.']\n",
            "-----------------\n",
            "[' Yes', ' The lowest average misclassification error was achieved using the method called \"RPA RMSA\".']\n",
            "-----------------\n",
            "[' Yes', ' In the table, Multi-X performed worse than another method in terms of misclassification error for simultaneous plane and cylinder fitting.']\n",
            "-----------------\n",
            "[' Yes', \" The Mean-Shift algorithm performs well in the presence of outliers. In the image, there are red dots, which are the outliers, and the algorithm is able to identify and separate them from the rest of the data. This is evident by the fact that the algorithm is able to generate a model parameter domain that includes the outliers, as well as the modes (green dots) and the ground truth instance parameters (red dots). The algorithm's ability to handle outliers\"]\n",
            "-----------------\n",
            "[' Yes', ' The method with the lowest average misclassification error is the one that uses the median method.']\n",
            "-----------------\n",
            "[' No', ' Based on the table provided, it appears that T-Linkage is generally faster for fitting planes and cylinders compared to Multi-X. The table shows that T-Linkage has a lower average processing time for this specific task. However, it is important to note that the processing times may vary depending on the specific problem and the complexity of the data being analyzed.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the performance of different methods for multi-shot and single-shot person re-identification on the BIWI dataset. The highest Top-1 Accuracy for multi-shot person re-identification is achieved by the method labeled \"3D-RAN\", which has a Top-1 Accuracy of 80.8%. In comparison, the best single-shot method, labeled \"3D-RAN\", has a Top-1 Acc']\n",
            "-----------------\n",
            "[' Yes', ' The proposed split-rate RGB-to-Depth transfer scheme is different from the R3D method of Yosinski et al. in that it uses a different initialization and learning rate for each layer. The R3D method uses a single initialization and learning rate for all layers, while the proposed scheme uses different initialization and learning rates for each layer. This allows for more flexibility in training and may lead to better performance.']\n",
            "-----------------\n",
            "[' Yes', ' The grayscale depth representation is a black and white image, while the result after background subtraction is a color image. The grayscale depth representation is a 2D image, while the result after background subtraction is a 3D image.']\n",
            "-----------------\n",
            "[' Yes', ' The highest top-1 accuracy in the multi-shot evaluation on TUM-GAID was achieved by the body depth and head RGB modality.']\n",
            "-----------------\n",
            "[' Yes', ' The Bernoulli parameter is a measure of the randomness of a sequence of events. In the image, the parameter is printed next to the sequence of images, indicating that the sequence is random and follows a Bernoulli distribution.']\n",
            "-----------------\n",
            "[' Yes', ' The Reinforced Temporal Attention (RTA) unit fw is responsible for deciding which frames are most important for the re-identification task.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of our RGB-to-Depth transfer compares favorably to Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned.']\n",
            "-----------------\n",
            "[' Yes', ' The difference between the filter responses from the \"conv1\", \"conv2\", and \"conv3\" layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data is that the first one uses RGB color information, while the second one uses depth information. This means that the first filter may rely more on color and texture information, while the second filter may']\n",
            "-----------------\n",
            "[' Yes', ' The best performing purely supervised method on the SICK dataset according to the MSE metric is the one with the lowest MSE value.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of pre-training with the ordering task on the ROUGE-L score for extractive summarization is that it improves the score.']\n",
            "-----------------\n",
            "[' Yes', ' The Entity-Grid model performs the best for the order discrimination task on the Accidents dataset. It outperforms the other data-driven approaches, including HMM, Graph, Window network, and sequence-to-sequence models.']\n",
            "-----------------\n",
            "[' Yes', ' Based on the t-SNE embeddings, it appears that the sentences in a document are related to each other, as they are all part of the same text. The color-coded positions of the sentences in the document suggest that they are arranged in a way that reflects their semantic relationships. This could indicate that the sentences are organized in a way that helps the reader understand the context and meaning of the text.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed model is more accurate than the other models on the NIPS Abstracts dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The code layer in the HDMF architecture is responsible for implementing the algorithms and processing the data.']\n",
            "-----------------\n",
            "[' Yes', ' The model that achieved the best overall performance in terms of ranking relevant tags for users is the one that is recommended by the table.']\n",
            "-----------------\n",
            "[' Yes', ' There are 1000 negative samples in the training set of the CNSE dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the accuracy and F1-score results of different algorithms on CNSE and CNSS datasets. The best performance on the CNSS dataset in terms of F1-score is achieved by the \"Variant 1\" model. The key components of this model include the use of a convolutional neural network (CNN) architecture, a pre-trained model, and a fine-tuning process on the CNSS dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents include: 1) Data collection and preprocessing, 2) Graph convolutional network classification, 3) Graph convolutional network training, and 4) Post-processing and evaluation.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm with the highest percentage of switched instances is the one that uses the synthesis algorithm.']\n",
            "-----------------\n",
            "[' Yes', ' The accuracy of the US-BS-MQ method is higher than that of the S-MQ method when adding SST examples.']\n",
            "-----------------\n",
            "[' Yes', ' The three types of sentences that the annotators are asked to write are: one sentence, three sentences, and a line from a non-fiction article.']\n",
            "-----------------\n",
            "[' Yes', ' The word that has the greatest difference in frequency of occurrence between MultiNLI and SNLI is \"dev.\"']\n",
            "-----------------\n",
            "[' Yes', \" The model that performs better on the MultiNLI dataset when considering the percentage of individual labels that match the author's label is the SNLI model.\"]\n",
            "-----------------\n",
            "[' Yes', ' The genre in the MultiNLI corpus with the highest percentage of sentences where the Stanford Parser produced a parse rooted with an \\'S\\' (sentence) node is \"Travel\". It has a percentage of 12.5, which is higher than the overall average for the corpus.']\n",
            "-----------------\n",
            "[' Yes', ' The ESIM model performs better when trained on both MNLI and SNLI combined, as indicated by the higher accuracy scores in the table.']\n",
            "-----------------\n",
            "[' No', ' The camera yaw angle is related to the silhouette distance, as the angle of the camera affects the perspective and the distance at which the subject is captured. The closer the camera is to the subject, the wider the angle will be, and the farther away the camera is, the narrower the angle will be. This relationship is important in photography, as it helps to capture the subject in the desired perspective and distance.']\n",
            "-----------------\n",
            "[' Yes', ' HUMBI has a larger number of subjects compared to other datasets.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the most concentrated distribution of gaze and head pose is the UT-Multiview dataset.']\n",
            "-----------------\n",
            "[' Yes', \" The differences between the results of the monocular 3D body prediction network trained with different dataset combinations are shown in the image. The top and bottom show the results tested on UP-3D and HUMBI Body, respectively. The image displays a variety of body poses and outfits, which are used to train the network. The network's performance can be affected by the quality and diversity of the training data, so the results may vary depending on the dataset used.\"]\n",
            "-----------------\n",
            "[' Yes', ' The image shows the different stages of HUMBI body and cloth reconstruction. The left side of the image shows the initial stage, where a woman is standing next to a mannequin. The center of the image displays the next stage, where a woman is standing next to a mannequin with a blue shirt on. The right side of the image shows the final stage, where a woman is standing next to a mannequin with a blue shirt on. The image also shows']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the results of the evaluation of 3D body keypoint prediction models. The table has four columns, each representing a different dataset. The first column is labeled \"Training,\" and the second column is labeled \"Testing.\" The third column is labeled \"Training + Testing,\" and the fourth column is labeled \"Training + Testing + Humibi.\" The table shows the average AUC of PCK calculated over an error range of 0']\n",
            "-----------------\n",
            "[' Yes', ' The lowest prediction error for both UP-3D and HUMBI test sets was achieved using the training data configuration of UP-3D.']\n",
            "-----------------\n",
            "[' Yes', \" The number of cameras used affects the accuracy of the garment reconstruction by providing more detailed information about the subject. In the image, there are multiple cameras, which allows for a more comprehensive understanding of the subject's features, leading to a more accurate reconstruction.\"]\n",
            "-----------------\n",
            "[' Yes', ' HUMBI captures diverse appearance of human expressions by using 107 high-definition cameras to capture the expressions of 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition.']\n",
            "-----------------\n",
            "[' Yes', ' The median appearance is the average or typical appearance of the object, while the view-specific appearance is the specific appearance of the object as it appears in a particular view.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the decoder in the 3D mesh prediction pipeline is to reconstruct the 3D mesh from the predicted 2D mesh.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that provides data for both facial expressions and full-body motion capture, including clothing, in a natural setting (not synthesized) is the \"Human body expression datasets\" dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performed best on average across all tasks is the one that is trained to compose a sentence representation from pretrained word vectors. This outperforms averaging word vectors, which supports the argument that learning helps to produce higher-quality sentence representations.']\n",
            "-----------------\n",
            "[' Yes', ' The corpus with more sentences is U.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model on the STS16 task with unsupervised training is the one with the underlined number.']\n",
            "-----------------\n",
            "[' Yes', ' The number of state-action pairs affects the reward landscape by determining the complexity of the model and the number of parameters that need to be learned. A larger number of state-action pairs can lead to a more complex model, which may require more data and computational resources to train effectively. On the other hand, a smaller number of state-action pairs may lead to a simpler model, which may be easier to train and less prone to overfitting. The optimal number of state-']\n",
            "-----------------\n",
            "[' Yes', ' The number of state-action pairs affects the optimization landscape for the PPO algorithm by determining the complexity of the reward landscape. In the image, there are two graphs showing the relationship between the number of state-action pairs and the optimization landscape. The graphs indicate that as the number of state-action pairs increases, the optimization landscape becomes more complex, making it more challenging to optimize the algorithm. This highlights the importance of carefully selecting the number of state-action pairs to achieve a']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a comparison of the convergence of TRPO and PPO to the true gradient. The x-axis represents the number of state-action pairs used to obtain the gradient estimates, while the y-axis represents the cosine similarity between the true gradient and the gradient estimates. The colored lines represent the convergence of different trained agents, and the dotted vertical black line indicates the sample regime used for gradient estimation in standard practical implementations of policy gradient methods. The image provides a visual representation of the']\n",
            "-----------------\n",
            "[' Yes', ' The landscape concentration of the humanoid-v2 PPO policy changes as the number of state-action pairs increases.']\n",
            "-----------------\n",
            "[' Yes', ' The quality of gradient estimation improves as the number of state-action pairs used in estimation increases.']\n",
            "-----------------\n",
            "[' Yes', \" The model that performs best when trained on the NYT dataset and evaluated on the WT14 dataset is the one that is trained with content-based sources, as indicated by the table. This model's performance is significantly better than the baselines, as indicated by the ↑ and ↓ symbols in the table.\"]\n",
            "-----------------\n",
            "[' Yes', ' The estimator that performs best in the presence of noisy confounders is the Entropy Balancing (EB) estimator. It outperforms all other estimators, including the Covariate Control (CC) estimator, in terms of accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The ATE estimator that is most affected by the presence of noisy confounders is the one that uses the weighted average of the two estimators.']\n",
            "-----------------\n",
            "[' Yes', ' The estimator that has the smallest bias and best MSE performance in the case of fully observed confounders is CC.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the audio encoder in the proposed conditional recurrent adversarial video generation network structure is to encode the audio signal from the input image. This encoded audio signal is then used as input to the video generation network, which generates the video output based on the audio and image inputs.']\n",
            "-----------------\n",
            "[' Yes', ' The loss function combination that is most important for generating realistic mouth movements is Lrec.']\n",
            "-----------------\n",
            "[' Yes', ' The sequential generation scheme generates a single image at a time, while the recurrent generation scheme generates multiple images at once. This means that the recurrent scheme can generate a more diverse and complex image, as it can incorporate information from multiple images into a single output.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs the best at SNR=0dB is the one with the highest NMSE value.']\n",
            "-----------------\n",
            "[' Yes', ' The square hashing process works by dividing the image into smaller squares, and then applying a mathematical formula to each square. This formula is used to create a unique code for each square, which is then combined to create a unique code for the entire image. This process is used to compress and store images, as well as to encrypt and decrypt data.']\n",
            "-----------------\n",
            "[' Yes', ' The buffer percentage is related to the width of the room, as it is a measure of the amount of space available for the occupants. A higher buffer percentage indicates that there is more space available, while a lower buffer percentage suggests that the room is more cramped. The width of the room can affect the buffer percentage by determining the amount of space available for the occupants to move around comfortably.']\n",
            "-----------------\n",
            "[' Yes', ' The edges in the graph stream are aggregated by grouping them together.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a graph with four different graphs, each representing a different configuration of GSS and TCM. The graphs are labeled with the names of the configurations. The Average Relative Error (ARE) of node queries is shown to change as the width increases for each configuration. The graphs show the relationship between the width and the ARE, providing insight into how the configuration affects the performance of the system.']\n",
            "-----------------\n",
            "[' No', ' The table and the graph sketch in the figure are related in that the table is displaying the graph sketch. The table is showing the graph sketch G, which is a sample map function.']\n",
            "-----------------\n",
            "[' Yes', ' The graph showing the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method is the one with the blue line.']\n",
            "-----------------\n",
            "[' Yes', ' The fastest data structure for updating on the email-EuAll dataset is the tcm data structure.']\n",
            "-----------------\n",
            "[' Yes', ' The average precision of TCM(256*memory) is compared to the other two algorithms in the email-EuAll dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The query type that has the highest accuracy when M/|V| is small is the \"Bilinear\" query.']\n",
            "-----------------\n",
            "[' Yes', ' 2001']\n",
            "-----------------\n",
            "[' Yes', ' The latent variable is represented by the shaded node in the generative model.']\n",
            "-----------------\n",
            "[' Yes', ' The best NPMI scores in the unsupervised setting are achieved by the Sage model. However, this model exhibits a trade-off in terms of accuracy compared to other models.']\n",
            "-----------------\n",
            "[' Yes', ' The highest internal coherence value is for the topic \"Gun control\".']\n",
            "-----------------\n",
            "[' Yes', ' The model that performed best on the IMDB dataset for classifying documents with categorical labels was the Scholar model. It performed 7.8% better than the SLDA model.']\n",
            "-----------------\n",
            "[' Yes', ' The topic most likely to be associated with a pro-immigration stance is \"pro-immigration\".']\n",
            "-----------------\n",
            "[' Yes', ' The model captures different perspectives on immigration by analyzing the text and categorizing it into topics related to immigration. The table shows the top words for topics on the left, and the corresponding anti-immigration and pro-immigration variations when treating tone as a covariate. This allows for a better understanding of the different viewpoints on immigration and how they may be influenced by the tone of the discussion.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the results of two different methods, FISVDD and Incremental SVM, on various datasets. The table indicates that the Incremental SVM generally achieved a lower objective function value (OFV) for the different datasets compared to FISVDD. This does not necessarily imply that one method is definitively better than the other, as the results may vary depending on the specific dataset and the context in which the methods are applied.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows three graphs, each representing the effect of the alpha parameter on recommendation accuracy for different sample sizes on the CLASS dataset. The graphs are labeled with the letters A, B, and C. The graphs show that the accuracy increases as the sample size increases, with the alpha parameter playing a role in determining the accuracy. The graphs indicate that the optimal alpha value is around 0.5.']\n",
            "-----------------\n",
            "[' Yes', ' The combination of method and dataset that achieved the highest Recall@20 score was the one with the highest Recall@20 score, which was 10% higher than the original GRU4Rec model.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of GRU4Rec relative to the baseline in terms of watch time is 100%.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the most interactions in the training set is the one with the number 15. It is 15 times larger than the dataset with the least interactions in the training set.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of negative sampling is to improve the efficiency of the learning process by reducing the number of examples that the model has to process.']\n",
            "-----------------\n",
            "[' Yes', ' The training time of the different losses changes as the number of additional samples increases.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the highest Recall@20 and MRR@20 is the one with the highest number of data points.']\n",
            "-----------------\n",
            "[' Yes', \" The addition of negative samples to the minibatch samples can affect the gradient of BPR and BPR-max by providing more context and diversity to the training process. This helps the model to better understand the target score and its relationship to the rank of the target item. The inclusion of negative samples can improve the model's ability to generalize and make accurate predictions, ultimately leading to better performance on the target task.\"]\n",
            "-----------------\n",
            "[' Yes', ' The two auxiliary tasks used in the ESMM architecture for CVR modeling are CTR and CTCVR.']\n",
            "-----------------\n",
            "[' Yes', ' Clicks are the number of times a user has clicked on an ad, while impressions are the number of times an ad has been displayed.']\n",
            "-----------------\n",
            "[' Yes', ' The click events are more prevalent than the conversion events in the Product Dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model is AUC.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a comparison of different models on the CVR task and CTCVR task with different training set sizes. The models are compared based on their performance on the Product Dataset. The image also shows a comparison of the ESMM model with other models on the CVR task and CTCVR task with different sampling rates. The image provides a visual representation of the performance of the models on the different tasks and sampling rates.']\n",
            "-----------------\n",
            "[' Yes', ' LSTM network is used to process the input data and generate the output.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the SIM saliency map improves as the number of fixations increases.']\n",
            "-----------------\n",
            "[' Yes', \" The table shows the results of different saliency map methods, with one method achieving the highest score for the sAUC metric. This method's performance is compared to other methods based on this metric.\"]\n",
            "-----------------\n",
            "[' Yes', ' The ground truth fixation density is used to predict different saliency maps depending on the intended metric. The saliency maps reflect the same underlying model, but they differ dramatically due to the different properties of the metrics. The predicted saliency map for the specific metric yields the best performance in all cases.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the number of fixations and the CC score is that the more fixations are sampled, the higher the CC score will be. This is because the mean empirical saliency map, which is created by averaging the empirical saliency maps, becomes a better approximation of the optimal saliency map for the CC metric as more fixations are sampled.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the fixation density map and the ground truth fixations is that the map is a prediction of the fixation density for a given stimulus, while the ground truth fixations are the actual fixation densities observed during an experiment. The map is expected to closely match the ground truth fixations, but due to limitations in the model, there may be deviations between the two.']\n",
            "-----------------\n",
            "[' Yes', ' The main difference between the Multilinear Conditioning architecture and the Randomized Multilinear Conditioning architecture is the way they handle the cross-domain gap. In Multilinear Conditioning, the domain-specific feature representation is conditioned on the classifier prediction via a multilinear map, while in Randomized Multilinear Conditioning, the domain-specific feature representation is conditioned on the classifier prediction via a randomized multiline']\n",
            "-----------------\n",
            "[' Yes', ' The most effective method at separating the two classes of data points is the CDAN-f method.']\n",
            "-----------------\n",
            "[' Yes', ' CDAN+E is considered a more versatile method for unsupervised domain adaptation compared to UNIT, CyCADA, and GTA because it uses a combination of unsupervised and supervised learning techniques. This allows it to adapt to new domains with less data, making it more flexible than the other methods. Additionally, CDAN+E is designed to handle both image and video data, which can be beneficial in certain applications.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the accuracy of CDAN variants on Office-31 for unsupervised domain adaptation (ResNet). The highest accuracy is achieved by using a sampling strategy that involves random matrices in CDAN+E. This strategy leads to the highest average accuracy across all domain adaptation tasks on Office-31 compared to the performance of CDAN+E variants that use random sampling.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs the best in terms of test error is the one with the lowest test error.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that shows a higher variance in F1 score with increasing buffer size is the Netflix dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm that performs best on the ENRON dataset is the one that has the highest accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The fastest algorithm across all datasets is LSH-E.']\n",
            "-----------------\n",
            "[' Yes', ' Based on the image and caption, it is not possible to definitively determine which method performs better between GB-KMV and LSH-E. However, the performance of both methods can change with different values of eleFreq and recSize. The image shows two graphs, one for GB-KMV and the other for LSH-E, with different values of eleFreq and recSize. The performance of these methods can be affected by the specific values of these parameters']\n",
            "-----------------\n",
            "[' Yes', ' The Jaccard similarity is a measure of the similarity between two sets of items, while the containment similarity is a measure of the similarity between a set and its subset.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that requires the most storage space when using the LSH-E method is Webspam.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm that performs better in terms of F1 score and precision when the space used is 5% is the one that has a higher precision.']\n",
            "-----------------\n",
            "[' Yes', ' The method that has the highest F-1 score when space used is 10% is the one that uses the least amount of space.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the highest average record length is the one with the most green boxes.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the element-hash value pairs and the signature size is that the element-hash value pairs are used to create the signature size, which is represented by the letter \"KMV\". This is a way of ensuring the authenticity and integrity of the data being stored or transmitted.']\n",
            "-----------------\n",
            "[' Yes', ' The running time of GB-KM varies with the F-1 score, with the running time decreasing as the F-1 score increases.']\n",
            "-----------------\n",
            "[' No', ' The method with a lower running time for all datasets is LSH-E.']\n",
            "-----------------\n",
            "[' Yes', ' The containment similarity of Q in X1 is 0.3.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows two graphs, one representing the accuracy and the other representing the space utilized. The graphs indicate that the accuracy is higher when there is less space utilized. This suggests that the more efficient method for utilizing space while maintaining high accuracy is to have less space utilized.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm that performs better in terms of F1 Score and Precision on ENRON is the one that has a higher Precision value.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the performance of different algorithms for a k-shot binary classification problem. The VAGER+Voting algorithm is compared to other VAGER variants in the 1-shot and 20-shot settings. The table displays the results, which can be used to evaluate the effectiveness of each algorithm in the given conditions.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of VAGER is worse than LR for the \"Bubble\" class in the 1-shot binary classification setting because the VAGER model might not have been trained on the specific data of the \"Bubble\" class, leading to less accurate predictions. On the other hand, the LR model might have been trained on a more diverse dataset, which includes the \"Bubble\" class, resulting in better performance. This highlights the importance of training models on specific data']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the Similarity Ratio and AUC Increasing is that the AUC is increasing as the Similarity Ratio increases. This is shown by the red line on the graph, which represents the AUC, increasing as the blue dots, representing the Similarity Ratio, increase.']\n",
            "-----------------\n",
            "[' Yes', ' The method that performs best in the 10 classes 1-shot multi-class classification problem is the one that is currently being used.']\n",
            "-----------------\n",
            "[' Yes', ' The novel class is related to the top-3 most similar base classes, as they are all part of the same image.']\n",
            "-----------------\n",
            "[' Yes', ' Vager voting']\n",
            "-----------------\n",
            "[' Yes', ' An \"agent-in-place\" action refers to a specific location where an agent is performing an action, while a generic action category refers to a broader category of actions that can be performed by an agent in any location.']\n",
            "-----------------\n",
            "[' Yes', ' The Layout-induced Video Representation Network uses different types of features such as semantic feature, distance-based place discretization, and place-based feature descriptions. These features are used to model moving directions and to aggregate the place-based feature descriptions at inference level.']\n",
            "-----------------\n",
            "[' Yes', ' The 3D ConvNet is used to process the input image and extract features that are then used to discretize the image into different places.']\n",
            "-----------------\n",
            "[' Yes', ' The LIVR framework decomposes semantic features into different places by extracting place-based feature descriptions individually and then dynamically aggregating them at inference time according to the topology of the scene. This is done through a combination of spatial decomposition and neural network operations.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed method outperforms the baselines for the action \"<person, move toward (home), walkway>\" because it uses a combination of motion detection and object tracking to accurately predict the movement of people and vehicles in the scene. This allows for more accurate tracking and prediction of actions, leading to better overall performance.']\n",
            "-----------------\n",
            "[' Yes', ' The most challenging actions for the network to recognize are those that require modeling moving directions, such as walking, running, or jumping. The proposed methods, including distance-based place discretization (DDD), topological feature aggregation (Topo-Agg), and place-based feature descriptions (PD), significantly improve the average precision on almost all action categories. These methods help the network better understand the context and spatial relationships within the image, leading to improved recognition of actions that']\n",
            "-----------------\n",
            "[' Yes', ' The method with the highest T-Diff on average for the Vid4 dataset is the one with the highest average T-Diff.']\n",
            "-----------------\n",
            "[' Yes', ' RecycleGAN']\n",
            "-----------------\n",
            "[' Yes', ' The VSR model that achieves the best balance of spatial detail and temporal coherence is the one with the red-dashed-box region in the image.']\n",
            "-----------------\n",
            "[' Yes', ' The highest PSNR on the Vid4 data set is achieved by the t-diff method.']\n",
            "-----------------\n",
            "[' Yes', ' The warped triplets in the conditional VSR Ds,t play a crucial role in the image processing and analysis. They are used to generate the image data and are essential for the proper functioning of the image processing system. The warped triplets are typically generated using mathematical algorithms and are then used to manipulate the image data, allowing for various image processing tasks such as image enhancement, object detection, and image segmentation.']\n",
            "-----------------\n",
            "[' Yes', ' The PP loss is a constraint that reduces the L2 distance between the forward and backward passes in the video sequence. This helps to improve temporal coherence by removing drifting artifacts and ensuring that the output sequence is symmetric.']\n",
            "-----------------\n",
            "[' Yes', ' The Motion Compensation block in the Frame-Recurrent Generator is responsible for generating the output image by incorporating the motion information of the input image. This helps in producing more realistic and natural-looking images.']\n",
            "-----------------\n",
            "[' No', ' TecoGAN']\n",
            "-----------------\n",
            "[' Yes', ' The method that produces the most realistic results for the Vid4 scenes is TecoGAN.']\n",
            "-----------------\n",
            "[' Yes', ' The learning rate for the generator in the DsOnly model is 0.05.']\n",
            "-----------------\n",
            "[' Yes', ' Flow estimation becomes less accurate near image boundaries because the structures moving into the view can cause problems, as seen at the bottom of the images. Additionally, objects moving out of the view can also cause difficulties in accurately estimating the flow.']\n",
            "-----------------\n",
            "[' Yes', ' According to the tOF score, the method with the best perceptual performance is ENet.']\n",
            "-----------------\n",
            "[' Yes', ' The TecoGAN model generated the sharpest details in both scenes.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the UVT cycle link is to provide a continuous flow of energy in the form of electricity.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the user study is to analyze the reference videos and understand the context of the image.']\n",
            "-----------------\n",
            "[' Yes', ' The re-synthesis using DAMs and the reference image show a significant difference in the appearance of the sphere. The reference image displays a complex material under natural illumination, while the re-synthesis using DAMs appears to be a more artificial representation of the sphere. This difference in appearance could be due to the use of novel deep appearance maps (DAMs) in the re-synthesis process, which may not accurately capture the intricate details of the sphere in']\n",
            "-----------------\n",
            "[' Yes', ' The method that performs best for the \"Representation\" task when the view is \"Novel\" is \"Our Method\".']\n",
            "-----------------\n",
            "[' Yes', ' The reconstructions appear to be a close representation of the original samples, as they are able to capture the details and characteristics of the materials.']\n",
            "-----------------\n",
            "[' No', ' The representation module is used to represent the data, while the learning-to-learn module is used to learn from the data.']\n",
            "-----------------\n",
            "[' Yes', ' The representation task maps an image to RGB values using a neural network, while the learning-to-learn task uses a neural network to map an image to a deep appearance map representation.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between gloss and representation error is that gloss is a measure of the visual appeal of a surface, while representation error is a measure of how well a computer model represents the actual surface. The lower the representation error, the better the computer model will represent the actual surface.']\n",
            "-----------------\n",
            "[' Yes', ' The time needed for learning a graph with a subset of allowed edges increases as the number of edges per node increases.']\n",
            "-----------------\n",
            "[' No', ' The fastest method for computing a graph with a small average node degree is the one that uses a word2vec representation, as it involves a smaller number of nodes compared to the 68 features used in the US Census 1990 data.']\n",
            "-----------------\n",
            "[' Yes', ' The digit with the highest average squared distance to other digits in the MNIST dataset is the number 1.']\n",
            "-----------------\n",
            "[' Yes', ' The figure shows a large scale logarithmic graph with a blue line and orange dots. The blue line represents the degree of sparsity, while the orange dots are the actual degree. The graph shows that the approximate bounds of $\\\\theta$ do not accurately predict the sparsity in the \"spherical\" dataset. The actual degree is significantly different from the predicted degree, indicating that the bounds do not provide a good representation of the data.']\n",
            "-----------------\n",
            "[' Yes', ' The log model (5) is the most effective at connecting digits with larger distances.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of adding Gaussian noise to the images on the measured sparsity is that it increases the sparsity of the image. This is because the added noise causes the image to have more sparse regions, which can be measured using the sparsity measure.']\n",
            "-----------------\n",
            "[' Yes', ' The connectivity of the Daitch hard scalable model is closer to the A-NN, while the Daitch soft scalable model is between the log and the `2 model. The soft model seems to favor connections between \"1\"s, but this effect becomes worse with higher density. These algorithms fail to give reasonable graphs for densities outside a small range, making it very difficult to control sparsity.']\n",
            "-----------------\n",
            "[' Yes', \" The learned graph's relevance of terms is better than the relevance assigned by k-NN and A-NN graphs.\"]\n",
            "-----------------\n",
            "[' Yes', ' The graph diameter measures the quality of the recovery for different methods and datasets. The left graph shows a small spherical data with 4096 nodes and 1920 signals. The middle graph shows the same data with 40 signals. The right graph shows word2vec data with 10,000 nodes and 300 features. The diameter of the graph increases with increasing average degree for different methods and datasets.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that exhibits the strongest seasonality is the one with the green line.']\n",
            "-----------------\n",
            "[' Yes', ' The most significant performance of LSTNet is the AR component.']\n",
            "-----------------\n",
            "[' Yes', ' The model LSTw/oAR seems to perform better in predicting electricity consumption.']\n",
            "-----------------\n",
            "[' Yes', ' LSTNet']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the highest temporal resolution is the one that has the highest sample rate.']\n",
            "-----------------\n",
            "[' Yes', \" The LSTNet model consists of three main layers: the input layer, the hidden layer, and the output layer. The input layer receives the input data, which is then processed by the hidden layer. The hidden layer contains the majority of the model's parameters and is responsible for transforming the input data into a more suitable representation for the output layer. The output layer, also known as the activation function, is responsible for generating the final output. The hidden layer is connected to the input layer\"]\n",
            "-----------------\n",
            "[' Yes', ' The performance of LSTNet-attn varies with the horizon on the Solar-Energy dataset.']\n",
            "-----------------\n",
            "[' Yes', ' Yes, the percentage of entity pairs in the NYT training set that have a corresponding relational fact in the Knowledge Base (KB) is 100%.']\n",
            "-----------------\n",
            "[' Yes', ' Conventional distant supervision is problematic because it relies on manual annotation and labeling of textual relations, which can be time-consuming and prone to human error. Additionally, it may not accurately capture the complex relationships between different pieces of information.']\n",
            "-----------------\n",
            "[' Yes', ' The GRU cell in the embedding model is responsible for mapping a textual relation embedding to a probability distribution over KB relations.']\n",
            "-----------------\n",
            "[' Yes', ' In the telescoping architecture, documents are first scanned using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages. This process helps to ensure that the most relevant documents are retrieved and ranked appropriately.']\n",
            "-----------------\n",
            "[' Yes', ' The RL policy is better than the baseline in terms of index blocks accessed, as it has a lower reduction in index blocks compared to the baseline.']\n",
            "-----------------\n",
            "[' No', ' The learned policy shows a significant reduction in the index blocks accessed compared to the production baseline for CAT2 queries. However, there is a cost of some loss in relevance for CAT1. The performance of the learned policy is better in terms of efficiency but may have a trade-off in terms of relevance.']\n",
            "-----------------\n",
            "[' Yes', ' The induced schema Win <A4, B3, C2> represents a 3-ary schema, which is a n-ary schema for n > 3. This is based on the tri-partite graph formed from the columns of matrices A, B, and C. The triangles in the graph represent a 3-ary schema, and the schema can be induced from the 3-ary schema.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the highest value for the hyperparameter λa is the MUC dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The shape of the tensor $x^1$ for the Shootings dataset is a 3x1 matrix.']\n",
            "-----------------\n",
            "[' Yes', ' The definition of a non-negative tensor is a tensor that has a non-negative eigenvalue spectrum.']\n",
            "-----------------\n",
            "[' Yes', ' In Step 1 of TFBA, OpenIE plays a crucial role in the joint Tucker decomposition of multiple 3-mode tensors, X1, X2, and X3, derived out of X. This joint factorization is performed using shared latent factors A, B, and C. The result of this joint factorization is stored as a cell in one of the core tensors G1, G2, and G3.']\n",
            "-----------------\n",
            "[' Yes', ' TFBA achieves the highest accuracy on the Shootings dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the true market state qt and the noisy version q̂t at time t is that q̂t is a combination of qt, independent Laplace noise vector zt, and the noise added at t. The noise added at t is a sum of noises obtained by following the arrows all the way back to 0, and each arrow originates at t, points backwards to s(t), and is labeled with independent Laplace noise vector zt.']\n",
            "-----------------\n",
            "[' Yes', ' The average number of inter-word semantic connections per word increases as the value of κ increases.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model on the AddOneSent dataset is the MRC model.']\n",
            "-----------------\n",
            "[' Yes', ' The Knowledge Aided Similarity Matrix (KAR) is a key component of the KAR model, which is an end-to-end model for multimedia retrieval. The KAR model uses a similarity matrix to measure the similarity between the query and the database of multimedia content. The similarity matrix is constructed by applying a set of predefined features to the query and the database, and then computing the similarity scores based on the feature values. The KAR model then uses these similarity scores to']\n",
            "-----------------\n",
            "[' Yes', ' The performance of KAR, SAN, and QANet (without data augmentation) will decrease as the proportion of available training examples decreases.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs the best when trained on 60% of the training data is QANet.']\n",
            "-----------------\n",
            "[' Yes', ' KAR']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between position and click-through rate (CTR) is that the higher the position of an ad, the higher the likelihood of a user clicking on it. This is because users are more likely to click on an ad when it is more prominent in their view.']\n",
            "-----------------\n",
            "[' Yes', ' The advertising rate for the \"Fix\" curve is lower than the \"Oracle\" curve at hour 14.']\n",
            "-----------------\n",
            "[' Yes', ' The Higher Level Policy is responsible for making decisions and setting the overall direction for the framework.']\n",
            "-----------------\n",
            "[' Yes', \" The advertising system selects the best items to show to the user by analyzing the user's preferences, interests, and behavior. It uses machine learning algorithms to understand the user's needs and offers personalized recommendations based on their past interactions with the system. This helps to improve user engagement and satisfaction, leading to better ad performance and ROI for the advertisers.\"]\n",
            "-----------------\n",
            "[' Yes', ' The effect of using CHER on the percentage of ads displayed for each user is shown in the image. The graphs show the learning curve for DDPG with CHER compared to DDPG without CHER. The graphs indicate that using CHER can improve the performance of the DDPG algorithm.']\n",
            "-----------------\n",
            "[' Yes', ' The human evaluators were more accurate in identifying machine-generated reviews.']\n",
            "-----------------\n",
            "[' Yes', ' GAN-based generators.']\n",
            "-----------------\n",
            "[' Yes', ' The task that the AMT workers are being asked to do is to read and understand the instructions provided in the image.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the proposed model is better than other models when trained on the LDC2017T10 dataset. This suggests that incorporating syntax into the model is effective in improving its performance.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a table with two columns. The first column displays the average number of acceptable realisations out of 3. The second column displays the difference between the two columns. The difference is significant with p < 0.001. The table is labeled \"Table 2.\"']\n",
            "-----------------\n",
            "[' No', ' The table shows the parsing scores on the LDC2017T10 dev set. The table has two columns, one for the model and the other for the unlabelled F1 score. The model that performs the best at predicting the delexicalised constituency tree of an example is the one with the highest score. The baseline model is the one with the lowest score.']\n",
            "-----------------\n",
            "[' Yes', ' The parameter network deforms the initial surface by applying weighting, resulting in the deformed surface shown in the image.']\n",
            "-----------------\n",
            "[' Yes', ' The parameter network is a simple neural network consisting of two fully connected layers, while the deformation network is more complex, consisting of several de-convolutional layers. The parameter network learns how to apply multiple long-range, non-linear deformation fields, while the deformation network learns to generate dense deformation fields to refine the final surface.']\n",
            "-----------------\n",
            "[' Yes', ' The correct gradient from forward advection leads to a more stable and lower loss value during training.']\n",
            "-----------------\n",
            "[' Yes', ' The flow of water changes as the central wall obstacle is shifted to the right, as it increases correspondingly.']\n",
            "-----------------\n",
            "[' No', ' The scene that requires more computation time for rendering is the Staris scene.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the resolution of the simulation and the training time is that a higher resolution requires more computational power and time to process. This is evident from the fact that the training time increases as the resolution increases, as more data points need to be processed to achieve the desired level of accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The parameter network in the weighting and refinement stage is responsible for inferring a weighting function and a dense deformation field, respectively.']\n",
            "-----------------\n",
            "[' Yes', ' The method that is able to reconstruct the shape of the liquid properly is the one that uses a trained parameter network.']\n",
            "-----------------\n",
            "[' Yes', ' The initial conditions of the simulations vary by changing the position of the liquid drop along x as α1, and its size as α2.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of increasing K on the test PPL of the different models is that it shows that dedicated matrices should be reserved for frequent words as hypothesized.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the comparison of a DNN trained with DLA and relevance signals extracted by click models. The table indicates significant improvements or degradations with respect to DLA. The best performance in terms of nDCG@10 and ERR@10 was achieved by using the DLA correction method.']\n",
            "-----------------\n",
            "[' Yes', ' DLA']\n",
            "-----------------\n",
            "[' Yes', ' TF-IDF is a technique used to measure the importance of words in a document, while BM25 is a language model that uses statistical methods to predict the probability of a word in a given context.']\n",
            "-----------------\n",
            "[' Yes', ' The role of the auxiliary discriminator $D_{X_{\\\\textit{aux}}}$ in the Conditional CycleGAN for identity-guided face generation is to verify the identity of the generated faces. This is done by incorporating a face verification network as both the source of the conditional vector z and the proposed identity loss in the auxiliary discriminator DXaux. The network DXaux is pretrained, and it helps ensure that the generated faces are consistent']\n",
            "-----------------\n",
            "[' Yes', ' The proposed method compares well to the method in \\\\cite{kim2017learning}.']\n",
            "-----------------\n",
            "[' Yes', ' Unsupervised GAN']\n",
            "-----------------\n",
            "[' Yes', \" The proposed attribute-guided face generation method compares to conventional face super-resolution methods in terms of identity preservation by using the input photo to provide overall shape constraint and transferring the man's identity from the low-res input to the generated high-res result.\"]\n",
            "-----------------\n",
            "[' Yes', ' The low-resolution input plays a crucial role in the identity-guided face generation process. It serves as the starting point for the algorithm, which then generates a high-resolution output that closely resembles the original input. This process helps to preserve the identity of the person in the generated images, ensuring that the generated faces remain consistent with the original input.']\n",
            "-----------------\n",
            "[' No', ' When there is a conflict between the low-res image and the feature vector, the feature vector is used to generate the high-res digit images.']\n",
            "-----------------\n",
            "[' Yes', ' The difference between the input and output of the frontal face generation process is that the input is a low-resolution template, while the output is a high-resolution frontal face. The process can generate corresponding frontal faces from different side faces, as shown in the image with the six different faces.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed method preserves facial details and expression during face swapping by using a combination of face and head pose constraints. This ensures that the transformed face maintains the original facial details, such as eyes, eyebrows, and hair, while also preserving the overall head shape and facial expression. The method achieves this by using a combination of identity and appearance constraints, which helps to maintain the identity of the person while also ensuring that the transformed face looks natural and']\n",
            "-----------------\n",
            "[' Yes', ' The proposed method can generate better images compared to icGAN by swapping facial attributes and generating four paired examples.']\n",
            "-----------------\n",
            "[' Yes', ' The attribute vector $z$ plays a crucial role in the Conditional CycleGAN network. It is used to train both the generator GY→X and the original generator GX→Y. The generator GY→X generates high-res face image X̂ given the low-res input Y and the attribute vector z, while the original generator GX→Y generates high-res face image X̂ given the low-res input Y. The attribute vector $z$ is']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the performance comparison of different methods. The best performing method according to the P@1 metric for the QA-Expert task is the D2V method. It performed 15% better compared to the average P@1 score of the D2V method.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the decline probability of an expert and whether or not they have a \"friend\" who has already declined is that the expert\\'s probability of declining is higher when they have a \"friend\" who has already declined. This is shown in the image, where the orange line is higher when the \"friend\" has already declined.']\n",
            "-----------------\n",
            "[' Yes', ' The LSTM-MDN network plays a crucial role in the training phase by unrolling for 50 time-steps and using the gripper pose and status (open/close) et and the pose of relevant objects qt at time-step t as input and output of the network to calculate and backpropagate the error to update the weights. This helps in improving the performance of the robot arm during the evaluation phase.']\n",
            "-----------------\n",
            "[' No', ' The number of demonstrations after the shift is not available for the \"Push to Pose\" task because the data for that task is not present in the input image.']\n",
            "-----------------\n",
            "[' Yes', ' The frequency reduction process creates multiple trajectories from a single demonstration by taking a single trajectory and then dividing it into multiple trajectories. This is done by taking the original trajectory and then dividing it into smaller segments. Each segment is then analyzed and compared to the original trajectory to create a new trajectory. This process is repeated multiple times to create multiple trajectories from a single demonstration.']\n",
            "-----------------\n",
            "[' Yes', ' The virtual environment plays a crucial role in the proposed approach. It is used to collect the trajectories of the ADL manipulation tasks, which are then used to train the neural network controller. This allows for the development of a system that can learn from the collected data and improve its performance over time.']\n",
            "-----------------\n",
            "[' Yes', ' In simulation, the pick and place task is performed by a robot controlled by a mixture density network with 3 layers of LSTM. In the real world, the task is performed by a robot controlled by a mixture density network with 3 layers of LSTM.']\n",
            "-----------------\n",
            "[' Yes', ' The three different network architectures used in the comparison study are: Feedforward-MSE, LSTM-MSE, and Feedforward-MDN.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between projection sparsity and normalized reconstruction error is that as the projection sparsity increases, the normalized reconstruction error decreases. This is because a sparse projection matrix allows for more efficient use of the available data, leading to better reconstruction results.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between compression factor and reconstruction error is that the reconstruction error decreases as the compression factor increases. This is because the compression factor is a measure of how much data is being compressed, and as the data is compressed, the reconstruction error is reduced because the data is being reconstructed more accurately.']\n",
            "-----------------\n",
            "[' Yes', ' The three steps involved in compressed matrix factorization are: (i) the matrix M̃ is a compressed version of the full data matrix M; (ii) we directly factorize M̃ to obtain matrices W̃ and H̃; and (iii) finally, we approximate the left factor of M via sparse recovery on each column of W̃.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that would likely benefit the most from using the Fac.-Recover approach instead of Recover-Fac. in terms of computational efficiency is the one with the highest runtime (seconds) for each stage of the NMF pipeline on compressed data. This is because the Fac.-Recover approach runs only r instances of sparse recovery, as opposed to the m instances used by the alternative, Recover-Factorize. By using Fac.-Recover, the runtime can be reduced']\n",
            "-----------------\n",
            "[' Yes', ' The effect of increasing the projection dimension d on the approximation error for sparse PCA and NMF is that it reduces the approximation error.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieves lower approximation error when the compression factor is greater than 3 is the RecoverFactorize method.']\n",
            "-----------------\n",
            "[' Yes', ' The two-way AdaQA model performs better than the one-way AdaQA model and other CNN-based baseline models on the WikiQA dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The ACNN framework learns context-sensitive filters by using a combination of convolutional and filter-generating modules. The input sentences to the filter-generating module and the convolution module could be different, as seen in the image. The framework uses a combination of these two modules to learn and adapt to different contexts, ultimately improving its performance in various natural language processing tasks.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model on the Quora Question Pairs dataset is the Adaptive Query Answering (AdaQA) model.']\n",
            "-----------------\n",
            "[' Yes', ' Based on the figure, ACNN performs the best on question types that involve a large number of filters.']\n",
            "-----------------\n",
            "[' Yes', ' The Adaptive Question Answering (AdaQA) model generates context-aware filters by analyzing the input image and caption, and then using that information to adaptively select and apply the appropriate filter. This allows the model to effectively process and understand the content of the input, and generate accurate and relevant filters based on the specific context of the image and caption.']\n",
            "-----------------\n",
            "[' Yes', ' Table 1 shows the statistics for different datasets. The dataset with the largest vocabulary size is the \"WikiOQLA\" dataset, which has a vocabulary size of 3,091,439 words. This is significantly larger than the average number of words per document in that dataset, which is 6,091 words. The other datasets have smaller vocabulary sizes and average word counts.']\n",
            "-----------------\n",
            "[' Yes', ' The authors claim that their S-ACNN model with a single filter is \"much more expressive\" than the basic S-CNN model because it uses a single convolutional filter, which allows for more flexibility in capturing different features and patterns in the image. This can lead to better performance on certain tasks or datasets, even if it doesn\\'t achieve the best overall performance on either dataset. The use of multiple convolutional filters in the M-ACNN model can also improve']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model on the SelQA dataset is the one that is marked with a star. This model outperforms the baseline CNN model reported in Jurczyk et al. (2016).']\n",
            "-----------------\n",
            "[' Yes', ' Batching affects the radio duty cycle by reducing the number of radio transmissions, which in turn lowers the overall power consumption. Similarly, batching affects the CPU duty cycle by reducing the number of CPU cycles, leading to lower power consumption in that context as well.']\n",
            "-----------------\n",
            "[' No', ' The technique that was most effective at reducing memory consumption in both send and receive buffers was the \"Recycling Buffer\" technique.']\n",
            "-----------------\n",
            "[' No', \" The reason TCP performs poorly on IEEE 802.15.4 networks compared to other network types listed is due to the differences in the network's structure and the way data is transmitted. In the image, there are four tables comparing different types of networks, including TCP/IP, M2M, and Wi-Fi. The comparison shows that the TCP protocol is designed for IP networks, which are not the same as the networks used in M2M and Wi\"]\n",
            "-----------------\n",
            "[' Yes', ' Relying on fragmentation is effective for reducing header overhead because it allows for the distribution of data across multiple fragments. This helps to reduce the amount of data that needs to be transmitted, which in turn reduces the amount of overhead required to manage the data. By breaking the data into smaller pieces, it becomes easier to manage and transmit the data, ultimately leading to more efficient communication.']\n",
            "-----------------\n",
            "[' Yes', ' Varying the buffer size affects TCP goodput by allowing for more efficient data transfer. In the image, there are two graphs showing the effect of varying the buffer size on TCP goodput. The graphs indicate that by increasing the buffer size, the TCP goodput increases, resulting in better data transfer efficiency.']\n",
            "-----------------\n",
            "[' Yes', ' The function of the Hamilton-based PCB in the ultrasonic anemometer is to control and manage the electronic components of the device.']\n",
            "-----------------\n",
            "[' Yes', ' CoAP has a faster response time than HTTP/TCP for a response size of 50 KiB.']\n",
            "-----------------\n",
            "[' Yes', ' The maximum link delay affects the segment loss rate and goodput in a TCP connection with one hop by influencing the time it takes for data to be transmitted and received. A higher link delay can lead to increased segment loss rate and reduced goodput, as it may cause delays in the transmission of data packets. This can result in a decrease in the overall efficiency and performance of the TCP connection.']\n",
            "-----------------\n",
            "[' Yes', ' TCP']\n",
            "-----------------\n",
            "[' Yes', ' The maximum link delay affects the number of TCP timeouts and fast retransmissions by causing the TCP to wait for a longer period before sending a retransmission. This can lead to an increased number of timeouts and fast retransmissions, as the TCP may take longer to receive an acknowledgement from the receiver.']\n",
            "-----------------\n",
            "[' Yes', ' The module of TCPlp that consumes the most memory in the active RAM on TinyOS is the protocol implementation. It utilizes 168 MB of memory.']\n",
            "-----------------\n",
            "[' Yes', ' The reliability of CoAP is lower than TCPlp and radio DC. This difference can be attributed to factors such as the use of CoAP, which is a connection-oriented protocol, and the radio DC, which is a connection-oriented protocol. CoAP is designed to provide reliable communication, while TCPlp is a connection-oriented protocol that is not as reliable as CoAP. Additionally, radio DC is a connection-oriented protocol that is not as']\n",
            "-----------------\n",
            "[' Yes', ' The memory usage of the RIOT OS posix\\\\_sockets module is significantly lower than the memory used by the protocol and socket layer combined, for both active and passive connections.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows a comparison of core features among embedded TCP stacks, including uIP (Contiki), BLIP (TinyOS), GNRC (RIOT), and Please. The table indicates that uIP (Contiki) provides the most complete implementation of core TCP features, while BLIP (TinyOS) lacks the most features.']\n",
            "-----------------\n",
            "[' Yes', ' There are 5 hops between the Hamilton and the Internet.']\n",
            "-----------------\n",
            "[' Yes', ' The sample complexity lower bound for recovering a tree-structured sparse signal using standard compressed sensing is given by the formula:\\n\\nd \\\\* s \\\\* log(B) \\\\* log(K)\\n\\nwhere d is the dimension of the true signal, s is the signal sparsity, B is the weight budget in the weighted graph model, K is the block sparsity, and J is the number of entries in a block.']\n",
            "-----------------\n",
            "[' Yes', \" The effect of performing a left-swap on a binary vector y at index j' is that it yields a new vector z such that the number of misclassified pairs h(z, ŷ) is one more than h(y, ŷ). Specifically, ŷ misclassifies pairs (3, 4), (3, 5), (3, 7), and (6, 7) w.r.t. to y, since for each such pair (\"]\n",
            "-----------------\n",
            "[' No', ' 0.35']\n",
            "-----------------\n",
            "[' Yes', ' The distance from equitability for the allocation that can be obtained by cutting at $x$ with the player order $(1,2)$ is $b-a$.']\n",
            "-----------------\n",
            "[' Yes', ' The upper bound on the query complexity for finding an ε-perfect allocation with minimum cuts for 3 or more players is 3.']\n",
            "-----------------\n",
            "[' Yes', ' The observed error is compared to the underlying true error as CPU time increases.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of SPIRAL-DTW-kMeans compares favorably to k-Shape and CLDS, as indicated by the circles below the diagonal in the image. This suggests that the method outperforms these state-of-the-art methods in terms of clustering performance, as measured by the NMI metric.']\n",
            "-----------------\n",
            "[' Yes', ' The method that performs the best in terms of NMI is SPIRAL-MSM-kMeans. It outperforms the other methods on 100% of the datasets.']\n",
            "-----------------\n",
            "[' No', ' State-based synchronization is the most efficient in terms of CPU processing time.']\n",
            "-----------------\n",
            "[' No', ' GCounter is a single-threaded version of GSet, while GSet is a multi-threaded version.']\n",
            "-----------------\n",
            "[' Yes', ' GCounter']\n",
            "-----------------\n",
            "[' Yes', ' The RR optimization plays a crucial role in the delta-based synchronization of a GSet. It helps to optimize the performance and efficiency of the system by minimizing the time and resources required for synchronization. By implementing the RR optimization, the system can achieve better overall performance and ensure smooth and efficient operation.']\n",
            "-----------------\n",
            "[' Yes', ' The average metadata required per node for the Op-based BP+RR approach increases as the number of nodes in the network increases.']\n",
            "-----------------\n",
            "[' No', ' GMap 10%']\n",
            "-----------------\n",
            "[' Yes', \" The difference between the `inc_i(p)` and `inc_i'(p)` operations in the Grow-only Counter data type is that the former uses a function call, while the latter uses a function pointer.\"]\n",
            "-----------------\n",
            "[' Yes', ' The mesh topology has the highest transmission rate for GMap 100%.']\n",
            "-----------------\n",
            "[' Yes', ' If a user with 100 followers posts a tweet, 1 CRDT update will be performed in total. This represents 0.03% of the overall workload.']\n",
            "-----------------\n",
            "[' Yes', ' Yes']\n",
            "-----------------\n",
            "[' Yes', ' The CPU overhead of classic delta-based is lower than delta-based BP+RR as the Zipf coefficient increases.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the residual connection in the decoding module is to add the output of a layer to the output of another layer. This helps in improving the accuracy of the model and making it more robust.']\n",
            "-----------------\n",
            "[' Yes', ' The differences between the results of the three methods, LiteFlowNet, PWC-Net, and Devon, compared to the ground truth, are likely to be in terms of accuracy, precision, and overall performance. The ground truth refers to the actual, correct results, which the methods are trying to achieve. The methods may have varying levels of success in accurately predicting the desired outcome, and the differences between the results and the ground truth would indicate the effectiveness of each method']\n",
            "-----------------\n",
            "[' Yes', ' A standard cost volume is a cost volume that is not deformable, meaning it does not change shape when subjected to external forces. In contrast, a deformable cost volume is a cost volume that can change shape when subjected to external forces.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the results of ablation experiments after training on FlyingChairs. The table has several columns, including one that shows the results of the ablation study. The most significant negative impact on performance for the KITTI 2015 dataset was the removal of the shared decoder.']\n",
            "-----------------\n",
            "[' Yes', ' The fastest processing time for both forward and backward passes was achieved by the shared decoder modification. The backward pass was 147 times faster than the full model, while the forward pass was 146 times faster.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the residual connection in the encoding module is to add the output of a layer to the output of another layer. This helps in improving the performance of the neural network and making it more accurate in predicting the output.']\n",
            "-----------------\n",
            "[' Yes', ' The relation module (Rt) in the Deformable Volume Network (Devon) is responsible for determining the spatial relationship between the different stages of the network. This is crucial for accurately estimating the motion of objects in the image. The relation module takes into account the spatial information from the encoding module (f) and the decoding module (gt) to create a coherent representation of the scene.']\n",
            "-----------------\n",
            "[' No', \" The rationale behind using five deformable cost volumes with different hyperparameter settings in Devon's relation module is to optimize the performance of the algorithm. By experimenting with various hyperparameters, the researchers can find the best combination that results in the most accurate and efficient predictions. This approach allows them to fine-tune the model to achieve the desired level of accuracy and computational efficiency.\"]\n",
            "-----------------\n",
            "[' Yes', ' The table shows the results of the KITTI 2015 test set, including the F1-all score. The best performance was achieved by the model \"flow-net2\" with an F1-all score of 0.92. This outperforms the model \"Devon (ft)\" which had an F1-all score of 0.88.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows that the method \"flow-net2\" performs the best on the Sintel \"Final\" test set. It has an error rate of 0.34, which is lower than the error rate of 0.42 for the method \"Devon (ft)\". This indicates that the \"flow-net2\" method is more effective in accurately detecting objects in the Sintel dataset.']\n",
            "-----------------\n",
            "[' Yes', ' LiteFlowNet']\n",
            "-----------------\n",
            "[' Yes', ' Increasing the parameter value for LIME with Euclidean distance, LIME with cosine similarity, and Parzen seems to affect the influence vectors by making the vectors more spread out or more clustered, depending on the specific parameter value. This can impact the accuracy and precision of the model, as well as the interpretability of the results.']\n",
            "-----------------\n",
            "[' Yes', ' The method that places the most emphasis on specific, localized features rather than smooth, gradual changes in pixel intensity is called \"Influence Stripped\" or \"Influence Shifted\".']\n",
            "-----------------\n",
            "[' Yes', ' The \"Last contact\" feature is a binary feature that indicates whether a person has recently contacted the system or not. In the two examples provided, the SSL score is significantly higher when this feature is present. This is likely because the system is more likely to be in use when a person has recently contacted it, which can be indicative of a more active and engaged user. The SSL algorithm may take this into account when determining the user\\'s level of engagement and influence on the']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the highest tracking accuracy is the one with the lowest sum of absolute percentage errors. This method likely had the best overall performance, as it minimized the sum of absolute percentage errors the most. However, it is important to note that the best overall performance may not necessarily be the one with the lowest sum of absolute percentage errors, as other factors such as computational efficiency, ease of implementation, and interpretability of the results may also play a role in determining the best method for a']\n",
            "-----------------\n",
            "[' Yes', ' The test accuracy of the different models varies as the hyperparameter λ is varied. In the left plot, the accuracy is shown to decrease as the value of λ increases. In the right plot, the convergence is shown to improve as the value of λ decreases.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieves the highest Top-1 accuracy on the CUB-200-2011 dataset is Pairwise Confusion (PC).']\n",
            "-----------------\n",
            "[' Yes', ' Pairwise Confusion (PC) improves the localization ability of a CNN by reducing the number of false positives and false negatives. This is demonstrated in the image where the PC method results in more accurate localization compared to the baseline VGG-16 network.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the highest number of samples per class is the one on the right.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows that the ImageNet-Dogs dataset benefited more from the Pairwise Confusion (PC) optimization method compared to the ImageNet-Random dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The shared weights in the Siamese-like architecture shown in the first figure play a crucial role in the training process. They are responsible for combining the features from the two input images, allowing the network to learn the relationships between the images. This helps the network to make accurate predictions and improve its performance over time.']\n",
            "-----------------\n",
            "[' Yes', ' The agent that values the entire share $Z\\\\_j$ is represented by the solid boxes in the image.']\n",
            "-----------------\n",
            "[' Yes', \" The purpose of the blank space labeled Z'5 is to represent the allocation of the 5th piece in the puzzle.\"]\n",
            "-----------------\n",
            "[' Yes', ' The table shows the relationship between the complexity of a cake shape and the minimum number of blanks required for a complete partition into smaller pieces. The table is divided into two sections, one for the case of a simple cake and the other for a more complex cake. The table shows that as the complexity of the cake shape increases, the number of blanks required for a complete partition also increases. This is because more complex shapes require more precise cutting to ensure that all pieces are separated']\n",
            "-----------------\n",
            "[' No', ' The author states that there is a qualitative difference between 2-D and 1-D division because in 2-D division, the cake is divided into two equal parts, while in 1-D division, the cake is divided into one part. This difference in division can have an impact on the efficiency of the allocation and the amount of cake that is left unallocated.']\n",
            "-----------------\n",
            "[' Yes', ' The minimum number of sides that a rectilinear polygon with four reflex vertices must have is 4.']\n",
            "-----------------\n",
            "[' Yes', ' The value of the learning rate α for the BAIR dataset is 0.02.']\n",
            "-----------------\n",
            "[' Yes', ' The feature with the highest dimensionality in the first two dimensions is the \"C\" feature.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the length of the interval and the uncertainty in the generated frames is that the shorter the interval, the more uncertain the generated frames will be. This is because the camera in scenario 1 captures every other frame, while the camera in scenario 2 captures 1 frame for every 4 frames. This difference in the frame capture rate leads to a higher level of uncertainty in the generated frames in scenario 2 compared to scenario 1.']\n",
            "-----------------\n",
            "[' No', ' The \"SDVI loss term 1&3\" model likely performs worse than the full SDVI model in terms of PSNR and SSIM across all datasets because it is a simplified version of the original model. By removing some of the loss terms, the model may not be able to capture the same level of detail and complexity in the images. This can result in a decrease in the model\\'s performance in terms of image quality and perceptual similarity.']\n",
            "-----------------\n",
            "[' Yes', ' The Inference module takes the previous frame Xt−1 and the dynamic constraint ĥt at each step, while the Posterior module takes the current frame Xt. This difference results in different output frames X̃infr.']\n",
            "-----------------\n",
            "[' Yes', ' Our method can model the dynamic correctly and generate better moving objects than SuperSloMo and SepConv.']\n",
            "-----------------\n",
            "[' Yes', ' The sliding tendency of SepConv will cause motion errors and high LMS.']\n",
            "-----------------\n",
            "[' Yes', ' The residual connections in the RBConvLSTM network serve as a way to maintain the input information throughout the network, allowing the network to learn and adapt to changes in the input data.']\n",
            "-----------------\n",
            "[' Yes', ' The sampled vector is applied to all locations in the feature maps of $\\\\sigma$ and $\\\\mu$.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the most 4-hop triples is the one with the most users.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a graph with four different graphs, each representing a different dataset. The graphs are labeled with the names of the datasets, such as MovieLens-1M, Book-Crossing, and BingNews. The graphs show the average number of k-hop neighbors that two items share in the KG w.r.t. whether they have common raters in. The graphs also show the ratio of the two average numbers with different hops. The']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model in terms of AUC on the MovieLens-1M dataset is the one that has the highest AUC value.']\n",
            "-----------------\n",
            "[' Yes', \" The ripple sets in the RippleNet framework play a crucial role in predicting the user's behavior and the likelihood of them clicking on a particular item. They are activated by the user's click history and help in determining the corresponding ripple sets to be activated, ultimately leading to the output of the predicted probability of the user clicking on the item.\"]\n",
            "-----------------\n",
            "[' Yes', ' \"Forrest Gump\" and \"Cast Away\" are both movies directed by Robert Zemeckis.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the highest AUC for all ripple set sizes is the one with a size of 2.']\n",
            "-----------------\n",
            "[' Yes', ' The dimension of embedding affects the AUC of RippleNet on MovieLens-1M by reducing the performance of the model.']\n",
            "-----------------\n",
            "[' Yes', ' The most network-related reboots occurred between 10:00 and 12:00.']\n",
            "-----------------\n",
            "[' Yes', ' The model with the lowest memory consumption and time cost on synthetic data is the one with the highest sequence length.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the test accuracy and training time per epoch for different models on the SNLI dataset. The model with the highest test accuracy is the MTSA model, which has a training time per epoch of 1.6 seconds.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performed best on the SNLI test set was the one that was pretrained on a large corpus.']\n",
            "-----------------\n",
            "[' Yes', ' Based on the table, MTSA appears to have a lower performance than the Bi-LSTM and Multi-CNN baselines. This is indicated by the lower scores in the \"Precision\" column. Additionally, MTSA has a longer training time, as indicated by the longer training time in the \"Time\" column.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the positional mask in the TSA mechanism is to ensure that the attention mechanism is applied only to the relevant positions in the input image, rather than to the entire image. This helps to improve the accuracy and efficiency of the model.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performed the best on the test data is the one with the highest accuracy score.']\n",
            "-----------------\n",
            "[' Yes', ' The size of the reference set used for DMP training differs between the Purchase/Texas datasets and the CIFAR datasets. In the Purchase/Texas datasets, the reference set consists of all the data, while in the CIFAR datasets, the reference set consists of only a portion of the data. The rationale behind this difference is that the CIFAR datasets are designed to be more challenging for the adversary, so a smaller reference set is used to make']\n",
            "-----------------\n",
            "[' Yes', ' The lower the entropy of predictions of unprotected model on Xref, the higher the membership privacy.']\n",
            "-----------------\n",
            "[' Yes', \" The model that performs the best for caption retrieval in terms of R@1 and MEDR is the one with the highest R@1 score and the lowest MEDR score. This model likely has a better balance of precision and recall, resulting in a higher overall performance. The reason for this better performance could be due to the model's ability to accurately capture the context and content of the image, leading to more accurate caption retrieval.\"]\n",
            "-----------------\n",
            "[' Yes', ' The table shows the accuracy results for sentence classification and entailment tasks. The table is divided into two sections, one for the MRPC task and the other for the baseline model (ST-LN). The MRPC task appears to be most beneficial for the ST-LN model, as it has a higher accuracy rate compared to the baseline model.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model on the SNLI dataset is GroundSent, which is an ensemble of two different STb-1024 models with different initializations. GroundSent performs better than the baseline STb-1024 model by a significant margin. The contribution of grounding to the performance of GroundSent is much higher than the baseline STb-1024 model, indicating that grounding plays a crucial role in improving the']\n",
            "-----------------\n",
            "[' Yes', ' The word embeddings learned by the Cap2Img model are compared to the original GloVe embeddings in terms of semantic similarity.']\n",
            "-----------------\n",
            "[' Yes', ' The \"max\" function in the model architecture is used to determine the maximum value of a particular feature or attribute in the input image or caption. This value is then used to make predictions or decisions based on the input.']\n",
            "-----------------\n",
            "[' Yes', ' The most significant impact on the FIT values for the CMS data set when the target rank is 15 is the constraint on the non-negativity of the Higgs field.']\n",
            "-----------------\n",
            "[' Yes', ' Some common medications used to treat Sickle Cell Anemia include hydroxyurea, which helps to reduce the frequency of painful crises, and antibiotics, which can help to prevent infections.']\n",
            "-----------------\n",
            "[' Yes', ' The three constraints imposed by COPA on PARAFAC2 model factors are: 1) temporal phenotyping via EHR data, 2) the use of a specific model, and 3) the targeting of certain factors.']\n",
            "-----------------\n",
            "[' Yes', ' The temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients. The first row is associated with a patient who has sickle cell anemia, while the second row is for a patient with leukemia.']\n",
            "-----------------\n",
            "[' Yes', ' The algorithm that converged faster in both cases of target rank was COPA.']\n",
            "-----------------\n",
            "[' Yes', ' COPA is faster than Helwig.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the largest number of clinical visits per patient is the one with the highest number of patients.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the Silver Snatch and the Gold Snatch is that the Gold Snatch is a more advanced version of the Silver Snatch. The Gold Snatch is a combination of the Silver Snatch and the Fullcycle, which is a more powerful and advanced version of the Silver Snatch.']\n",
            "-----------------\n",
            "[' Yes', ' The projection-based system performs best on the IT domain in terms of full-cycle Smatch score.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the BLEU scores for Moses, Nematus, and Google Translate (GT) on the LDC2015E86 test set. The table is organized in such a way that the first column shows the names of the systems, the second column shows the BLEU scores, and the third column shows the names of the systems.\\n\\nMoses and Nematus are both statistical machine translation systems, while Google Translate (GT) is a rule']\n",
            "-----------------\n",
            "[' Yes', ' The parsing trees for \"I like eating\" and \"I like grapes\" are different because they represent different meanings of the sentence. The first sentence means \"I like the act of eating,\" while the second sentence means \"I like the fruit grapes.\" The parsing trees reflect these different meanings by showing the relationships between the words and phrases in the sentences.']\n",
            "-----------------\n",
            "[' Yes', ' The task-oriented dialog system that performs the best when the percentage of unseen information in the KB is high is BLEU.']\n",
            "-----------------\n",
            "[' Yes', ' The Seq2Seq and Mem2Seq models performed poorly when the percentage of unseen entities in the knowledge base (KB) increased because they were not designed to handle such situations. These models rely on the assumption that the input text is a linear sequence of words, and when the input contains unseen entities, they struggle to generate coherent and meaningful responses. This highlights the importance of developing models that can better handle ambiguous or unseen entities in the input text.']\n",
            "-----------------\n",
            "[' Yes', ' The highest combined score for informativeness and grammatical correctness on the CamRest dataset was achieved by the SMD model.']\n",
            "-----------------\n",
            "[' Yes', \" The BOSSNET with multi-hop encoder performs better on bAbI tasks 3 and 5 compared to the 1-hop encoder because it can process multiple input sentences at once, allowing it to generate more diverse and accurate responses. This is particularly useful for tasks that require understanding and generating natural language, as it can better handle the complexities and nuances of human language. The multi-hop encoder's ability to process multiple sentences simultaneously enables it to\"]\n",
            "-----------------\n",
            "[' Yes', ' The encoder understands the last user utterance by using only the memory cell representations.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the per-response and per-dialog accuracies for different models on the BOSSNET dialog tasks. The table is organized in a way that makes it easy to compare the performance of different models. The table shows that the model that performs best on the T3 and T3-OOV tasks in terms of per-dialog accuracy is the baseline. However, its performance differs between the two test sets.']\n",
            "-----------------\n",
            "[' Yes', ' The authors might claim that although BOSSNET achieves a lower BLEU score than Mem2Seq on the SMD dataset, it still performs better in conveying necessary entity information because BOSSNET is designed to focus on conveying necessary entity information, while Mem2Seq is designed to generate text that is more human-like. This means that BOSSNET may prioritize the correct identification and representation of entities in the text, even if it sacrifices some of the flu']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low is the one with the lowest Entity F1 score.']\n",
            "-----------------\n",
            "[' Yes', ' The original SMD Navigate data is a list of words, while the pre-processed SMD Navigate data is a list of word sequences. The pre-processed data is more suitable for use in a machine learning model, as it is easier to work with and analyze.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs best when the percentage of unseen entities in the response is low is the one with the highest accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the learning rates for different datasets and the hyperparameters used to train BOSSNET on each dataset. The highest learning rate is 0.001, which is used for the CamRest dataset. This is significantly higher than the learning rate used for the other datasets.']\n",
            "-----------------\n",
            "[' Yes', ' The difference between the attention weights in the two-level attention model and the one-level attention model is that the two-level model has two sets of attention weights, one for each level, while the one-level model has only one set of attention weights. This means that the two-level model can better capture the complexity of the input data by considering multiple levels of abstraction, whereas the one-level model may not be able to capture the same level of complexity.']\n",
            "-----------------\n",
            "[' No', ' The key differences between Seq2Sick and existing attack methods on RNN-based models lie in the targeted attack approach and the specific classes or keywords being targeted. While Seq2Sick focuses on targeted attacks to specific classes or keywords, existing attack methods may not have this level of specificity or may not be designed to target specific classes or keywords. Additionally, Seq2Sick may employ different techniques or strategies compared to existing attack methods, which could lead to improved effect']\n",
            "-----------------\n",
            "[' Yes', ' The success rate of the non-overlapping attack is higher when there are fewer words changed in the input sentence. This is because the adversarial inputs are crafted to be very similar to their originals, and the fewer words changed, the more similar the adversarial input will be to the original. This leads to higher success rates in generating a summarization that differs with the original at every position for all three datasets.']\n",
            "-----------------\n",
            "[' Yes', ' The difficulty of performing a successful targeted keywords attack increases as the number of targeted keywords increases.']\n",
            "-----------------\n",
            "[' Yes', ' The non-overlapping method resulted in the highest BLEU score.']\n",
            "-----------------\n",
            "[' Yes', ' Yes']\n",
            "-----------------\n",
            "[' Yes', ' The best performance in terms of distortion for both cover and secret images when embedding 2 bits per channel is achieved by the method that has the lowest SIR value.']\n",
            "-----------------\n",
            "[' Yes', ' GeoCUTS performs comparably to other clusterings for highly active users.']\n",
            "-----------------\n",
            "[' Yes', ' The Hilbert space-filling curve is constructed recursively up to any desired resolution.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the percentage of queries from clusters with a Q-metric of ≥ 25% for different numbers of clusters in France. The table also shows the percentage of queries from clusters with a Q-metric of ≥ 50% for different numbers of clusters in France. The table is comparing the performance of GeoCUTS to the Grid method in identifying highly mobile clusters when the number of clusters is increased from approximately 25 to 50.']\n",
            "-----------------\n",
            "[' Yes', ' The query graph and the interference graph are related in that the interference graph is a folded version of the query graph. The folded graph is created by taking the unnormalized weights of the query graph and normalizing them to 1. This results in a graph where the edge weights correspond to the unnormalized weights, and the resulting graph is the interference graph.']\n",
            "-----------------\n",
            "[' Yes', ' Bay Area']\n",
            "-----------------\n",
            "[' Yes', ' Paris, Bordeaux, and Lyon are correctly identified by the GeoCUTS algorithm in France.']\n",
            "-----------------\n",
            "[' Yes', ' GeoCUTS and Linear Embedding produced the smallest cut size for highly active users in the US dataset. GeoCUTS produced a cut size that was 14.7% smaller than the grid partitioning, while Linear Embedding produced a cut size that was 15.4% smaller than the grid partitioning.']\n",
            "-----------------\n",
            "[' Yes', ' GeoCUTS performs best for highly active users in the US.']\n",
            "-----------------\n",
            "[' Yes', ' The greedy EM-type learning process took 22 iterations to learn the part models for the watch image.']\n",
            "-----------------\n",
            "[' Yes', ' The Compositional Active Basis Model (CABM) is a mathematical model that describes the relationship between random variables. In the image, there are two diagrams showing the dependence structure between random variables in a CABM. The first diagram is a simple binary-tree structured Markov random field, while the second diagram is a graphical model of a generalized multi-layer CABM. The hierarchical dependence structure of the CABM is shown, which includes the number of']\n",
            "-----------------\n",
            "[' Yes', ' The hierarchical part dictionary learned with the bottom-up process is a representation of the parts and their relationships in a hierarchical structure. The holistic object model learned with the top-down process is a representation of the whole object and its relationships in a hierarchical structure. The bottom-up process starts with the parts and builds up the hierarchy, while the top-down process starts with the whole object and breaks it down into parts.']\n",
            "-----------------\n",
            "[' Yes', ' The top-down compositional learning scheme involves composing the learned hierarchical part dictionary into a holistic object model, while the bottom-up process groups basis filters into higher-order parts until no further compositions are found.']\n",
            "-----------------\n",
            "[' Yes', ' The optimal threshold is chosen to be at the right margin of the region around the mode of the histogram because it is the point where the number of observations is maximized. This helps to ensure that the majority of the data is included in the analysis and that the results are more accurate and reliable.']\n",
            "-----------------\n",
            "[' No', ' The table shows the results of different methods for detecting changes in images. The table is divided into several categories, and each category has its own score. The best overall F1 score is achieved by the method labeled \"S-GRADE\". This method consistently performs the best across all individual categories.']\n",
            "-----------------\n",
            "[' Yes', ' The fastest processing time per frame is achieved by the algorithm called \"Algorithm 6: Mean processing time per input frame (in seconds) on the \"baseline/highway\" video-sequence from <http://changedetection.net>. Note, our implementation does not use any parallelisation at the moment. This was done on purpose to run on a machine serving multiple cameras simultaneously.\"']\n",
            "-----------------\n",
            "[' No', ' The residuals prior to thresholding are the differences between the original video frames and the estimated background. These residuals are then used to create a Boolean map, which is a binary representation of the image. The Boolean map is obtained by thresholding the residuals, which means that any residual value above a certain threshold will be represented as a 1, while any residual value below the threshold will be represented as a 0. This Boolean map can then be used for various image processing tasks']\n",
            "-----------------\n",
            "[' Yes', ' The MCMC sampler that appears to have the most consistent performance across the different variables is PE-HMC (N=5).']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the PE-N=5 sampler compares to the HMC sampler in terms of log-predictive density on held-out test data for the prostate cancer dataset. The HMC sampler has a higher log-predictive density, indicating better performance in this specific case.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of ITN-V2 is better than other methods when both DDT and ST transformations are applied to the CIFAR-10 dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing method when trained with only 1% of the MNIST training data is the method that performs the best when trained with the entire dataset. Data augmentation can improve its performance by generating additional training data from the existing dataset.']\n",
            "-----------------\n",
            "[' Yes', \" The ITN framework generates pseudo-negative samples by adding a small amount of random noise to the positive samples during the training process. This helps to improve the discriminator's ability to distinguish between positive and negative samples.\"]\n",
            "-----------------\n",
            "[' No', ' The method that performs best on the CIFAR-10 dataset is the one that has the highest accuracy rate.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the lowest testing error on the miniImageNet dataset is the one that uses the resnet34 network.']\n",
            "-----------------\n",
            "[' Yes', ' AC-GATN']\n",
            "-----------------\n",
            "[' Yes', ' The quality of the generated samples changes as the update threshold increases. The higher the threshold, the fewer the samples will be, and the more likely it is that the samples will be close to the original image.']\n",
            "-----------------\n",
            "[' Yes', \" The relationship between the update threshold (Tu) and the performance of ITN (B-CNN) on the MNIST dataset is that the Tu value affects the number of iterations performed during the training process. The lower the Tu value, the more iterations are performed, which can potentially improve the accuracy of the model. However, if the Tu value is too low, it may lead to overfitting and decrease the model's performance. The optimal Tu value depends on the specific problem\"]\n",
            "-----------------\n",
            "[' Yes', ' ITN-NG']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a comparison between the accuracy of AC-GATN and ITN models on the MNIST dataset. The MNIST dataset is a collection of 70,000 grayscale images of handwritten digits, which is commonly used for evaluating the performance of image recognition models. The image displays two different models, AC-GATN and ITN, with their respective accuracy rates. The accuracy rate of AC-GATN is higher than that of IT']\n",
            "-----------------\n",
            "[' Yes', ' The best performing method on the TMTA task is the B-CNN method. Data augmentation contributes significantly to its performance, as it helps to improve the robustness of the model to various input conditions.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the highest accuracy on the 45-tag Penn WSJ dataset is the one with the highest accuracy, which is 7.']\n",
            "-----------------\n",
            "[' No', \" The factor that contributes the most to the best model's performance compared to the baseline model is the configuration of the model.\"]\n",
            "-----------------\n",
            "[' Yes', ' The table shows the comparison of the reported results with the CRF autoencoders in many-to-one accuracy (M2O) and the V-measure (VM). The table also shows the average V-measure (VM) across all languages for each method. The Baum-Welch method achieved the highest average V-measure (VM) across all languages by 0.08 compared to the CRF autoencoders.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the highest accuracy on the Italian language data set is the one that uses a fixed hyperparameter configuration optimized on the 45-tag Penn WSJ.']\n",
            "-----------------\n",
            "[' Yes', ' BiLSTM is a type of neural network architecture that is used in natural language processing tasks. In the image, the BiLSTM is shown as a part of the architecture, which is used to process the input text and generate the output. The BiLSTM architecture is designed to handle both left-to-right and right-to-left text, making it suitable for tasks such as text classification, translation, and language generation.']\n",
            "-----------------\n",
            "[' Yes', ' The best performance for the MSRP task on the Ordered Sentences dataset was achieved by the model SAE.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing model on the SICK 2014 dataset in terms of average Spearman and Pearson correlation is the SVM model.']\n",
            "-----------------\n",
            "[' Yes', \" The relationship between the $L_2$-norm of a word vector and its frequency is that the $L_2$-norm is a measure of the magnitude of the word vector, and the frequency is a measure of how often the word appears in the data. The $L_2$-norm is calculated by taking the square root of the sum of the squares of the word vector's components, while the frequency is calculated by dividing the number of occurrences of the word by the total\"]\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the shortest average sentence length is the one on the left.']\n",
            "-----------------\n",
            "[' Yes', ' The best performance for the Twitter dataset was achieved by the Sent2Vec model.']\n",
            "-----------------\n",
            "[' Yes', ' The range of values for the context number hyperparameter is 0.01 to 0.99.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of increasing the margin on the AUC and MAP values is that it reduces the sensitivity of the results to the choice of margin.']\n",
            "-----------------\n",
            "[' Yes', \" The model that performs best on the PubMed + UMLS dataset is the one that uses the BERT+PubMed model and is trained on the PubMed dataset. This model outperforms the DPE baseline, achieving an AUC of 0.88 compared to the DPE baseline's AUC of 0.78.\"]\n",
            "-----------------\n",
            "[' Yes', ' The dataset with the most entities is \"Entity\".']\n",
            "-----------------\n",
            "[' Yes', ' The four steps involved in the synonym discovery process using the model are: 1) Query, 2) Candidate Exploration, 3) Candidate Exclusion, and 4) Synonym Discovery.']\n",
            "-----------------\n",
            "[' Yes', ' The Leaky Unit in the SYNONYMNET model is responsible for minimizing the loss calculated using multiple pieces of contexts. It achieves this by bilateral matching with the entities in the model.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the performance of different models on synonym discovery. The best performing model is the one with the largest performance gap between it and the baseline model DPE.']\n",
            "-----------------\n",
            "[' Yes', ' The discrimination in the prediction of the two-phase framework (MSG) is compared to that of DI, both with and without classifier tweaking, when the sample size is 2000.']\n",
            "-----------------\n",
            "[' Yes', ' The shape of the IRLS weight function changes as the shape parameter α increases, with the function becoming more curved and concave.']\n",
            "-----------------\n",
            "[' Yes', ' The \"Mean Reconstruction\" shows the average face of the input image, while the \"Sampled Reconstruction\" shows a random face sampled from the latent space.']\n",
            "-----------------\n",
            "[' Yes', ' The adaptive model with individual adaptation of shape parameter to each coefficient outperforms the fixed model with different values of α.']\n",
            "-----------------\n",
            "[' Yes', ' The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the \"Pedigree\" dataset, and it improved by approximately 15%.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the shape parameter α and the shape of the loss function is that different values of α reproduce existing loss functions. The input image shows a graph with two different graphs, one on the left and one on the right. The left graph represents the general loss function, while the right graph represents its gradient. The shape parameter α is used to control the shape of the loss function, and different values of α reproduce different loss functions.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients is an improvement in depth estimates.']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieved the best performance in terms of average error was the one that used the adaptive shape parameter of the proposed loss function. This method offered a significant improvement of around 20% compared to the reproduced baseline.']\n",
            "-----------------\n",
            "[' Yes', \" The authors chose to use a nonlinearity to curve α before fitting the cubic hermite spline to improve the accuracy of the approximation. By increasing knot density near α = 2 and decreasing knot density when α > 4, the nonlinearity helps to better represent the distribution's log partition function in the transformed space. This approach allows for a more accurate and smooth interpolation of the log(Z (α)) function.\"]\n",
            "-----------------\n",
            "[' Yes', ' The range of values for the shape parameter α is from 0 to 2.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of gFGR increases as the shape parameter α increases.']\n",
            "-----------------\n",
            "[' Yes', ' The image representation that results in the sharpest and highest-quality samples is the one that uses the general distributions (columns) to model the coefficients of three different image representations (rows).']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that shows the greatest sensitivity to the choice of $\\\\power$$ is the one with the highest accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The shape of the negative log-likelihood (NLL) and probability density functions change as the value of α increases.']\n",
            "-----------------\n",
            "[' Yes', ' The results of the baseline and the proposed method are compared in terms of accuracy.']\n",
            "-----------------\n",
            "[' Yes', ' The choice of distribution affects the quality of the reconstructions by altering the statistical properties of the coefficients. Different distributions can lead to different patterns of variation in the coefficients, which in turn can impact the fidelity and detail of the reconstructed images. For example, using a Cauchy distribution can result in a more focused and detailed reconstruction, while using a normal distribution can lead to a more general and less detailed reconstruction. The choice of distribution can also influence the']\n",
            "-----------------\n",
            "[' Yes', ' The network architecture that has the highest accuracy on the CIFAR-10 dataset is the DenseNet.']\n",
            "-----------------\n",
            "[' Yes', ' A deep residual network is a neural network architecture that uses residual connections to improve the training process. It consists of multiple layers, including an input layer, one or more hidden layers, and an output layer. The residual connections allow the network to learn more efficiently by reducing the reliance on backpropagation. In contrast, a network built by stacking inception-like blocks is a neural network architecture that uses multiple inception-like blocks to improve the training process. It consists']\n",
            "-----------------\n",
            "[' Yes', ' The residual block is a building block in ResNets that consists of two residual branches sequentially, while the merge-and-run block assembles the same two residual branches in parallel. The residual block uses an identity mapping, while the merge-and-run block uses a proposed merge-and-run mapping.']\n",
            "-----------------\n",
            "[' Yes', ' The training loss of DMRNet is lower than that of ResNet on the CIFAR-10 dataset with L = 30.']\n",
            "-----------------\n",
            "[' No', ' The network with the shortest average path length when L = 9 is the one on the left.']\n",
            "-----------------\n",
            "[' Yes', ' The model with the lowest Top-1 validation error on ImageNet is DMRNet.']\n",
            "-----------------\n",
            "[' Yes', ' Yes']\n",
            "-----------------\n",
            "[' Yes', ' The classification error of a residual network decreases as the average path length increases.']\n",
            "-----------------\n",
            "[' Yes', ' Convolution is represented in the frequency domain by a product of the impulse response and the input signal.']\n",
            "-----------------\n",
            "[' Yes', ' The number of classes affects the setup and online time for the Softmax by increasing the number of parameters that need to be learned. With more classes, the Softmax will have more parameters to learn, which can increase the time it takes to train the model. Additionally, the setup time may increase due to the need to allocate more memory and resources for the model. However, the online time may decrease as the model will be able to process more data in parallel, leading to faster processing']\n",
            "-----------------\n",
            "[' No', ' The framework with the lowest total communication cost for MNIST is the CNN framework.']\n",
            "-----------------\n",
            "[' Yes', ' The function of the DataPreprocessing function is to process data by performing operations such as adding, subtracting, multiplying, and dividing.']\n",
            "-----------------\n",
            "[' Yes', ' The FC layer is faster when using the Gazelle framework.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the `SubsetGate` function in the MaxPooling function is to select the maximum value from the pool of pixels in a given region.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the activation layer in a convolutional neural network is to introduce non-linearity into the network. This allows the network to learn more complex and diverse patterns in the input data.']\n",
            "-----------------\n",
            "[' Yes', \" The filter in the convolution operation is used to process the image, and it is designed to enhance or modify the image's features.\"]\n",
            "-----------------\n",
            "[' No', ' ReLU has the lowest online time.']\n",
            "-----------------\n",
            "[' Yes', ' An overlapping case is a case that has some overlap with another case, while an error case is a case that has an error in it.']\n",
            "-----------------\n",
            "[' Yes', ' Rest15 has a higher proportion of sentences containing multiple aspects.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of all models is generally lower on Rest15 compared to Rest14 because Rest15 has more aspect categories, which can make it more challenging for the models to accurately predict the image. The increased complexity of the task may lead to lower performance for the models.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the results of the ALSC task in multi-task settings in terms of accuracy (%) and Macro-F1 (%). The best performing model for binary classification is the one with the highest accuracy. However, the table does not provide enough information to determine which model performed best on the Rest15 dataset for 3-way classification.']\n",
            "-----------------\n",
            "[' Yes', ' The two main tasks that the CAN network is designed to perform are aspect-specific sentence representations for ALSC and ACD tasks, and aspect level sentiment prediction and aspect category detection.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the test accuracy of different models on the \"All\" category of Visual7W. The best performing model is MLPA, which achieved an accuracy of 84.4%. This is higher than the human performance of 78.8%. The other models also performed well, with some achieving higher accuracy than humans.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the accuracy percentages for different methods on the VQA-2014val dataset. The methods include MLP-A, MLP-Ia, MLP-Hieco, and human-iQA. The table shows that the MLP-A method performs the best overall, with an accuracy of 82.5%. This is followed by MLP-Ia with an accuracy of 78.5%, and MLP-H']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the test accuracy of different models on qaVG. The models include MLP-A, MLP-Ia, MLP-PQA, and MLP-HiecoAtt. The table also shows the performance of humans on qaVG. The models perform well on qaVG, with MLP-Ia performing the best. However, the performance of the models is not as accurate as humans.']\n",
            "-----------------\n",
            "[' Yes', ' The shortcuts in the Visual7W dataset can be remedied by creating alternative decoys that make it more challenging for machines to make mistakes. This can be achieved by examining either the image or the question alone, and then creating decoys that are highly likely to be the correct answer. The alternative decoys suggested by the procedures are better designed to gauge how well a learning algorithm can understand all information equally well.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that presents the biggest challenge for a model trying to distinguish true triplets from decoys is the one with the highest number of decoys. This is because the model has to be more accurate in identifying the true triplets among the decoys, which can be more difficult than identifying the true triplets in the other datasets. The presence of a higher number of decoys in the dataset increases the complexity of the task, making it more challenging for the model']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the test accuracy percentages for different embeddings on the Visual7W dataset. The best overall performance was achieved by the GLOVE method, with an accuracy of 72.5%. The other methods, including the GLOVE-no-decoys and GLOVE-no-decoys-no-QoU, also performed well, with accuracy percentages ranging from 68.5% to 71.5%. The']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the test accuracy of different models on the COCOQA dataset. The table is divided into two sections, one for identifying irrelevant image-question pairs (IU) and the other for irrelevant question-answer pairs (QU). The table shows that the model that only observes answers performs better than the other models. The table also shows that the model that performs the best on COCOQA dataset when considering the combined accuracy of identifying irrelevant image-question pairs (I']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the test accuracy percentages for different models on the VQA2-2017 validation set. The model that performs best when considering all three sources of information is the one that uses all three sources, while the model that only uses answers has a lower accuracy percentage.']\n",
            "-----------------\n",
            "[' Yes', ' The equation that describes the motion of a mass attached to a spring is x = -kx^2 + kx^4.']\n",
            "-----------------\n",
            "[' Yes', ' The system from Ortiz et al. that achieved the highest BLEU and METEOR scores is the one labeled \"MLBLB\". It has a BLEU score of 7.3 and a METEOR score of 17.2. This system outperforms the CCA inference algorithm in terms of performance.']\n",
            "-----------------\n",
            "[' Yes', ' The difference between the outputs of the machine translation system (SMT) and the CCA inference is that the SMT outputs are based on statistical patterns, while the CCA inference outputs are based on a deep learning model. The CCA inference outputs are generally rated higher by human evaluations than the SMT outputs.']\n",
            "-----------------\n",
            "[' No', ' SMT']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the singular value decomposition step in the CCA algorithm is to reduce the dimensionality of the input data, allowing for more efficient and effective learning.']\n",
            "-----------------\n",
            "[' Yes', ' In CCA inference, the input space is the image on the left, and the output space is the text on the right. The relationship between the input space and the output space is that the closest unit vector from the input space is mapped to a unit vector in the output space, which represents the text on the right. This mapping is done using the cosine of the angle between the two unit vectors, which is denoted by ρ(u(x), v(y)).']\n",
            "-----------------\n",
            "[' No', ' The temperature variable t in the CCA decoding algorithm is used to control the rate of convergence of the algorithm.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between BLEU score and human ranking for CCA and SMT systems is that the BLEU score is a measure of the quality of the machine translation, and the human ranking is a measure of the quality of the translation as perceived by a human evaluator. The image shows a scatter plot of the BLEU scores and human rankings for both CCA and SMT systems, with the BLEU scores on the x-axis and the human rankings on the']\n",
            "-----------------\n",
            "[' Yes', ' Yes']\n",
            "-----------------\n",
            "[' Yes', \" The answering procedure works by first mapping the question into a graph representation using a sequence-to-sequence LSTM based model. Then, the recursive answering procedure follows the graph, searching for a valid assignment in the image. At each step, the handled node is set and objects (extracted using mask R-CNN) are examined according to the node's requirements (utilizing corresponding visual estimators). If succeeded, a new node is set (according to a DFS travers\"]\n",
            "-----------------\n",
            "[' Yes', ' Yes']\n",
            "-----------------\n",
            "[' Yes', ' Training on more diverse data allows better generalization across domains, resulting in improved accuracy of graph representation for VQA.']\n",
            "-----------------\n",
            "[' Yes', ' The estimator that achieves the highest accuracy on the CLEVR validation set is the one that is labeled as \"accurate\".']\n",
            "-----------------\n",
            "[' Yes', ' The method that achieves the highest overall accuracy on the validation set is U-']\n",
            "-----------------\n",
            "[' Yes', ' The ReOrth Layer in the Projection Block of the Grassmann Network architecture is responsible for rearranging the input data to improve the performance of the network.']\n",
            "-----------------\n",
            "[' Yes', ' The most accurate pooling method for the AFEW database is the ProjPooling across or within projections (A-ProjPooling, W-ProjPooling).']\n",
            "-----------------\n",
            "[' Yes', ' The best performing method on the PaSC dataset for the handheld testing scenario (PaSC2) is the G-block method. It outperforms the other methods, including the AFEW and HDM05 datasets.']\n",
            "-----------------\n",
            "[' Yes', ' The SRL-NW network has the lowest failure rate, and its inference time is comparable to other genres within the same network.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows that the genre that shows the largest absolute improvement in F1 score on the failure set after applying GBI for both syntactic parsing and SRL is the \"PT\" genre.']\n",
            "-----------------\n",
            "[' Yes', ' The accuracy of the model improved as the iterations progressed, as the red color in the image indicates errors that were corrected.']\n",
            "-----------------\n",
            "[' Yes', ' The effect of enforcing syntactic constraints on the semantic role labeling output is to improve the accuracy and consistency of the output. By correctly applying the constraints, the method is able to correctly assign the correct semantic roles to the words in the sentence, resulting in a more accurate and consistent output.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the number of shifts and the accuracy of the output is that the more shifts are used, the more accurate the output will be. However, if there are more tokens in the input than the number of shifts, the output will not be accurate.']\n",
            "-----------------\n",
            "[' Yes', ' The inference method that consistently leads to the highest F1 score on the failure set across all three networks (Net3, Net4, and Net5) is the greedy method.']\n",
            "-----------------\n",
            "[' Yes', \" GBI is shown to reduce the disagreement rate compared to A* on the SRL-100 network's failure set.\"]\n",
            "-----------------\n",
            "[' Yes', ' The method that has the fastest convergence and highest converged performance is the one that uses iterative kinetics.']\n",
            "-----------------\n",
            "[' Yes', ' The dataset that would be the most challenging for a model trained on MSRA-B to perform well on would be the one with the lowest contrast. This is because the contrast between the foreground and background is the lowest, making it more difficult for the model to accurately identify and classify the objects in the image. The lower the contrast, the more challenging the dataset is, and thus the model would need to be more advanced and sophisticated to perform well on this dataset']\n",
            "-----------------\n",
            "[' Yes', ' The FLoss variant of the model performs best in terms of Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset.']\n",
            "-----------------\n",
            "[' Yes', ' FLoss achieves a better balance between precision and recall.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed FLoss method performs better than the balanced cross-entropy loss because it uses an F-measure criterion that can automatically adjust data unbalance, whereas the balanced cross-entropy loss relies on assigning loss weights to foreground/background samples, which may not be sufficient to address the unbalancing problem effectively.']\n",
            "-----------------\n",
            "[' Yes', ' Increasing the value of β2 in the model will decrease the precision and increase the recall. This is because a larger β2 allows for more flexibility in adjusting the balance between recall and precision. In a recall-first application, a larger β2 would be used, while in other cases, a lower β2 would be preferred.']\n",
            "-----------------\n",
            "[' Yes', ' FLoss']\n",
            "-----------------\n",
            "[' Yes', ' The Filtering algorithm performs better than MLE with noise, as indicated by the lower error values in the image.']\n",
            "-----------------\n",
            "[' No', ' The method that performs the best when there is a high fraction of corrupted samples is the one that uses the sampling error without noise as a benchmark. This is because the benchmark provides a reference for the performance of the algorithm, and allows for a comparison of the performance of the algorithm (Filtering), empirical mean with noise (MLE), and RANSAC.']\n",
            "-----------------\n",
            "[' Yes', ' The Joint Attention Module in the model is responsible for predicting the common concept independently for each pair (i). This helps in understanding the context of the image and the phrase, and in generating a more accurate representation of the scene.']\n",
            "-----------------\n",
            "[' Yes', ' The quality of the output heatmap changes when the selected concept, predicted concept, and the real entity to be grounded are all aligned.']\n",
            "-----------------\n",
            "[' Yes', ' The Semantic self-supervision model performs better on ReferIt (mask) compared to ReferIt (bbox) because the mask metric is more accurate in detecting the object of interest. In the ReferIt dataset, the mask metric is used to detect the object of interest, while the bbox metric is used to detect the bounding box of the object. The mask metric is more accurate because it does not rely on the bounding box of the object, which can be more difficult to']\n",
            "-----------------\n",
            "[' Yes', \" The proposed method's attention map is generated from the image, while the VGG16 feature map is a pre-trained model's output. The attention map focuses on specific areas of the image, while the feature map is a general representation of the image. The attention map can be more accurate and relevant to the specific task, while the feature map provides a broader understanding of the image.\"]\n",
            "-----------------\n",
            "[' Yes', ' The loss type that performs best when the concept batch size is 5k is the independent and common concept loss.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the model varies with respect to the bounding box area and the similarity of the concept with ImageNet classes. In the image, there are two graphs showing the correlation between the bounding box area and the similarity of the concept with ImageNet classes. The graphs indicate that the performance of the model increases as the bounding box area increases and the similarity of the concept with ImageNet classes increases. This suggests that the model performs better when it is given more information about the object it']\n",
            "-----------------\n",
            "[' No', ' The dataset that would be the easiest for a model to localize phrases in would be the one with the lowest number of phrases per image. This is because the lower the number of phrases, the less complex the dataset is, making it easier for the model to accurately identify and localize phrases.']\n",
            "-----------------\n",
            "[' Yes', ' The image shows a flowchart of the methodology involved in reconstructing implicit warrants for argument reasoning comprehension. The flowchart consists of several steps, including understanding the problem, identifying the relevant information, analyzing the information, and finally, reconstructing the implicit warrant. The image provides a clear visual representation of the process involved in this task.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the number of workers per \"expert\" and Cohen\\'s kappa agreement for stance annotation is that the more workers are assigned to a single expert, the higher the kappa agreement will be. This is because the more workers are assigned to a single expert, the more likely it is that the workers will agree on the annotations, leading to a higher kappa agreement.']\n",
            "-----------------\n",
            "[' Yes', ' The approach that performs best on the development set is the human average approach.']\n",
            "-----------------\n",
            "[' Yes', ' Yes']\n",
            "-----------------\n",
            "[' Yes', \" Step 6, which involved filtering the instances by their 'logic score' and discarding the weakest 30%.\"]\n",
            "-----------------\n",
            "[' Yes', ' The intra-warrant attention mechanism works by using a vector to represent the attention of the model towards a specific part of the input. The attention vector is constructed by taking the dot product of the query vector and the key vector, where the query vector is a linear combination of the input and the key vector. The result is a weighted sum of the input, which is then passed through a non-linear activation function to produce the output. The attention mechanism allows the model to focus on']\n",
            "-----------------\n",
            "[' Yes', ' The policy learning method that achieved the lowest regret in Ex. 2 is the one that uses the letters \"C\" and \"D\" in the policy.']\n",
            "-----------------\n",
            "[' Yes', ' The validation accuracy of the SRU model is lower than that of the cuDNN LSTM and CNN models.']\n",
            "-----------------\n",
            "[' Yes', ' The processing time of SRU is faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.']\n",
            "-----------------\n",
            "[' Yes', ' According to Table 1 and the passage, the SRU model outperforms the LSTM model in terms of both accuracy and training speed on the SQuAD dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The training process handles large vocabulary sizes by using a technique called \"tokenization.\" This involves breaking down the text into smaller units, such as words or phrases, which can be processed more efficiently. The table also shows that the training process uses a \"batch size\" of 512, which means that the training data is divided into smaller chunks of 512 units, allowing for more efficient processing of the large vocabulary.']\n",
            "-----------------\n",
            "[' Yes', ' The variance of the hidden state $h_t$ is higher than the variance of the input $x_t$ in deep layers of the SRU model.']\n",
            "-----------------\n",
            "[' Yes', ' Scaling correction improves the training progress of SRU models, especially for deeper models with many stacked layers.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of SRU with 8 layers compares well to the best reported results on the SUBJ dataset. However, the training time of SRU is longer than the other models in the \"Our setup\" section.']\n",
            "-----------------\n",
            "[' Yes', ' ResNet-56 (RCE) performed better on the MNIST dataset.']\n",
            "-----------------\n",
            "[' Yes', ' The combination of training procedure and thresholding metric that consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types is ResNet-32 with a baseline of 1/0.26 and a defense method of σ2 CE = 1/0.26 and σ2 RCE = 0.1/0.26.']\n",
            "-----------------\n",
            "[' No', ' The most effective attack method to reduce the accuracy of the Resnet-32 model on the MNIST dataset is adversarial examples.']\n",
            "-----------------\n",
            "[' Yes', ' Resnet-32']\n",
            "-----------------\n",
            "[' Yes', ' The accuracy of the model increases as the value of c increases.']\n",
            "-----------------\n",
            "[' No', ' The objective function that resulted in a higher ratio of f2(x∗) > 0 for the MNIST dataset is the one that minimizes distortions of the adversarial examples crafted by C&W-wb. The model used is Resnet-32.']\n",
            "-----------------\n",
            "[' Yes', ' The adversarial example generation method that results in images that are visually more similar to the original images is called C&W-wb with minimal distortions.']\n",
            "-----------------\n",
            "[' Yes', ' The proposed metric of non-ME helps detect adversarial examples by providing a visual representation of the decision boundary of the classifier. The decision boundary is the boundary between the two classes, and the isolines of non-ME are the boundary between the original classes for normal examples and the target classes for adversarial ones. The visualization of the final hidden vectors on CIFAR-10, the Resnet-32 model, and the training procedure of RCE or CE can help identify']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the average time costs (s) on crafting each adversarial example via different attacks. The values are also the average values between MNIST and CIFAR-10. The models is Resnet-32. The most efficient attack method in terms of time taken to craft an adversarial example is the attack method that is 2.5 times faster compared to the slowest method for the same objective function.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the configuration of the model and the impact of removing different components on the F1 score. The table shows that the \"reat\" component has the biggest impact on the F1 score on the SQuAD dataset. Removing this component would result in a significant decrease in the F1 score.']\n",
            "-----------------\n",
            "[' Yes', ' The model that performs the best on the AddOneSent dataset in terms of F1 score is the one with the highest F1 score.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the single R.M-Reader model compares well to the best single models of other approaches on the SQuAD test set.']\n",
            "-----------------\n",
            "[' Yes', ' The two types of attention mechanisms used in the Reinforced Mnemonic Reader architecture are refined Et to attend the query and refined Bt to attend the context.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the fusion modules in the interactive alignment and self-alignment modules is to improve the accuracy and efficiency of the alignment process. By combining different alignment strategies, the fusion modules can adapt to various input sequences and improve the overall performance of the alignment algorithm. This can lead to better results in terms of sequence similarity and quality of the alignment.']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the impact of reattention on different blocks of attention distributions. The rows represent different attention distributions, while the columns represent the different blocks. The table shows the redundancy and deficiency of attention distributions for each block. The results indicate that reattention affects the redundancy and deficiency of attention distributions differently for each block. The table provides a comparison of the impact of reattention on different attention distributions on the SQuAD dev set.']\n",
            "-----------------\n",
            "[' Yes', ' The relationship between the number of Monte Carlo samples (N) and the translation performance of the BR-CSGAN model is that as the value of N increases, the translation performance tends to improve. However, there is a trade-off when choosing the value of N, as increasing N can also increase computational cost and decrease the efficiency of the model. The optimal value of N depends on the specific task and resources available.']\n",
            "-----------------\n",
            "[' Yes', ' The initial accuracy of the discriminator affects the BLEU score by influencing the overall performance of the model. In the image, there are different initial accuracies for the discriminators, which are labeled as \"0.6-acc,\" \"0.8-acc,\" and \"0.4-acc.\" The discriminators with higher initial accuracies are more likely to produce better results, leading to a higher BLEU score. Conversely,']\n",
            "-----------------\n",
            "[' Yes', ' The table shows the BLEU score for both MRT and BR-CSGAN on Chinese-English and English-German translation tasks. MRT has a higher BLEU score than BR-CSGAN in both cases. The likely reason for this difference is that MRT is a more advanced and sophisticated model compared to BR-CSGAN. MRT may have better capabilities in handling complex language structures and translating them accurately, leading to a higher B']\n",
            "-----------------\n",
            "[' Yes', ' The best performance on the Chinese-English translation task is achieved by the Transformer model, which offers a 1.5% improvement compared to the baseline RNNSearch model.']\n",
            "-----------------\n",
            "[' Yes', ' The discriminator (D) in the proposed BR-CSGAN model is trained to differentiate between real sentence pairs translated by a human and the generated sentence pairs by the generator (G). The discriminator receives a conditional reward from the generator, and the final reward is provided by the discriminator and the quality model (Q).']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the model with convolutional self-correction is expected to improve as the number of images in set $\\\\mathcal{F}$ increases. This is because the model can learn from its own mistakes and correct them, leading to better overall performance. On the other hand, the model without self-correction may struggle to improve its performance as it relies solely on the training set for learning.']\n",
            "-----------------\n",
            "[' Yes', ' The best performing method on the PASCAL VOC 2012 test set is the one that uses self-correction. This method is called \"self-correction\" and it outperforms the baseline model without self-correction.']\n",
            "-----------------\n",
            "[' Yes', ' The input to the convolutional self-correction model is a logit from the primary and ancillary models.']\n",
            "-----------------\n",
            "[' Yes', ' The performance of the \"Conv. Self-Correction\" method is better than other methods when using 30% of the training examples as $\\\\F$ and the remaining as $\\\\W$ on the Cityscapes validation set.']\n",
            "-----------------\n",
            "[' Yes', ' The bounding box encoder network influences the segmentation process by introducing a new layer that encodes the box information. This information is then used to create an attention map, which is combined with the feature maps from the encoder at different scales. This combination of information is then passed to the decoder, which helps in improving the accuracy of the segmentation process.']\n",
            "-----------------\n",
            "[' Yes', ' The self-correction module refines the segmentations generated by the ancillary model and the current primary model for the weak set. It helps improve the accuracy of the primary model by using the cross-entropy loss that matches its output to either ground-truth segmentation labels for the fully supervised examples or soft refined labels generated by the self-correction module for the weak set.']\n",
            "-----------------\n",
            "[' Yes', ' The purpose of the ancillary heatmap shown in this paper is to demonstrate the effectiveness of the PASCAL VOC 2012 auxiliary in correcting the labels for missing or oversegmented objects in the images.']\n",
            "-----------------\n",
            "[' Yes', ' The outcome curves for the black and white groups differ in that the black group has a higher utility curve, indicating a higher likelihood of positive outcomes. This suggests that the decision rule thresholds for the black group may be set at a higher level than for the white group.']\n",
            "-----------------\n",
            "[' Yes', ' The selection rate affects the expected outcome and institution utilities for different decision rules by determining the selection rates resulting from various decision rules. The maxima of the utility curves determine the selection rates, and the institution utilities can be plotted as a function of selection rate for one group. This information can be used to make informed decisions and optimize outcomes.']\n",
            "-----------------\n",
            "[' Yes', ' The probability of repaying a debt increases with credit score.']\n",
            "-----------------\n",
            "[' Yes', ' The fairness criteria that results in the highest loan approval rate for the Black group when the loss/profit ratio is -4 is DemParity.']\n",
            "-----------------\n",
            "[' Yes', ' The outcome curve shows that the selection rate and mean change in score are positively related. The higher the selection rate, the higher the mean change in score, and vice versa. This indicates that the selection rate is an important factor in determining the overall outcome of a particular population.']\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the responses folder\n",
        "!zip -r ./responses-llava.zip ./responses\n",
        "\n",
        "#Download the file to local desktop\n",
        "from google.colab import files\n",
        "files.download(\"./responses-llava.zip\")"
      ],
      "metadata": {
        "id": "J7JX8vXtMRaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "500b3adb-41df-4871-9c13-6f8b364c722e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: responses/ (stored 0%)\n",
            "  adding: responses/1704.04539v2_response.json (deflated 66%)\n",
            "  adding: responses/1708.00160v2_response.json (deflated 68%)\n",
            "  adding: responses/1707.01922v5_response.json (deflated 70%)\n",
            "  adding: responses/1906.10843v1_response.json (deflated 66%)\n",
            "  adding: responses/1805.07567v2_response.json (deflated 70%)\n",
            "  adding: responses/1611.04684v1_response.json (deflated 66%)\n",
            "  adding: responses/1705.07164v8_response.json (deflated 66%)\n",
            "  adding: responses/1705.09882v2_response.json (deflated 74%)\n",
            "  adding: responses/1809.03550v3_response.json (deflated 64%)\n",
            "  adding: responses/1812.10735v2_response.json (deflated 69%)\n",
            "  adding: responses/1805.02349v2_response.json (deflated 39%)\n",
            "  adding: responses/1708.03797v1_response.json (deflated 60%)\n",
            "  adding: responses/1710.05654v2_response.json (deflated 72%)\n",
            "  adding: responses/1705.10667v4_response.json (deflated 69%)\n",
            "  adding: responses/1706.00633v4_response.json (deflated 72%)\n",
            "  adding: responses/1704.00774v3_response.json (deflated 45%)\n",
            "  adding: responses/1811.08257v1_response.json (deflated 71%)\n",
            "  adding: responses/1802.07459v2_response.json (deflated 61%)\n",
            "  adding: responses/1804.04786v3_response.json (deflated 67%)\n",
            "  adding: responses/1803.01128v3_response.json (deflated 68%)\n",
            "  adding: responses/1812.00281v3_response.json (deflated 73%)\n",
            "  adding: responses/1703.10730v2_response.json (deflated 70%)\n",
            "  adding: responses/1611.03780v2_response.json (deflated 73%)\n",
            "  adding: responses/1805.08751v2_response.json (deflated 48%)\n",
            "  adding: responses/1704.08615v2_response.json (deflated 71%)\n",
            "  adding: responses/1702.08694v3_response.json (deflated 71%)\n",
            "  adding: responses/1707.00524v2_response.json (deflated 70%)\n",
            "  adding: responses/1708.02153v2_response.json (deflated 62%)\n",
            "  adding: responses/1805.04609v3_response.json (deflated 61%)\n",
            "  adding: responses/1705.09966v2_response.json (deflated 73%)\n",
            "  adding: responses/1805.06447v3_response.json (deflated 74%)\n",
            "  adding: responses/1706.08146v3_response.json (deflated 74%)\n",
            "  adding: responses/1608.02784v2_response.json (deflated 70%)\n",
            "  adding: responses/1803.02750v3_response.json (deflated 73%)\n",
            "  adding: responses/1704.05958v2_response.json (deflated 63%)\n",
            "  adding: responses/1809.01989v2_response.json (deflated 57%)\n",
            "  adding: responses/1705.08016v3_response.json (deflated 69%)\n",
            "  adding: responses/1706.04269v2_response.json (deflated 67%)\n",
            "  adding: responses/1812.06589v2_response.json (deflated 69%)\n",
            "  adding: responses/1708.01425v4_response.json (deflated 67%)\n",
            "  adding: responses/1707.01917v2_response.json (deflated 68%)\n",
            "  adding: responses/1707.08608v3_response.json (deflated 72%)\n",
            "  adding: responses/1708.05239v3_response.json (deflated 59%)\n",
            "  adding: responses/1701.06171v4_response.json (deflated 71%)\n",
            "  adding: responses/1901.00056v2_response.json (deflated 68%)\n",
            "  adding: responses/1611.07718v2_response.json (deflated 73%)\n",
            "  adding: responses/1706.04284v3_response.json (deflated 68%)\n",
            "  adding: responses/1603.00286v5_response.json (deflated 69%)\n",
            "  adding: responses/1710.01507v4_response.json (deflated 42%)\n",
            "  adding: responses/1811.02553v4_response.json (deflated 72%)\n",
            "  adding: responses/1805.04687v2_response.json (deflated 75%)\n",
            "  adding: responses/1804.07707v2_response.json (deflated 65%)\n",
            "  adding: responses/1703.07015v3_response.json (deflated 69%)\n",
            "  adding: responses/1803.03467v4_response.json (deflated 69%)\n",
            "  adding: responses/1804.05938v2_response.json (deflated 59%)\n",
            "  adding: responses/1703.00899v2_response.json (deflated 58%)\n",
            "  adding: responses/1611.02654v2_response.json (deflated 68%)\n",
            "  adding: responses/1702.03584v3_response.json (deflated 65%)\n",
            "  adding: responses/1701.03077v10_response.json (deflated 76%)\n",
            "  adding: responses/1805.00912v4_response.json (deflated 66%)\n",
            "  adding: responses/1606.07384v2_response.json (deflated 59%)\n",
            "  adding: responses/1809.03449v3_response.json (deflated 72%)\n",
            "  adding: responses/1707.06320v2_response.json (deflated 67%)\n",
            "  adding: responses/1804.05995v2_response.json (deflated 67%)\n",
            "  adding: responses/1611.05742v3_response.json (deflated 62%)\n",
            "  adding: responses/1809.03149v2_response.json (deflated 66%)\n",
            "  adding: responses/1802.07351v2_response.json (deflated 72%)\n",
            "  adding: responses/1811.06635v1_response.json (deflated 49%)\n",
            "  adding: responses/1703.02507v3_response.json (deflated 70%)\n",
            "  adding: responses/1803.04572v2_response.json (deflated 68%)\n",
            "  adding: responses/1804.04410v2_response.json (deflated 65%)\n",
            "  adding: responses/1705.02798v6_response.json (deflated 69%)\n",
            "  adding: responses/1710.06177v2_response.json (deflated 68%)\n",
            "  adding: responses/1811.02721v3_response.json (deflated 71%)\n",
            "  adding: responses/1706.00827v2_response.json (deflated 72%)\n",
            "  adding: responses/1809.01246v1_response.json (deflated 71%)\n",
            "  adding: responses/1704.05426v4_response.json (deflated 70%)\n",
            "  adding: responses/1804.07931v2_response.json (deflated 70%)\n",
            "  adding: responses/1804.00863v3_response.json (deflated 72%)\n",
            "  adding: responses/1704.07854v4_response.json (deflated 74%)\n",
            "  adding: responses/1906.06589v3_response.json (deflated 63%)\n",
            "  adding: responses/1709.02418v2_response.json (deflated 47%)\n",
            "  adding: responses/1809.00458v1_response.json (deflated 77%)\n",
            "  adding: responses/1611.04363v2_response.json (deflated 65%)\n",
            "  adding: responses/1603.03833v4_response.json (deflated 68%)\n",
            "  adding: responses/1705.09296v2_response.json (deflated 68%)\n",
            "  adding: responses/1804.07849v4_response.json (deflated 67%)\n",
            "  adding: responses/1703.04887v4_response.json (deflated 67%)\n",
            "  adding: responses/1803.06506v3_response.json (deflated 70%)\n",
            "  adding: responses/1703.00060v2_response.json (deflated 60%)\n",
            "  adding: responses/1704.07121v2_response.json (deflated 73%)\n",
            "  adding: responses/1709.00139v4_response.json (deflated 55%)\n",
            "  adding: responses/1605.07496v3_response.json (deflated 73%)\n",
            "  adding: responses/1705.07384v2_response.json (deflated 45%)\n",
            "  adding: responses/1805.01216v3_response.json (deflated 72%)\n",
            "  adding: responses/1811.10673v1_response.json (deflated 74%)\n",
            "  adding: responses/1812.00108v4_response.json (deflated 64%)\n",
            "  adding: responses/1805.08465v3_response.json (deflated 55%)\n",
            "  adding: responses/1804.05936v2_response.json (deflated 69%)\n",
            "  adding: responses/1811.08481v2_response.json (deflated 70%)\n",
            "  adding: responses/1707.00189v3_response.json (deflated 54%)\n",
            "  adding: responses/1809.00263v5_response.json (deflated 71%)\n",
            "  adding: responses/1804.01429v3_response.json (deflated 71%)\n",
            "  adding: responses/1706.03847v3_response.json (deflated 72%)\n",
            "  adding: responses/1705.02946v3_response.json (deflated 66%)\n",
            "  adding: responses/1803.05776v2_response.json (deflated 34%)\n",
            "  adding: responses/1805.06431v4_response.json (deflated 76%)\n",
            "  adding: responses/1802.07222v1_response.json (deflated 45%)\n",
            "  adding: responses/1709.08294v3_response.json (deflated 71%)\n",
            "  adding: responses/1709.02755v5_response.json (deflated 70%)\n",
            "  adding: responses/1811.09393v4_response.json (deflated 73%)\n",
            "  adding: responses/1803.04383v2_response.json (deflated 71%)\n",
            "  adding: responses/1612.02803v5_response.json (deflated 51%)\n",
            "  adding: responses/1809.02731v3_response.json (deflated 64%)\n",
            "  adding: responses/1708.06832v3_response.json (deflated 66%)\n",
            "  adding: responses/1811.07073v3_response.json (deflated 72%)\n",
            "  adding: responses/1901.00398v2_response.json (deflated 61%)\n",
            "  adding: responses/1809.04276v2_response.json (deflated 67%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d8920891-0146-4731-a17c-9babe6c6e198\", \"responses-llava.zip\", 148869)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1CeYvOoB1jMEhEgJ_W2dm1OThMXptKCU6/view?usp=sharing\n",
        "!tar -xzvf pycocoevalcap_spiqa.tar.gz && rm -rf pycocoevalcap_spiqa.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OJ8XfJ51AFb",
        "outputId": "20f93614-7199-468c-903d-05d624739cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CeYvOoB1jMEhEgJ_W2dm1OThMXptKCU6\n",
            "From (redirected): https://drive.google.com/uc?id=1CeYvOoB1jMEhEgJ_W2dm1OThMXptKCU6&confirm=t&uuid=b7922449-b0ce-4e6a-88a3-9a260728901e\n",
            "To: /content/pycocoevalcap_spiqa.tar.gz\n",
            "100% 44.6M/44.6M [00:00<00:00, 58.6MB/s]\n",
            "pycocoevalcap_spiqa/\n",
            "pycocoevalcap_spiqa/__pycache__/\n",
            "pycocoevalcap_spiqa/__pycache__/__init__.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/__pycache__/eval.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/__pycache__/eval.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/__pycache__/eval.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/bleu/\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu_scorer.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu_scorer.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu_scorer.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/bleu/LICENSE\n",
            "pycocoevalcap_spiqa/bleu/bleu.py\n",
            "pycocoevalcap_spiqa/bleu/bleu_scorer.py\n",
            "pycocoevalcap_spiqa/cider/\n",
            "pycocoevalcap_spiqa/cider/__pycache__/\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider_scorer.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider_scorer.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider_scorer.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/cider/cider.py\n",
            "pycocoevalcap_spiqa/cider/cider_scorer.py\n",
            "pycocoevalcap_spiqa/example/\n",
            "pycocoevalcap_spiqa/example/captions_val2014.json\n",
            "pycocoevalcap_spiqa/example/captions_val2014_fakecap_results.json\n",
            "pycocoevalcap_spiqa/example/coco_eval_example.py\n",
            "pycocoevalcap_spiqa/meteor/\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/meteor.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/meteor.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/meteor.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/meteor/data/\n",
            "pycocoevalcap_spiqa/meteor/data/paraphrase-en.gz\n",
            "pycocoevalcap_spiqa/meteor/meteor-1.5.jar\n",
            "pycocoevalcap_spiqa/meteor/meteor.py\n",
            "pycocoevalcap_spiqa/rouge/\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/rouge.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/rouge.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/rouge.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/rouge/rouge.py\n",
            "pycocoevalcap_spiqa/spice/\n",
            "pycocoevalcap_spiqa/spice/__pycache__/\n",
            "pycocoevalcap_spiqa/spice/__pycache__/__init__.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/__init__.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/get_stanford_models.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/get_stanford_models.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/spice.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/spice.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/__init__.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/spice.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/get_stanford_models.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/spice/stanford-corenlp-full-2015-12-09/\n",
            "pycocoevalcap_spiqa/spice/stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0.jar\n",
            "pycocoevalcap_spiqa/spice/__init__.py\n",
            "pycocoevalcap_spiqa/spice/get_stanford_models.py\n",
            "pycocoevalcap_spiqa/spice/spice-1.0.jar\n",
            "pycocoevalcap_spiqa/spice/spice.py\n",
            "pycocoevalcap_spiqa/tokenizer/\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/ptbtokenizer.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/ptbtokenizer.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/ptbtokenizer.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/tokenizer/ptbtokenizer.py\n",
            "pycocoevalcap_spiqa/tokenizer/stanford-corenlp-3.4.1.jar\n",
            "pycocoevalcap_spiqa/.gitignore\n",
            "pycocoevalcap_spiqa/README.md\n",
            "pycocoevalcap_spiqa/eval.py\n",
            "pycocoevalcap_spiqa/license.txt\n",
            "pycocoevalcap_spiqa/setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python open_models_metrics.py --response_root ./responses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiBPf_A21jf8",
        "outputId": "8ba3996c-4fc9-47d1-c20c-86e330bb8538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 328kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.92MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 8.33MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.10MB/s]\n",
            "2025-02-23 17:30:05.066500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740331805.095253   34593 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740331805.104044   34593 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-23 17:30:05.140368: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 440M/440M [00:01<00:00, 238MB/s]\n",
            "result file saved to ./pycocoeval_pred.json\n",
            "result file saved to ./pycocoeval_gt.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "PTBTokenizer tokenized 18943 tokens at 42565.86 tokens per second.\n",
            "PTBTokenizer tokenized 25449 tokens at 89897.19 tokens per second.\n",
            "setting up scorers...\n",
            "computing Bleu score...\n",
            "{'testlen': 22801, 'reflen': 16605, 'guess': [22801, 22176, 21572, 20973], 'correct': [7915, 4692, 3371, 2465]}\n",
            "ratio: 1.3731406202950092\n",
            "Bleu_1: 0.347\n",
            "Bleu_2: 0.271\n",
            "Bleu_3: 0.226\n",
            "Bleu_4: 0.192\n",
            "computing METEOR score...\n",
            "METEOR: 0.220\n",
            "computing Rouge score...\n",
            "ROUGE_L: 0.343\n",
            "computing CIDEr score...\n",
            "CIDEr: 1.148\n",
            ".......Printing results.......\n",
            "Bleu_1: 0.347\n",
            "Bleu_2: 0.271\n",
            "Bleu_3: 0.226\n",
            "Bleu_4: 0.192\n",
            "METEOR: 0.220\n",
            "ROUGE_L: 0.343\n",
            "CIDEr: 1.148\n",
            "BERTScore F1:  tensor([0.6151])\n",
            "Examples with Failed Parsing: 0\n",
            "all:  666\n",
            "No samples:  41\n"
          ]
        }
      ]
    }
  ]
}