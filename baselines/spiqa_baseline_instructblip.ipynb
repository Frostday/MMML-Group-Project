{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14f9ad86e7d641928b3d2e6aa425eed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1836b42d3dda428f9db5cf0d12f51d82",
              "IPY_MODEL_2b975e87d76d402ab904b07a9d6097b9",
              "IPY_MODEL_76f68af3e8d348deb72eb30a0a29ea6c"
            ],
            "layout": "IPY_MODEL_a022f9a904a943aab0bc2e82a7cc8546"
          }
        },
        "1836b42d3dda428f9db5cf0d12f51d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6783c1fee2c482fbc31feacae5549d4",
            "placeholder": "​",
            "style": "IPY_MODEL_2e58ce50d4ea4247909e496fe2673fc7",
            "value": "test-A%2FSPIQA_testA.json: 100%"
          }
        },
        "2b975e87d76d402ab904b07a9d6097b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f966f18e0a543b2b914a72525870d7d",
            "max": 777983,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d160df27a4144a0ba875054ac47f496a",
            "value": 777983
          }
        },
        "76f68af3e8d348deb72eb30a0a29ea6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a58773a00d64638a65d9c0ef11bbba2",
            "placeholder": "​",
            "style": "IPY_MODEL_69daa8f432c345d78c35ba86005c9b0c",
            "value": " 778k/778k [00:00&lt;00:00, 3.96MB/s]"
          }
        },
        "a022f9a904a943aab0bc2e82a7cc8546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6783c1fee2c482fbc31feacae5549d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e58ce50d4ea4247909e496fe2673fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f966f18e0a543b2b914a72525870d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d160df27a4144a0ba875054ac47f496a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a58773a00d64638a65d9c0ef11bbba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69daa8f432c345d78c35ba86005c9b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9cc73066b6c48abb67e6e4a7975b754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87b3400ba5394dc4a044181a4bd93886",
              "IPY_MODEL_6b2ba70ae2e24ec88f5addfb33af100e",
              "IPY_MODEL_e81d09a6c0bc42e2ad2fea36c134a9fd"
            ],
            "layout": "IPY_MODEL_29997d57dd564f21be44977bca4141ce"
          }
        },
        "87b3400ba5394dc4a044181a4bd93886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb1977b88adf41538a1038ca0d98e6a2",
            "placeholder": "​",
            "style": "IPY_MODEL_565699dfa0a84767a3d273fca77e2dc7",
            "value": "SPIQA_testA_Images_224px.zip: 100%"
          }
        },
        "6b2ba70ae2e24ec88f5addfb33af100e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_933edc5f60c74bc89ad5b4485804abbe",
            "max": 47111250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23f0699a04ec49eda96977a471693c4f",
            "value": 47111250
          }
        },
        "e81d09a6c0bc42e2ad2fea36c134a9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ca5a5ef7d6f4ea786b0245a95f129fa",
            "placeholder": "​",
            "style": "IPY_MODEL_4be2b35c09974a2293765a2f37807e69",
            "value": " 47.1M/47.1M [00:01&lt;00:00, 40.6MB/s]"
          }
        },
        "29997d57dd564f21be44977bca4141ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1977b88adf41538a1038ca0d98e6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "565699dfa0a84767a3d273fca77e2dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "933edc5f60c74bc89ad5b4485804abbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f0699a04ec49eda96977a471693c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ca5a5ef7d6f4ea786b0245a95f129fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be2b35c09974a2293765a2f37807e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcEkSjUj1ZPE",
        "outputId": "2c8bb0eb-f123-4ce7-88ce-a765a20e4a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "QzjM3_wX9QSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394fed5e-8447-4328-d02e-275abc18803b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.48.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert-score\n",
            "Successfully installed bert-score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_zoZuQVoruF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "14f9ad86e7d641928b3d2e6aa425eed8",
            "1836b42d3dda428f9db5cf0d12f51d82",
            "2b975e87d76d402ab904b07a9d6097b9",
            "76f68af3e8d348deb72eb30a0a29ea6c",
            "a022f9a904a943aab0bc2e82a7cc8546",
            "c6783c1fee2c482fbc31feacae5549d4",
            "2e58ce50d4ea4247909e496fe2673fc7",
            "3f966f18e0a543b2b914a72525870d7d",
            "d160df27a4144a0ba875054ac47f496a",
            "9a58773a00d64638a65d9c0ef11bbba2",
            "69daa8f432c345d78c35ba86005c9b0c",
            "a9cc73066b6c48abb67e6e4a7975b754",
            "87b3400ba5394dc4a044181a4bd93886",
            "6b2ba70ae2e24ec88f5addfb33af100e",
            "e81d09a6c0bc42e2ad2fea36c134a9fd",
            "29997d57dd564f21be44977bca4141ce",
            "cb1977b88adf41538a1038ca0d98e6a2",
            "565699dfa0a84767a3d273fca77e2dc7",
            "933edc5f60c74bc89ad5b4485804abbe",
            "23f0699a04ec49eda96977a471693c4f",
            "9ca5a5ef7d6f4ea786b0245a95f129fa",
            "4be2b35c09974a2293765a2f37807e69"
          ]
        },
        "outputId": "cf96d3a1-56e5-4188-fc49-36c2b98e5063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-A%2FSPIQA_testA.json:   0%|          | 0.00/778k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14f9ad86e7d641928b3d2e6aa425eed8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SPIQA_testA_Images_224px.zip:   0%|          | 0.00/47.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9cc73066b6c48abb67e6e4a7975b754"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'test-A/SPIQA_testA_Images_224px.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(repo_id=\"google/spiqa\", filename=\"test-A/SPIQA_testA.json\", repo_type=\"dataset\", local_dir='.') ### Mention the local directory path\n",
        "hf_hub_download(repo_id=\"google/spiqa\", filename=\"test-A/SPIQA_testA_Images_224px.zip\", repo_type=\"dataset\", local_dir='.') ### Mention the local directory path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the tar file below and put it in test-A directory\n",
        "!unzip -q ./test-A/SPIQA_testA_Images_224px.zip"
      ],
      "metadata": {
        "id": "WL1KiEbVwzSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "testA_metadata = json.load(open('test-A/SPIQA_testA.json', 'r'))\n",
        "paper_id = '1702.03584v3'\n",
        "print(testA_metadata[paper_id]['qa'])"
      ],
      "metadata": {
        "id": "EcXPPzz_rQ1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adaa73b1-52a9-4e0a-aa34-f36d6e260ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'question': 'How does the observed error compare to the underlying true error as CPU time increases?', 'answer': 'The observed error is initially higher than the underlying true error, but it quickly decreases and converges to the true error as CPU time increases.', 'explanation': 'The figure shows two lines, one representing the observed error and the other representing the underlying true error. Both lines decrease as CPU time increases, but the observed error line decreases more quickly and eventually converges to the true error line.', 'reference': '1702.03584v3-Figure1-1.png'}, {'question': 'How does the performance of SPIRAL-DTW-kMeans compare to k-Shape and CLDS?', 'answer': 'SPIRAL-DTW-kMeans performs better than k-Shape and CLDS on most datasets.', 'explanation': 'In Figure (a) and (b), the circles below the diagonal line indicate datasets where SPIRAL-DTW-kMeans yields better clustering performance in terms of NMI than k-Shape and CLDS, respectively.', 'reference': '1702.03584v3-Figure2-1.png'}, {'question': 'Which method performs the best in terms of NMI and what percentage of datasets does it outperform the other methods on?', 'answer': 'The SPIRAL-MSM-kMeans method performs the best in terms of NMI with a score of 0.365. It outperforms the other methods on 89.4% of the datasets.', 'explanation': 'The table shows the NMI scores and the percentage of datasets on which each method outperforms the other methods. The SPIRAL-MSM-kMeans method has the highest NMI score and the highest percentage of datasets on which it outperforms the other methods.', 'reference': '1702.03584v3-Table1-1.png'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the provided python file\n",
        "!python instructblip_qa_test-a_evaluation_image+caption.py --response_root ./responses --image_resolution 224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJraJe7yXhP",
        "outputId": "01ec9679-4627-4ae2-e2a7-96f5c7add14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Caption: Encoding module f . The residual connection denotes the output of a layer is added to the output of another layer.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the purpose of the residual connection in the encoding module?. 1024x768\\nYes, it's used for adding an additional value from each previous convolutional filter stage as well as providing more information about what has been learned by that specific model so far during training process?\", 'Caption: Encoding module f . The residual connection denotes the output of a layer is added to the output of another layer.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the purpose of the residual connection in the encoding module?. To add the output of a layer to the output of another layer']\n",
            "-----------------\n",
            "['Caption: Deformable Volume Network (Devon) with three stages. I denotes the first image, J denotes the second image, f denotes the encoding module (§4.1), Rt denotes the relation module (§4.2), gt denotes the decoding module (§4.3) and Ft denotes the estimated optical flow for stage t.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the role of the relation module (Rt) in the Deformable Volume Network (Devon)?. 0\\nYes', 'Caption: Deformable Volume Network (Devon) with three stages. I denotes the first image, J denotes the second image, f denotes the encoding module (§4.1), Rt denotes the relation module (§4.2), gt denotes the decoding module (§4.3) and Ft denotes the estimated optical flow for stage t.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the role of the relation module (Rt) in the Deformable Volume Network (Devon)?. 1']\n",
            "-----------------\n",
            "[\"Caption: Table 1. Hyperparameters of deformable cost volumes in Devon.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Explain the rationale behind using five deformable cost volumes with different hyperparameter settings in Devon's relation module.. 2015-03-16_deformation\\\\_costvolumes.png The reason for this is that it allows us to compare how well each volume performs on a given task, such as detecting objects within an environment by comparing\", \"Caption: Table 1. Hyperparameters of deformable cost volumes in Devon.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Explain the rationale behind using five deformable cost volumes with different hyperparameter settings in Devon's relation module.. The rationale behind using five deformable cost volumes with different hyperparameter settings in Devon's relation module is to evaluate the effect of hyperparameters on the performance of the model\"]\n",
            "-----------------\n",
            "['Caption: Table 4. Results on KITTI. (ft) denotes the fine-tuning. EPE denotes end-point error. Fl-all denotes the ratio of pixels where the flow estimate is incorrect by both ≥ 3 pixels and ≥ 5%.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Based on the table, which model achieved the best performance on the KITTI 2015 test set in terms of F1-all score, and how does its performance compare to Devon (ft) on the same dataset?. 9867a', 'Caption: Table 4. Results on KITTI. (ft) denotes the fine-tuning. EPE denotes end-point error. Fl-all denotes the ratio of pixels where the flow estimate is incorrect by both ≥ 3 pixels and ≥ 5%.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Based on the table, which model achieved the best performance on the KITTI 2015 test set in terms of F1-all score, and how does its performance compare to Devon (ft) on the same dataset?. Pix4Learn']\n",
            "-----------------\n",
            "['Caption: Table 2. Results on Sintel (end-point error). (ft) denotes the finetuning.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Based on the table, which method performs best on the Sintel \"Final\" test set, and how does its performance compare to Devon (ft) on the same set? . 105679348', 'Caption: Table 2. Results on Sintel (end-point error). (ft) denotes the finetuning.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Based on the table, which method performs best on the Sintel \"Final\" test set, and how does its performance compare to Devon (ft) on the same set? .']\n",
            "-----------------\n",
            "[\"Caption: FlyingChairs (validation set). Green arrows indicate the small object that moves fast.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which of the three methods, LiteFlowNet, PWC-Net, or Devon, most accurately predicts the motion of the small object in the scene?. 1024x768 resolution images with a green arrow pointing at it are used for validation purposes only during training time when using these models on this dataset called flying chains where you have two objects moving independently but they're connected by\", 'Caption: FlyingChairs (validation set). Green arrows indicate the small object that moves fast.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which of the three methods, LiteFlowNet, PWC-Net, or Devon, most accurately predicts the motion of the small object in the scene?. LiteFlowNet']\n",
            "-----------------\n",
            "['Caption: Top 5 section recommendations for the Wikipedia article lausanne, according to various methods.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What are the top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method?. 100% certainty no', 'Caption: Top 5 section recommendations for the Wikipedia article lausanne, according to various methods.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What are the top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method?. Swiss Federal Institute of Technology Lausanne University of Lausanne International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee World Anti-Doping Agency International Olympic Committee']\n",
            "-----------------\n",
            "[\"Caption: Precision and recall as a function of the number of recommended sections k , for all methods we evaluate.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the trend in precision and recall as the number of recommended sections k increases?. 10% increase each time\\nNo, it's not possible with this information provided by you at present date/time (27-Feb)\", 'Caption: Precision and recall as a function of the number of recommended sections k , for all methods we evaluate.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the trend in precision and recall as the number of recommended sections k increases?. Increase']\n",
            "-----------------\n",
            "['Caption: Percentage of categories (y-axis) that can generate at least x recommendations using the section-count–based. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the percentage of categories that can generate at least 10 recommendations using the section-count-based method?. 95%', 'Caption: Percentage of categories (y-axis) that can generate at least x recommendations using the section-count–based. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the percentage of categories that can generate at least 10 recommendations using the section-count-based method?. 10']\n",
            "-----------------\n",
            "['Caption: Table 3: Test accuracy (%) on Visual7W.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model performed best on the \"All\" category of Visual7W, and how did its performance compare to human performance?. 1024x568qgao-molpqa all 90%', 'Caption: Table 3: Test accuracy (%) on Visual7W.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model performed best on the \"All\" category of Visual7W, and how did its performance compare to human performance?. Method 1']\n",
            "-----------------\n",
            "['Caption: Table 7: Accuracy (%) on VQA−-2014val, which contains 76,034 triplets.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which method performs best overall on VQA-2014val, and how does its performance compare to human performance on the same dataset?. 58% accuracy for MLPAA with a difference of only +/-9%.', 'Caption: Table 7: Accuracy (%) on VQA−-2014val, which contains 76,034 triplets.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which method performs best overall on VQA-2014val, and how does its performance compare to human performance on the same dataset?. MPEG-4 QUANTIZATION ACCURACY PERFORMANCE COMPARISON ON VQA-2014VAL']\n",
            "-----------------\n",
            "['Caption: Table 5: Test accuracy (%) on qaVG.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model performs the best on qaVG when considering both image understanding (IU) and question understanding (QU) individually, and how does its performance compare to humans?. 1024x768 MLPA with a threshold of +3 for IU vs QU?', 'Caption: Table 5: Test accuracy (%) on qaVG.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model performs the best on qaVG when considering both image understanding (IU) and question understanding (QU) individually, and how does its performance compare to humans?. Method 1']\n",
            "-----------------\n",
            "['Caption: Figure 1: An illustration of how the shortcuts in the Visual7W dataset (Zhu et al., 2016) should be remedied. In the original dataset, the correct answer “A train” is easily selected by a machine as it is far often used as the correct answer than the other decoy (negative) answers. (The numbers in the brackets are probability scores computed using eq. (2)). Our two procedures — QoU and IoU (cf. Sect. 4) — create alternative decoys such that both the correct answer and the decoys are highly likely by examining either the image or the question alone. In these cases, machines make mistakes unless they consider all information together. Thus, the alternative decoys suggested our procedures are better designed to gauge how well a learning algorithm can understand all information equally well.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How can the shortcuts in the Visual7W dataset be remedied?. \\u200b', 'Caption: Figure 1: An illustration of how the shortcuts in the Visual7W dataset (Zhu et al., 2016) should be remedied. In the original dataset, the correct answer “A train” is easily selected by a machine as it is far often used as the correct answer than the other decoy (negative) answers. (The numbers in the brackets are probability scores computed using eq. (2)). Our two procedures — QoU and IoU (cf. Sect. 4) — create alternative decoys such that both the correct answer and the decoys are highly likely by examining either the image or the question alone. In these cases, machines make mistakes unless they consider all information together. Thus, the alternative decoys suggested our procedures are better designed to gauge how well a learning algorithm can understand all information equally well.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How can the shortcuts in the Visual7W dataset be remedied?.']\n",
            "-----------------\n",
            "['Caption: Table 2: Summary of Visual QA datasets.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys, and why?. 100% yes', 'Caption: Table 2: Summary of Visual QA datasets.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys, and why?. VQA 2.0']\n",
            "-----------------\n",
            "['Caption: Table 11: Test accuracy (%) on Visual7W, comparing different embeddings for questions and answers. The results are reported for the IoU +QoU-decoys.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which embedding method performed the best overall across all model architectures on the Visual7W dataset with IoU + QoU decoys? Was there a significant difference in performance compared to the other methods?. 20869534\\nNo | Yes/Yes', 'Caption: Table 11: Test accuracy (%) on Visual7W, comparing different embeddings for questions and answers. The results are reported for the IoU +QoU-decoys.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which embedding method performed the best overall across all model architectures on the Visual7W dataset with IoU + QoU decoys? Was there a significant difference in performance compared to the other methods?. Yes']\n",
            "-----------------\n",
            "['Caption: Table 8: Test accuracy (%) on COCOQA.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model performs the best on COCOQA dataset when considering the combined accuracy of identifying irrelevant image-question pairs (IU) and irrelevant question-answer pairs (QU)? How does this compare to the performance of the model that only observes answers?. 10% better than a baseline with no information about questions at all?', 'Caption: Table 8: Test accuracy (%) on COCOQA.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model performs the best on COCOQA dataset when considering the combined accuracy of identifying irrelevant image-question pairs (IU) and irrelevant question-answer pairs (QU)? How does this compare to the performance of the model that only observes answers?.']\n",
            "-----------------\n",
            "['Caption: Table 9: Test accuracy (%) on VQA2-2017val.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: On the VQAv2-2017 validation set, which model performs best when considering all three sources of information (images, questions, and answers) and how does its performance compare to the model that only uses answers?. 36854 random forest qaqc', 'Caption: Table 9: Test accuracy (%) on VQA2-2017val.. Please provide a brief answer to the following question after looking into the input image and caption. Question: On the VQAv2-2017 validation set, which model performs best when considering all three sources of information (images, questions, and answers) and how does its performance compare to the model that only uses answers?.']\n",
            "-----------------\n",
            "['Caption: Basic statistics of the three datasets.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset has the most 4-hop triples?. 102369758\\nNo', 'Caption: Basic statistics of the three datasets.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset has the most 4-hop triples?. Book Crossing News']\n",
            "-----------------\n",
            "['Caption: The average number of k-hop neighbors that two items share in the KG w.r.t. whether they have common raters in (a) MovieLens-1M, (b) Book-Crossing, and (c) BingNews datasets. (d) The ratio of the two average numberswith different hops.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the number of common k-hop neighbors change as the hop distance increases for items with and without common raters?. 0% no rating required\\nYes', 'Caption: The average number of k-hop neighbors that two items share in the KG w.r.t. whether they have common raters in (a) MovieLens-1M, (b) Book-Crossing, and (c) BingNews datasets. (d) The ratio of the two average numberswith different hops.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the number of common k-hop neighbors change as the hop distance increases for items with and without common raters?. 0.25']\n",
            "-----------------\n",
            "['Caption: The results of AUC and Accuracy in CTR prediction.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model performs the best in terms of AUC on the MovieLens-1M dataset?. 052697843 Model', 'Caption: The results of AUC and Accuracy in CTR prediction.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model performs the best in terms of AUC on the MovieLens-1M dataset?. Book Crossing News']\n",
            "-----------------\n",
            "[\"Caption: Figure 2: The overall framework of theRippleNet. It takes one user and one item as input, and outputs the predicted probability that the user will click the item. The KGs in the upper part illustrate the corresponding ripple sets activated by the user’s click history.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the role of the ripple sets in the RippleNet framework?. 1980's music videos?\", 'Caption: Figure 2: The overall framework of theRippleNet. It takes one user and one item as input, and outputs the predicted probability that the user will click the item. The KGs in the upper part illustrate the corresponding ripple sets activated by the user’s click history.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the role of the ripple sets in the RippleNet framework?. Predicting user behavior']\n",
            "-----------------\n",
            "['Caption: Illustration of ripple sets of \"Forrest Gump\" in KG ofmovies. The concentric circles denotes the ripple setswith different hops. The fading blue indicates decreasing relatedness between the center and surrounding entities.Note that the ripple sets of different hops are not necessarily disjoint in practice.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the relationship between the movies \"Forrest Gump\" and \"Cast Away\"?. 1984-2056', 'Caption: Illustration of ripple sets of \"Forrest Gump\" in KG ofmovies. The concentric circles denotes the ripple setswith different hops. The fading blue indicates decreasing relatedness between the center and surrounding entities.Note that the ripple sets of different hops are not necessarily disjoint in practice.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the relationship between the movies \"Forrest Gump\" and \"Cast Away\"?. They were both directed by the same person, Robert Zemeckis.']\n",
            "-----------------\n",
            "['Caption: The results of AUC w.r.t. different sizes of a user’s ripple set.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset has the highest AUC for all ripple set sizes?. 1024x768', 'Caption: The results of AUC w.r.t. different sizes of a user’s ripple set.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset has the highest AUC for all ripple set sizes?. 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000']\n",
            "-----------------\n",
            "[\"Caption: Parameter sensitivity of RippleNet.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the dimension of embedding affect the AUC of RippleNet on MovieLens-1M?. 0.58692743 is a good number for this problem, but it's not as high at higher dimensions (e g., d = w) where we see an average decrease by about half from around r=w\", 'Caption: Parameter sensitivity of RippleNet.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the dimension of embedding affect the AUC of RippleNet on MovieLens-1M?.']\n",
            "-----------------\n",
            "['Caption: Figure 3: A histogram of residuals. The histogram was truncated from the original 3·255 residuals to allow for some clarity of presentation. In green, there is the middle of the least-width interval representing half of the mass. In yellow, there are the end-points of the interval. In red, the “optimal” threshold we use.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Why is the optimal threshold chosen to be at the right margin of the region around the mode of the histogram?. \\u200bYes!', 'Caption: Figure 3: A histogram of residuals. The histogram was truncated from the original 3·255 residuals to allow for some clarity of presentation. In green, there is the middle of the least-width interval representing half of the mass. In yellow, there are the end-points of the interval. In red, the “optimal” threshold we use.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Why is the optimal threshold chosen to be at the right margin of the region around the mode of the histogram?. The optimal threshold is chosen to be at the right margin of the region around the mode of the histogram.']\n",
            "-----------------\n",
            "['Caption: Table 5: Further results on http://changedetection.net.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which method achieves the best overall F1 score across all categories? Is this method consistently the best across all individual categories?. 2016-03-17_results table 4', 'Caption: Table 5: Further results on http://changedetection.net.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which method achieves the best overall F1 score across all categories? Is this method consistently the best across all individual categories?. Yes']\n",
            "-----------------\n",
            "['Caption: Table 6: Mean processing time per input frame (in seconds) on the “baseline/highway” video-sequence from http://changedetection. net. Note, our implementation does not use any parallelisation at the moment. This was done on purpose to run on a machine serving multiple cameras simultaneously.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which algorithm achieves the fastest processing time per frame and how much faster is it compared to the slowest algorithm listed?. 10%\\nYes', 'Caption: Table 6: Mean processing time per input frame (in seconds) on the “baseline/highway” video-sequence from http://changedetection. net. Note, our implementation does not use any parallelisation at the moment. This was done on purpose to run on a machine serving multiple cameras simultaneously.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which algorithm achieves the fastest processing time per frame and how much faster is it compared to the slowest algorithm listed?. The algorithm that achieves the fastest processing time per frame is the LBG algorithm, with a mean processing time per frame of 0.005 seconds.']\n",
            "-----------------\n",
            "['Caption: One snapshot from the video baseline/highway (from the top left, clock-wise): one frame of the original video, our estimate of the background, our residuals prior to thresholding, the ground truth, an exponential smoothing of all frames prior to the current one with smoothing factor of 1/35, and finally, our Boolean map obtained by thresholding residuals.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the relationship between the residuals prior to thresholding and the Boolean map?. 0', 'Caption: One snapshot from the video baseline/highway (from the top left, clock-wise): one frame of the original video, our estimate of the background, our residuals prior to thresholding, the ground truth, an exponential smoothing of all frames prior to the current one with smoothing factor of 1/35, and finally, our Boolean map obtained by thresholding residuals.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the relationship between the residuals prior to thresholding and the Boolean map?. The relationship between the residuals prior to thresholding and the Boolean map is that the residuals are used to obtain the Boolean map.']\n",
            "-----------------\n",
            "['Caption: Figure 1: A telescoping architecture employed in Bing’s retrieval system. Documents are scanned using a pre-defined match plan. Matched documents are passed through additional rank-and-prune stages.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How are documents ranked and pruned in the telescoping architecture?. 2016/03 / 57894321', 'Caption: Figure 1: A telescoping architecture employed in Bing’s retrieval system. Documents are scanned using a pre-defined match plan. Matched documents are passed through additional rank-and-prune stages.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How are documents ranked and pruned in the telescoping architecture?. Rank-and-prune']\n",
            "-----------------\n",
            "['Caption: Figure 2: The reduction in index blocks accessed from the learned policy for CAT2 queries on the weighted set. We intentionally leave out the actual page access numbers on the y-axis because of confidentiality. The queries on the x-axis are sorted by page access independently for each treatment.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the RL policy compare to the baseline in terms of index blocks accessed?. 10% less than average, but still within a reasonable range compared with other policies that have been used before (either through random search over all possible values).', 'Caption: Figure 2: The reduction in index blocks accessed from the learned policy for CAT2 queries on the weighted set. We intentionally leave out the actual page access numbers on the y-axis because of confidentiality. The queries on the x-axis are sorted by page access independently for each treatment.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the RL policy compare to the baseline in terms of index blocks accessed?. Reduced']\n",
            "-----------------\n",
            "['Caption: Table 1: Changes in NCG and the index blocks accessed u from our learned policy relative to production baselines. In both categories, we observe significant reduction in index blocks accessed, although at the cost of some loss in relevance in case of CAT1. All the differences in NCG and u are statistically significant (p < 0.01). Coverage of CAT2 queries in the unweighted set is too low to report numbers.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the performance of the learned policy compare to the production baseline for CAT2 queries in terms of relevance and efficiency?.  Not Applicable/No | Source The authors do not provide a comparison between their results on this topic with those obtained by other researchers using different methods; they only present statistics about changes made during training time as well as before and after it was', 'Caption: Table 1: Changes in NCG and the index blocks accessed u from our learned policy relative to production baselines. In both categories, we observe significant reduction in index blocks accessed, although at the cost of some loss in relevance in case of CAT1. All the differences in NCG and u are statistically significant (p < 0.01). Coverage of CAT2 queries in the unweighted set is too low to report numbers.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the performance of the learned policy compare to the production baseline for CAT2 queries in terms of relevance and efficiency?. Significant reduction in index blocks accessed, although at the cost of some loss in relevance in case of CAT1.']\n",
            "-----------------\n",
            "['Caption: (a) Memory consumption and (b) time cost vs. sequence length on synthetic data; (c) memory load (x-axis), inference time on dev set (y-axis) and test accuracy on the SNLI dataset.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model has the lowest memory consumption and time cost on synthetic data?. 1024 bits per pixel, 3 channels RGB images with a resolution of approximately $65\\\\times$89 pixels? no', 'Caption: (a) Memory consumption and (b) time cost vs. sequence length on synthetic data; (c) memory load (x-axis), inference time on dev set (y-axis) and test accuracy on the SNLI dataset.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model has the lowest memory consumption and time cost on synthetic data?. (c) memory load (x-axis), inference time on dev set (y-axis) and test accuracy on the SNLI dataset.']\n",
            "-----------------\n",
            "['Caption: Table 1: Experimental results for different methods with comparative parameter number on SNLI. |θ|: the number of parameters (excluding word embedding part); Time/Epoch: averaged training time per epoch with batch size 128; Inf. Time: averaged dev inference time with batch size 128; Memory: memory load on synthetic data of sequence length 64 and batch size 64 with back-propagation considered; Train Acc. and Test Acc.: the accuracies on training/test sets. All state-of-the-art methods in leaderboard are listed in Table 1&2 up to Sep. 2018.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model has the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model?. 3579', 'Caption: Table 1: Experimental results for different methods with comparative parameter number on SNLI. |θ|: the number of parameters (excluding word embedding part); Time/Epoch: averaged training time per epoch with batch size 128; Inf. Time: averaged dev inference time with batch size 128; Memory: memory load on synthetic data of sequence length 64 and batch size 64 with back-propagation considered; Train Acc. and Test Acc.: the accuracies on training/test sets. All state-of-the-art methods in leaderboard are listed in Table 1&2 up to Sep. 2018.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model has the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model?. The MTSA model has the highest test accuracy on the SNLI dataset, and its training time per epoch is significantly lower than the MTSA model.']\n",
            "-----------------\n",
            "['Caption: Experimental results on sentence-encoding based SNLI and MultiNLI benchmark tasks. “Transfer” denotes pretrained language model on large corpus for transfer learning, which detailed by Radford et al. (2018). References: a(Nie and Bansal, 2017), b(Chen et al., 2018), c(Talman et al., 2018).. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model performed best on the SNLI test set?. 3a', 'Caption: Experimental results on sentence-encoding based SNLI and MultiNLI benchmark tasks. “Transfer” denotes pretrained language model on large corpus for transfer learning, which detailed by Radford et al. (2018). References: a(Nie and Bansal, 2017), b(Chen et al., 2018), c(Talman et al., 2018).. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model performed best on the SNLI test set?. Model A']\n",
            "-----------------\n",
            "['Caption: Table 4: Experimental Results of SRL for single models on CoNLL-05 with gold predicates. ∗Multi-head baseline is equivalent to the model in Tan et al. (2017). For fair comparisons, first, we use the hyper-parameters provided by Tan et al. (2017) instead of tuning them; second, all listed models are independent of external linguistics information, e.g., PoS, dependency parsing.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Based on the table, how does MTSA compare to the Bi-LSTM and Multi-CNN baselines in terms of performance and training time?. 36% better than both bi Lstm Baselined Model and multi Cnn Baselincd Model\\nYes', 'Caption: Table 4: Experimental Results of SRL for single models on CoNLL-05 with gold predicates. ∗Multi-head baseline is equivalent to the model in Tan et al. (2017). For fair comparisons, first, we use the hyper-parameters provided by Tan et al. (2017) instead of tuning them; second, all listed models are independent of external linguistics information, e.g., PoS, dependency parsing.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Based on the table, how does MTSA compare to the Bi-LSTM and Multi-CNN baselines in terms of performance and training time?. It outperforms the Bi-LSTM and Multi-CNN baselines in terms of performance and training time.']\n",
            "-----------------\n",
            "['Caption: Tensorized self-attention (TSA) Mechanism.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the purpose of the positional mask in the TSA mechanism?. 1024x768 resolutions', 'Caption: Tensorized self-attention (TSA) Mechanism.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the purpose of the positional mask in the TSA mechanism?. Positional mask']\n",
            "-----------------\n",
            "['Caption: The effect of the alpha parameter on recommendation accuracy at different sample sizes on the CLASS dataset. Left: cross-entropy loss; Middle: TOP1-max loss; Right: BPR-max loss.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the alpha parameter affect recommendation accuracy for different sample sizes on the CLASS dataset?. 2048x3659', 'Caption: The effect of the alpha parameter on recommendation accuracy at different sample sizes on the CLASS dataset. Left: cross-entropy loss; Middle: TOP1-max loss; Right: BPR-max loss.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the alpha parameter affect recommendation accuracy for different sample sizes on the CLASS dataset?. Cross-entropy loss']\n",
            "-----------------\n",
            "['Caption: Table 2: Recommendation accuracy with additional samples and different loss functions compared to item-kNN and the original GRU4Rec. Improvements over item-kNN and the original GRU4Rec (with TOP1 loss) results are shown in parentheses. Best results are typeset bold.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which combination of method and dataset achieved the highest Recall@20 score, and how much higher was it compared to the original GRU4Rec model?. 365897\\nYes', 'Caption: Table 2: Recommendation accuracy with additional samples and different loss functions compared to item-kNN and the original GRU4Rec. Improvements over item-kNN and the original GRU4Rec (with TOP1 loss) results are shown in parentheses. Best results are typeset bold.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which combination of method and dataset achieved the highest Recall@20 score, and how much higher was it compared to the original GRU4Rec model?. Item-kNN']\n",
            "-----------------\n",
            "[\"Caption: Performance of GRU4Rec relative to the baseline in the online A/B test.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the performance of GRU4Rec relative to the baseline in terms of watch time?. 10% less than control group's average response rate, but more responders watched videos for at least a minute compared with controls (85 vs %67) as well as higher click-through rates on ad links from video players versus\", 'Caption: Performance of GRU4Rec relative to the baseline in the online A/B test.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the performance of GRU4Rec relative to the baseline in terms of watch time?. 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% 105% 110% 115% 120% 125% 130% 135% 140% 145% 150% 155% 160% 165% 170% 175% 180% 185% 190% 195% 200% 205% 210% 215% 220% 225% 230% 235% 240% 245% 250% 255% 260% 265% 270% 275% 280']\n",
            "-----------------\n",
            "['Caption: Table 1: Properties of the datasets.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset contains the most interactions (events) in the training set and how much larger is it compared to the dataset with the least interactions in the training set?. 20,365 events more than any other event type for a given session number within each video clip\\nYes', 'Caption: Table 1: Properties of the datasets.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset contains the most interactions (events) in the training set and how much larger is it compared to the dataset with the least interactions in the training set?. The dataset that contains the most interactions in the training set has 10,000 interactions, while the dataset with the least interactions in the training set has 1,000 interactions.']\n",
            "-----------------\n",
            "[\"Caption: Mini-batch based negative sampling.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the purpose of negative sampling?. 100% positive samples are not possible, so we use a small number (e g., 5) from each class as representative for all instances within that classes\\nYes it's true\", 'Caption: Mini-batch based negative sampling.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the purpose of negative sampling?. Negative Sampling']\n",
            "-----------------\n",
            "['Caption: Training times with different sample sizes on the CLASS dataset.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the training time of the different losses change as the number of additional samples increases?. 10% increase for each additional ten thousand samples\\nYes, that is correct! The graph shows a clear trend where increasing by an extra hundred thousands leads only about five percent more improvement over previous batches when using cross-entropy loss function', 'Caption: Training times with different sample sizes on the CLASS dataset.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the training time of the different losses change as the number of additional samples increases?. Slower']\n",
            "-----------------\n",
            "['Caption: Results with unified embeddings. Relative improvement over the same network without unified item representation is shown in the parentheses.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset has the highest Recall@20 and MRR@20?. 1984', 'Caption: Results with unified embeddings. Relative improvement over the same network without unified item representation is shown in the parentheses.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset has the highest Recall@20 and MRR@20?. MNIST']\n",
            "-----------------\n",
            "['Caption: Median negative gradients of BPR and BPR-max w.r.t. the target score against the rank of the target item. Left: only minibatch samples are used (minibatch size: 32); Center: 2048 additional negative samples were added to the minibatch samples; Right: same setting as the center, focusing on ranks 0-200.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the addition of negative samples affect the gradient of BPR and BPR-max with respect to the target score?. 19657/Yes', 'Caption: Median negative gradients of BPR and BPR-max w.r.t. the target score against the rank of the target item. Left: only minibatch samples are used (minibatch size: 32); Center: 2048 additional negative samples were added to the minibatch samples; Right: same setting as the center, focusing on ranks 0-200.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the addition of negative samples affect the gradient of BPR and BPR-max with respect to the target score?. The gradient of BPR and BPR-max with respect to the target score decreases as the number of negative samples increases.']\n",
            "-----------------\n",
            "['Caption: Table 1: Performance comparison of different methods.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which method performed best according to the P@1 metric for the QA-Expert task, and how much better did it perform compared to the average P@1 score of the D2V method? . 05689743; 0.0% worse than avg\\nYes/No no yes', 'Caption: Table 1: Performance comparison of different methods.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which method performed best according to the P@1 metric for the QA-Expert task, and how much better did it perform compared to the average P@1 score of the D2V method? .']\n",
            "-----------------\n",
            "['Caption: Figure 1: Decline probability of an expert conditioned on whether or not the expert has a correlated “friend” who has already declined on two datasets: QA-Expert and Paper-Reviewer, and our online evaluation of journal reviewer recommendation.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the relationship between the decline probability of an expert and whether or not they have a \"friend\" who has already declined?. 0% (No)\\nYes I am interested only if it\\'s yes!', 'Caption: Figure 1: Decline probability of an expert conditioned on whether or not the expert has a correlated “friend” who has already declined on two datasets: QA-Expert and Paper-Reviewer, and our online evaluation of journal reviewer recommendation.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the relationship between the decline probability of an expert and whether or not they have a \"friend\" who has already declined?. Correlated']\n",
            "-----------------\n",
            "['Caption: Classification error comparison with state-of-the-arts. The results of DenseNets are based on the networks without bottlenecks. The DMRNet-Wide is the wide version of a DMRNet, 4× wider, i.e., the widths of the threes stages are 64, 128, and 256, respectively.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which network architecture has the highest accuracy on the CIFAR-10 dataset?. 3D Convolutional Neural Network (CNN)\\nYes', 'Caption: Classification error comparison with state-of-the-arts. The results of DenseNets are based on the networks without bottlenecks. The DMRNet-Wide is the wide version of a DMRNet, 4× wider, i.e., the widths of the threes stages are 64, 128, and 256, respectively.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which network architecture has the highest accuracy on the CIFAR-10 dataset?. DenseNet-Wide']\n",
            "-----------------\n",
            "['Caption: (a) a deep residual network; (b) a network built by stacking inception-like blocks; (c) our deep merge-and-run neural network built by stacking merge-and-run blocks. The trapezoid shape indicates that down-sampling occurs in the corresponding layer, and the dashed line denotes a projection shortcut as in [7].. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the difference between a deep residual network and a network built by stacking inception-like blocks?. 1024x568 pixels\\nYes | no', 'Caption: (a) a deep residual network; (b) a network built by stacking inception-like blocks; (c) our deep merge-and-run neural network built by stacking merge-and-run blocks. The trapezoid shape indicates that down-sampling occurs in the corresponding layer, and the dashed line denotes a projection shortcut as in [7].. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the difference between a deep residual network and a network built by stacking inception-like blocks?. A deep residual network is a type of convolutional neural network, while a network built by stacking inception-like blocks is also a type of convolutional neural network.']\n",
            "-----------------\n",
            "['Caption: Illustrating the building blocks: (a) Two residual blocks; (b) An inception-like block; (c) A merge-and-run block. (a) corresponds to two blocks in ResNets and assembles two residual branches sequentially while (b) and (c) both assemble the same two residual branches in parallel. (b) and (c) adopt two different skip connections: identity mappings and our proposed merge-andrun mappings. The dot circle denotes the average operation, and the solid circle denotes the sum operation.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the difference between the residual block and the merge-and-run block?. 1098765432 times yes | no\\n|Answer In One Word :Yes', 'Caption: Illustrating the building blocks: (a) Two residual blocks; (b) An inception-like block; (c) A merge-and-run block. (a) corresponds to two blocks in ResNets and assembles two residual branches sequentially while (b) and (c) both assemble the same two residual branches in parallel. (b) and (c) adopt two different skip connections: identity mappings and our proposed merge-andrun mappings. The dot circle denotes the average operation, and the solid circle denotes the sum operation.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the difference between the residual block and the merge-and-run block?. The main difference between the residual block and the merge-and-run block is that the merge-and-run block uses a different skip connection than the identity mapping used in the residual block.']\n",
            "-----------------\n",
            "['Caption: Comparing the optimization of ResNets and the DMRNets with the same number of layers/parameters. The vertical axis corresponds to training losses and testing errors, and the horizontal axis corresponds to #epochs.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the training loss of DMRNet compare to that of ResNet on the CIFAR-10 dataset with L = 30?. 25% less than resnet at epoch=48 (Residual Network)', 'Caption: Comparing the optimization of ResNets and the DMRNets with the same number of layers/parameters. The vertical axis corresponds to training losses and testing errors, and the horizontal axis corresponds to #epochs.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the training loss of DMRNet compare to that of ResNet on the CIFAR-10 dataset with L = 30?.']\n",
            "-----------------\n",
            "[\"Caption: Comparing the distributions of the path lengths for three networks. Different networks: (avg length ± std). Left: L = 9. Right: L = 24.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which network has the shortest average path length when L = 9?. 10 meter long cable with a standard deviation between two cables that are both about twice as thick but only half again longer than each other's diameter? The left side is correct, it would be shorter by an order magnitude compared to right which\", 'Caption: Comparing the distributions of the path lengths for three networks. Different networks: (avg length ± std). Left: L = 9. Right: L = 24.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which network has the shortest average path length when L = 9?. Left']\n",
            "-----------------\n",
            "['Caption: The validation (single 224×224 center crop) and training errors (%) of ResNet-101 (44.5M) and our DMRNet (43.3M) on ImageNet.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which model has the lowest Top-1 validation error on ImageNet?. 8976a', 'Caption: The validation (single 224×224 center crop) and training errors (%) of ResNet-101 (44.5M) and our DMRNet (43.3M) on ImageNet.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which model has the lowest Top-1 validation error on ImageNet?. ResNet-101']\n",
            "-----------------\n",
            "['Caption: Comparison between merge-and-run mappings and identity mappings. Sharing = share the first conv. and the last FC.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Does sharing the first convolutional layer and the last fully connected layer improve the accuracy of the merge-and-run mapping?. 1024x768 359 ± 10% 1e+05 1s √v=10cm λ=0.001 σ=0.001', 'Caption: Comparison between merge-and-run mappings and identity mappings. Sharing = share the first conv. and the last FC.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Does sharing the first convolutional layer and the last fully connected layer improve the accuracy of the merge-and-run mapping?. Yes']\n",
            "-----------------\n",
            "['Caption: Illustrating how the testing errors of residual networks change as the average path length increases. The results are reported on CIFAR-10.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the classification error of a residual network change as the average path length increases?. 25% increase for each additional layer added, but it plateaus at around five layers before increasing again with more than sixteen neurons per hidden unit (layer).', 'Caption: Illustrating how the testing errors of residual networks change as the average path length increases. The results are reported on CIFAR-10.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the classification error of a residual network change as the average path length increases?. Increases']\n",
            "-----------------\n",
            "['Caption: (left) Variation of test accuracy on CUB-200-2011 with logarithmic variation in hyperparameter λ. (right) Convergence plot of GoogLeNet on CUB-200-2011.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the test accuracy of the different models vary with the hyperparameter λ?. 3569478, 3569478 are two numbers that have a difference between them but they both contain three digits each followed by four zeros after it which is not an actual number value as such for this context', 'Caption: (left) Variation of test accuracy on CUB-200-2011 with logarithmic variation in hyperparameter λ. (right) Convergence plot of GoogLeNet on CUB-200-2011.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the test accuracy of the different models vary with the hyperparameter λ?. Logarithmic']\n",
            "-----------------\n",
            "[\"Caption: Pairwise Confusion (PC) obtains state-of-the-art performance on six widelyused fine-grained visual classification datasets (A-F). Improvement over the baseline model is reported as (∆). All results averaged over 5 trials.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which method achieves the highest Top-1 accuracy on the CUB-200-2011 dataset?. √ Yes, Method A\\nDescription The authors of this paper have achieved a new record for top performing models using convolutional neural networks with backpropagation training algorithms by improving upon their previous best result from last year's competition at CVPR\", 'Caption: Pairwise Confusion (PC) obtains state-of-the-art performance on six widelyused fine-grained visual classification datasets (A-F). Improvement over the baseline model is reported as (∆). All results averaged over 5 trials.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which method achieves the highest Top-1 accuracy on the CUB-200-2011 dataset?. State-of-the-art']\n",
            "-----------------\n",
            "['Caption: Fig. 3. Pairwise Confusion (PC) obtains improved localization performance, as demonstrated here with Grad-CAM heatmaps of the CUB-200-2011 dataset images (left) with a VGGNet-16 model trained without PC (middle) and with PC (right). The objects in (a) and (b) are correctly classified by both networks, and (c) and (d) are correctly classified by PC, but not the baseline network (VGG-16). For all cases, we consistently observe a tighter and more accurate localization with PC, whereas the baseline VGG-16 network often latches on to artifacts, even while making correct predictions.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does Pairwise Confusion (PC) affect the localization ability of a CNN?. \\u200bYes | Source', 'Caption: Fig. 3. Pairwise Confusion (PC) obtains improved localization performance, as demonstrated here with Grad-CAM heatmaps of the CUB-200-2011 dataset images (left) with a VGGNet-16 model trained without PC (middle) and with PC (right). The objects in (a) and (b) are correctly classified by both networks, and (c) and (d) are correctly classified by PC, but not the baseline network (VGG-16). For all cases, we consistently observe a tighter and more accurate localization with PC, whereas the baseline VGG-16 network often latches on to artifacts, even while making correct predictions.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does Pairwise Confusion (PC) affect the localization ability of a CNN?. Pairwise Confusion (PC) improves the localization ability of a Convolutional Neural Network (CNN).']\n",
            "-----------------\n",
            "['Caption: A comparison of fine-grained visual classification (FGVC) datasets with largescale visual classification (LSVC) datasets. FGVC datasets are significantly smaller and noisier than LSVC datasets.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset has the highest number of samples per class?. 1024x768', 'Caption: A comparison of fine-grained visual classification (FGVC) datasets with largescale visual classification (LSVC) datasets. FGVC datasets are significantly smaller and noisier than LSVC datasets.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset has the highest number of samples per class?. Largescale Visual Classification']\n",
            "-----------------\n",
            "['Caption: Table 3. Experiments with ImageNet and CIFAR show that datasets with large intraclass variation and high inter-class similarity benefit from optimization with Pairwise Confusion. Only the mean accuracy over 3 Imagenet-Random experiments is shown.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which dataset benefited more from the Pairwise Confusion (PC) optimization method: ImageNet-Dogs or ImageNet-Random?. 10% of images are mislabeled as dogs, but only a few have been labled incorrectly by humans; most labels were generated randomly using an algorithm called random walk down error pathway which has no prior knowledge about dog breeds\\nYes', 'Caption: Table 3. Experiments with ImageNet and CIFAR show that datasets with large intraclass variation and high inter-class similarity benefit from optimization with Pairwise Confusion. Only the mean accuracy over 3 Imagenet-Random experiments is shown.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which dataset benefited more from the Pairwise Confusion (PC) optimization method: ImageNet-Dogs or ImageNet-Random?. ImageNet-Dogs']\n",
            "-----------------\n",
            "['Caption: Fig. 1. CNN training pipeline for Pairwise Confusion (PC). We employ a Siamese-like architecture, with individual cross entropy calculations for each branch, followed by a joint energy-distance minimization loss. We split each incoming batch of samples into two mini-batches, and feed the network pairwise samples.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the role of the shared weights in the Siamese-like architecture shown in the first figure?. 2018', 'Caption: Fig. 1. CNN training pipeline for Pairwise Confusion (PC). We employ a Siamese-like architecture, with individual cross entropy calculations for each branch, followed by a joint energy-distance minimization loss. We split each incoming batch of samples into two mini-batches, and feed the network pairwise samples.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the role of the shared weights in the Siamese-like architecture shown in the first figure?. The role of the shared weights in the Siamese-like architecture shown in the first figure is to reduce the number of parameters that need to be optimized during the training process.']\n",
            "-----------------\n",
            "['Caption: Comparison of the performance of ALOQ, MAP and RQ-ALOQ policies when p(θ) must be estimated. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which policy resulted in the highest average cost?. 10% Policy (Aloq Policies Cost Range)', 'Caption: Comparison of the performance of ALOQ, MAP and RQ-ALOQ policies when p(θ) must be estimated. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which policy resulted in the highest average cost?. ALOQ']\n",
            "-----------------\n",
            "['Caption: Performance and learned configurations on the robotic arm joint breakage task.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which of the algorithms performs the best on the robotic arm joint breakage task?. 1024x768\\nNo, it is not possible for me as a human being with my current knowledge base at this time (as I am writing) determine which algorithm would perform better than another based solely upon their respective resolution', 'Caption: Performance and learned configurations on the robotic arm joint breakage task.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which of the algorithms performs the best on the robotic arm joint breakage task?. LSTM\\n\\nAnswer: The LSTM (Long Short-Term Memory) algorithm performs the best on the robotic arm joint breakage task.']\n",
            "-----------------\n",
            "['Caption: Quartiles of the expected cost of the final π̂∗ estimated by each algorithm across 20 independent runs for the Robotic Arm Simulator experiments.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which algorithm performs the best in the Joint Breakage experiment?. λαβογράφωσις', 'Caption: Quartiles of the expected cost of the final π̂∗ estimated by each algorithm across 20 independent runs for the Robotic Arm Simulator experiments.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which algorithm performs the best in the Joint Breakage experiment?. Algol']\n",
            "-----------------\n",
            "['Caption: Performance of Reinforce and TRPO on the Robotic Arm Simulator experiments.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which algorithm performed better on the arm breakage task?. 0123456789TRPOTRQOEAECBRLFGHJKLMNOPRSUYVWXZ', 'Caption: Performance of Reinforce and TRPO on the Robotic Arm Simulator experiments.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which algorithm performed better on the arm breakage task?. TRPO']\n",
            "-----------------\n",
            "['Caption: Comparison of runtime of all methods on the FSRE test functions.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Between WSN and ALOQ, which method is the most efficient in terms of runtime for both F-SRE1 and F-SRE2?. 05689734\\nNo', 'Caption: Comparison of runtime of all methods on the FSRE test functions.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Between WSN and ALOQ, which method is the most efficient in terms of runtime for both F-SRE1 and F-SRE2?. ALOQ']\n",
            "-----------------\n",
            "['Caption: Comparison of performance of all methods on the F-SRE test functions (higher is better) .. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which method performs the best on the F-SRE1 test function?. 025678943\\nQuestion no longer exists yes it does perform well with a value around that number but not quite as good at lower values than some other algorithms tested herein are you sure? I am unsure if this', 'Caption: Comparison of performance of all methods on the F-SRE test functions (higher is better) .. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which method performs the best on the F-SRE1 test function?. 1']\n",
            "-----------------\n",
            "['Caption: ALOQ models the return f as a function of (π, θ); (a) the predicted mean based on some observed data; (b) the predicted return of π = 1.5 for different θ, together with the uncertainty associated with them, given p(θ); (c) ALOQ marginalises out θ and computes f̄(π) and its associated uncertainty, which is used to actively select π.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the predicted return change as a function of θ for a fixed value of π = 1.5?. 0% no | 28 % yes\\nNo', 'Caption: ALOQ models the return f as a function of (π, θ); (a) the predicted mean based on some observed data; (b) the predicted return of π = 1.5 for different θ, together with the uncertainty associated with them, given p(θ); (c) ALOQ marginalises out θ and computes f̄(π) and its associated uncertainty, which is used to actively select π.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the predicted return change as a function of θ for a fixed value of π = 1.5?. The predicted return of π = 1.5 for different θ, together with the uncertainty associated with them, given p(θ); (c) ALOQ marginalises out θ and computes f̄(π) and its associated uncertainty, which is used to actively select π.']\n",
            "-----------------\n",
            "['Caption: Contour plot of F-SRE1 and F-SRE2 (values in SRE region have been reduced by a factor of 10).. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the difference between the \"True max\" and the \"ALOQ\" curves?. 35% True Maximum occurs at lower values than ALOQ for both cases, but it\\'s more pronounced on case B compared with CaseA\\nNo true maximum value exists below ALDO threshold level which means that there are no significant', 'Caption: Contour plot of F-SRE1 and F-SRE2 (values in SRE region have been reduced by a factor of 10).. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the difference between the \"True max\" and the \"ALOQ\" curves?. The ALOQ curve is lower than the True max curve']\n",
            "-----------------\n",
            "['Caption: Comparison of performance of all methods on the modified Branin and Hartmann 6 test functions used by Williams, Santner, and Notz.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which method performs the best on the Branin function?. 108529743\\nQuestion Type: Multiple Choice Quizlet', 'Caption: Comparison of performance of all methods on the modified Branin and Hartmann 6 test functions used by Williams, Santner, and Notz.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which method performs the best on the Branin function?. Exponential moving average']\n",
            "-----------------\n",
            "['Caption: Overview of the methodology of reconstructing implicit warrants for argument reasoning comprehension.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?. 1024x768', 'Caption: Overview of the methodology of reconstructing implicit warrants for argument reasoning comprehension.. Please provide a brief answer to the following question after looking into the input image and caption. Question: What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?. 1. 2. 3. 4. 5.']\n",
            "-----------------\n",
            "['Caption: Figure 3: Cohen’s κ agreement for stance annotation on 98 comments. As a trade-off between reducing costs (i.e., discarding fewer instances) and increasing reliability, we chose 5 annotators and a threshold of 0.95 for this task, which resulted in κ = 0.58 (moderate to substantial agreement).. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: What is the relationship between the number of workers per \"expert\" and Cohen\\'s kappa agreement for stance annotation?. 1st degree correlation with no direct causation linkage but indirectly related through cost savings by not having more experts than necessary while maintaining high accuracy level using only five expert annotations at an acceptable value that can be considered moderately reliable as', 'Caption: Figure 3: Cohen’s κ agreement for stance annotation on 98 comments. As a trade-off between reducing costs (i.e., discarding fewer instances) and increasing reliability, we chose 5 annotators and a threshold of 0.95 for this task, which resulted in κ = 0.58 (moderate to substantial agreement).. Please provide a brief answer to the following question after looking into the input image and caption. Question: What is the relationship between the number of workers per \"expert\" and Cohen\\'s kappa agreement for stance annotation?. Moderate to substantial']\n",
            "-----------------\n",
            "['Caption: Accuracy of each approach (humans and systems) on the development set and test set, respectively.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which approach performs best on the development set?. 1056789432', 'Caption: Accuracy of each approach (humans and systems) on the development set and test set, respectively.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which approach performs best on the development set?. Humans']\n",
            "-----------------\n",
            "['Caption: Human upper bounds on the argument reasoning comprehension task with respect to education and formal training in reasoning, logic, or argumentation. For each configuration, the mean values are displayed together with the number of participants (above the bar) and with their standard deviations (error bars).. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?. 10% no\\nYes | No  25789463-cbaa-bdfd-fcae-cbcdedebbefaaa/iq_test_results?imageSize=large&', 'Caption: Human upper bounds on the argument reasoning comprehension task with respect to education and formal training in reasoning, logic, or argumentation. For each configuration, the mean values are displayed together with the number of participants (above the bar) and with their standard deviations (error bars).. Please provide a brief answer to the following question after looking into the input image and caption. Question: Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?. Yes']\n",
            "-----------------\n",
            "['Caption: Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process. *Input instances were filtered by their ‘logic score’ assigned in Step 6, such that the weakest 30% were discarded. A more detailed description is available in the readme file of the source code.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: Which step in the methodology resulted in the largest decrease in the size of the dataset?. 1928574-large_increase\\nYes', 'Caption: Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process. *Input instances were filtered by their ‘logic score’ assigned in Step 6, such that the weakest 30% were discarded. A more detailed description is available in the readme file of the source code.. Please provide a brief answer to the following question after looking into the input image and caption. Question: Which step in the methodology resulted in the largest decrease in the size of the dataset?.']\n",
            "-----------------\n",
            "['Caption: Figure 5: Intra-warrant attention. Only the attention vector for the warrant W1 is shown; the attention vector for W0 is constructed analogously. Grey areas represent a modification with additional context.. Is the input image and caption helpful to answer the following question. Answer in one word - Yes or No. Question: How does the intra-warrant attention mechanism work?. 24 bit', 'Caption: Figure 5: Intra-warrant attention. Only the attention vector for the warrant W1 is shown; the attention vector for W0 is constructed analogously. Grey areas represent a modification with additional context.. Please provide a brief answer to the following question after looking into the input image and caption. Question: How does the intra-warrant attention mechanism work?. The attention vector for the warrant W1 is shown, the attention vector for W0 is constructed analogously']\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the responses folder\n",
        "!zip -r ./responses-instruct-blip.zip ./responses\n",
        "\n",
        "#Download the file to local desktop\n",
        "from google.colab import files\n",
        "files.download(\"./responses-instruct-blip.zip\")"
      ],
      "metadata": {
        "id": "J7JX8vXtMRaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5121056-30af-4233-8c0c-03c454d935f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: responses/ (stored 0%)\n",
            "  adding: responses/1703.00060v2_response.json (deflated 65%)\n",
            "  adding: responses/1611.04684v1_response.json (deflated 73%)\n",
            "  adding: responses/1811.08481v2_response.json (deflated 82%)\n",
            "  adding: responses/1704.00774v3_response.json (deflated 60%)\n",
            "  adding: responses/1703.07015v3_response.json (deflated 78%)\n",
            "  adding: responses/1803.04572v2_response.json (deflated 77%)\n",
            "  adding: responses/1812.06589v2_response.json (deflated 76%)\n",
            "  adding: responses/1812.00108v4_response.json (deflated 75%)\n",
            "  adding: responses/1809.00263v5_response.json (deflated 78%)\n",
            "  adding: responses/1901.00398v2_response.json (deflated 74%)\n",
            "  adding: responses/1708.02153v2_response.json (deflated 74%)\n",
            "  adding: responses/1709.02418v2_response.json (deflated 69%)\n",
            "  adding: responses/1811.02721v3_response.json (deflated 78%)\n",
            "  adding: responses/1805.02349v2_response.json (deflated 57%)\n",
            "  adding: responses/1606.07384v2_response.json (deflated 77%)\n",
            "  adding: responses/1809.01989v2_response.json (deflated 66%)\n",
            "  adding: responses/1811.07073v3_response.json (deflated 82%)\n",
            "  adding: responses/1704.04539v2_response.json (deflated 77%)\n",
            "  adding: responses/1809.00458v1_response.json (deflated 82%)\n",
            "  adding: responses/1809.03449v3_response.json (deflated 79%)\n",
            "  adding: responses/1805.00912v4_response.json (deflated 77%)\n",
            "  adding: responses/1804.05936v2_response.json (deflated 76%)\n",
            "  adding: responses/1611.04363v2_response.json (deflated 73%)\n",
            "  adding: responses/1707.06320v2_response.json (deflated 78%)\n",
            "  adding: responses/1805.08751v2_response.json (deflated 57%)\n",
            "  adding: responses/1811.08257v1_response.json (deflated 78%)\n",
            "  adding: responses/1703.10730v2_response.json (deflated 77%)\n",
            "  adding: responses/1611.03780v2_response.json (deflated 79%)\n",
            "  adding: responses/1603.03833v4_response.json (deflated 77%)\n",
            "  adding: responses/1603.00286v5_response.json (deflated 77%)\n",
            "  adding: responses/1701.06171v4_response.json (deflated 80%)\n",
            "  adding: responses/1703.00899v2_response.json (deflated 72%)\n",
            "  adding: responses/1811.06635v1_response.json (deflated 73%)\n",
            "  adding: responses/1804.05938v2_response.json (deflated 70%)\n",
            "  adding: responses/1705.02798v6_response.json (deflated 76%)\n",
            "  adding: responses/1710.05654v2_response.json (deflated 81%)\n",
            "  adding: responses/1705.02946v3_response.json (deflated 77%)\n",
            "  adding: responses/1704.07854v4_response.json (deflated 80%)\n",
            "  adding: responses/1802.07351v2_response.json (deflated 81%)\n",
            "  adding: responses/1803.01128v3_response.json (deflated 79%)\n",
            "  adding: responses/1708.05239v3_response.json (deflated 71%)\n",
            "  adding: responses/1805.07567v2_response.json (deflated 81%)\n",
            "  adding: responses/1702.03584v3_response.json (deflated 74%)\n",
            "  adding: responses/1803.06506v3_response.json (deflated 77%)\n",
            "  adding: responses/1803.02750v3_response.json (deflated 81%)\n",
            "  adding: responses/1901.00056v2_response.json (deflated 74%)\n",
            "  adding: responses/1802.07222v1_response.json (deflated 55%)\n",
            "  adding: responses/1705.09882v2_response.json (deflated 82%)\n",
            "  adding: responses/1804.00863v3_response.json (deflated 79%)\n",
            "  adding: responses/1809.01246v1_response.json (deflated 82%)\n",
            "  adding: responses/1706.00633v4_response.json (deflated 80%)\n",
            "  adding: responses/1705.07384v2_response.json (deflated 64%)\n",
            "  adding: responses/1804.07931v2_response.json (deflated 76%)\n",
            "  adding: responses/1707.00524v2_response.json (deflated 79%)\n",
            "  adding: responses/1811.10673v1_response.json (deflated 80%)\n",
            "  adding: responses/1705.09966v2_response.json (deflated 82%)\n",
            "  adding: responses/1805.08465v3_response.json (deflated 68%)\n",
            "  adding: responses/1708.01425v4_response.json (deflated 78%)\n",
            "  adding: responses/1705.09296v2_response.json (deflated 80%)\n",
            "  adding: responses/1703.02507v3_response.json (deflated 81%)\n",
            "  adding: responses/1802.07459v2_response.json (deflated 71%)\n",
            "  adding: responses/1701.03077v10_response.json (deflated 82%)\n",
            "  adding: responses/1809.02731v3_response.json (deflated 76%)\n",
            "  adding: responses/1804.04410v2_response.json (deflated 75%)\n",
            "  adding: responses/1708.06832v3_response.json (deflated 74%)\n",
            "  adding: responses/1704.05958v2_response.json (deflated 78%)\n",
            "  adding: responses/1705.08016v3_response.json (deflated 77%)\n",
            "  adding: responses/1707.08608v3_response.json (deflated 80%)\n",
            "  adding: responses/1710.06177v2_response.json (deflated 78%)\n",
            "  adding: responses/1803.04383v2_response.json (deflated 78%)\n",
            "  adding: responses/1805.04609v3_response.json (deflated 72%)\n",
            "  adding: responses/1906.06589v3_response.json (deflated 73%)\n",
            "  adding: responses/1812.10735v2_response.json (deflated 76%)\n",
            "  adding: responses/1805.04687v2_response.json (deflated 83%)\n",
            "  adding: responses/1704.05426v4_response.json (deflated 79%)\n",
            "  adding: responses/1703.04887v4_response.json (deflated 78%)\n",
            "  adding: responses/1811.09393v4_response.json (deflated 80%)\n",
            "  adding: responses/1612.02803v5_response.json (deflated 59%)\n",
            "  adding: responses/1704.08615v2_response.json (deflated 81%)\n",
            "  adding: responses/1812.00281v3_response.json (deflated 79%)\n",
            "  adding: responses/1805.06431v4_response.json (deflated 82%)\n",
            "  adding: responses/1805.01216v3_response.json (deflated 80%)\n",
            "  adding: responses/1708.03797v1_response.json (deflated 68%)\n",
            "  adding: responses/1707.01917v2_response.json (deflated 78%)\n",
            "  adding: responses/1611.07718v2_response.json (deflated 80%)\n",
            "  adding: responses/1809.04276v2_response.json (deflated 76%)\n",
            "  adding: responses/1803.03467v4_response.json (deflated 80%)\n",
            "  adding: responses/1608.02784v2_response.json (deflated 78%)\n",
            "  adding: responses/1804.04786v3_response.json (deflated 76%)\n",
            "  adding: responses/1702.08694v3_response.json (deflated 78%)\n",
            "  adding: responses/1705.10667v4_response.json (deflated 79%)\n",
            "  adding: responses/1811.02553v4_response.json (deflated 84%)\n",
            "  adding: responses/1809.03550v3_response.json (deflated 77%)\n",
            "  adding: responses/1804.05995v2_response.json (deflated 83%)\n",
            "  adding: responses/1804.01429v3_response.json (deflated 81%)\n",
            "  adding: responses/1710.01507v4_response.json (deflated 55%)\n",
            "  adding: responses/1803.05776v2_response.json (deflated 58%)\n",
            "  adding: responses/1706.00827v2_response.json (deflated 80%)\n",
            "  adding: responses/1906.10843v1_response.json (deflated 77%)\n",
            "  adding: responses/1709.00139v4_response.json (deflated 64%)\n",
            "  adding: responses/1809.03149v2_response.json (deflated 74%)\n",
            "  adding: responses/1605.07496v3_response.json (deflated 79%)\n",
            "  adding: responses/1704.07121v2_response.json (deflated 80%)\n",
            "  adding: responses/1611.05742v3_response.json (deflated 73%)\n",
            "  adding: responses/1708.00160v2_response.json (deflated 76%)\n",
            "  adding: responses/1805.06447v3_response.json (deflated 81%)\n",
            "  adding: responses/1706.08146v3_response.json (deflated 80%)\n",
            "  adding: responses/1709.02755v5_response.json (deflated 78%)\n",
            "  adding: responses/1706.04284v3_response.json (deflated 79%)\n",
            "  adding: responses/1611.02654v2_response.json (deflated 77%)\n",
            "  adding: responses/1705.07164v8_response.json (deflated 76%)\n",
            "  adding: responses/1706.03847v3_response.json (deflated 79%)\n",
            "  adding: responses/1804.07707v2_response.json (deflated 76%)\n",
            "  adding: responses/1709.08294v3_response.json (deflated 80%)\n",
            "  adding: responses/1707.00189v3_response.json (deflated 64%)\n",
            "  adding: responses/1804.07849v4_response.json (deflated 76%)\n",
            "  adding: responses/1706.04269v2_response.json (deflated 77%)\n",
            "  adding: responses/1707.01922v5_response.json (deflated 80%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d1cac062-511d-4bd3-9cc4-a5d33b3f7735\", \"responses-instruct-blip.zip\", 222963)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1CeYvOoB1jMEhEgJ_W2dm1OThMXptKCU6/view?usp=sharing\n",
        "!tar -xzvf pycocoevalcap_spiqa.tar.gz && rm -rf pycocoevalcap_spiqa.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHG-ZG9ZJbTB",
        "outputId": "3894234c-f1b9-4646-8953-4c62a6324d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CeYvOoB1jMEhEgJ_W2dm1OThMXptKCU6\n",
            "From (redirected): https://drive.google.com/uc?id=1CeYvOoB1jMEhEgJ_W2dm1OThMXptKCU6&confirm=t&uuid=57762b4a-46fa-47c9-a91a-cc2b73701020\n",
            "To: /content/pycocoevalcap_spiqa.tar.gz\n",
            "100% 44.6M/44.6M [00:00<00:00, 86.1MB/s]\n",
            "pycocoevalcap_spiqa/\n",
            "pycocoevalcap_spiqa/__pycache__/\n",
            "pycocoevalcap_spiqa/__pycache__/__init__.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/__pycache__/eval.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/__pycache__/eval.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/__pycache__/eval.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/bleu/\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu_scorer.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu_scorer.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/bleu/__pycache__/bleu_scorer.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/bleu/LICENSE\n",
            "pycocoevalcap_spiqa/bleu/bleu.py\n",
            "pycocoevalcap_spiqa/bleu/bleu_scorer.py\n",
            "pycocoevalcap_spiqa/cider/\n",
            "pycocoevalcap_spiqa/cider/__pycache__/\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider_scorer.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider_scorer.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/cider/__pycache__/cider_scorer.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/cider/cider.py\n",
            "pycocoevalcap_spiqa/cider/cider_scorer.py\n",
            "pycocoevalcap_spiqa/example/\n",
            "pycocoevalcap_spiqa/example/captions_val2014.json\n",
            "pycocoevalcap_spiqa/example/captions_val2014_fakecap_results.json\n",
            "pycocoevalcap_spiqa/example/coco_eval_example.py\n",
            "pycocoevalcap_spiqa/meteor/\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/meteor.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/meteor.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/meteor/__pycache__/meteor.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/meteor/data/\n",
            "pycocoevalcap_spiqa/meteor/data/paraphrase-en.gz\n",
            "pycocoevalcap_spiqa/meteor/meteor-1.5.jar\n",
            "pycocoevalcap_spiqa/meteor/meteor.py\n",
            "pycocoevalcap_spiqa/rouge/\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/rouge.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/rouge.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/rouge/__pycache__/rouge.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/rouge/rouge.py\n",
            "pycocoevalcap_spiqa/spice/\n",
            "pycocoevalcap_spiqa/spice/__pycache__/\n",
            "pycocoevalcap_spiqa/spice/__pycache__/__init__.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/__init__.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/get_stanford_models.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/get_stanford_models.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/spice.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/spice.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/__init__.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/spice.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/spice/__pycache__/get_stanford_models.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/spice/stanford-corenlp-full-2015-12-09/\n",
            "pycocoevalcap_spiqa/spice/stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0.jar\n",
            "pycocoevalcap_spiqa/spice/__init__.py\n",
            "pycocoevalcap_spiqa/spice/get_stanford_models.py\n",
            "pycocoevalcap_spiqa/spice/spice-1.0.jar\n",
            "pycocoevalcap_spiqa/spice/spice.py\n",
            "pycocoevalcap_spiqa/tokenizer/\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/ptbtokenizer.cpython-310.pyc\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/ptbtokenizer.cpython-38.pyc\n",
            "pycocoevalcap_spiqa/tokenizer/__pycache__/ptbtokenizer.cpython-39.pyc\n",
            "pycocoevalcap_spiqa/tokenizer/ptbtokenizer.py\n",
            "pycocoevalcap_spiqa/tokenizer/stanford-corenlp-3.4.1.jar\n",
            "pycocoevalcap_spiqa/.gitignore\n",
            "pycocoevalcap_spiqa/README.md\n",
            "pycocoevalcap_spiqa/eval.py\n",
            "pycocoevalcap_spiqa/license.txt\n",
            "pycocoevalcap_spiqa/setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python open_models_metrics.py --response_root ./responses"
      ],
      "metadata": {
        "id": "jUvH2b9l9Urr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}