{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApLdKYUxpx61"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install tiktoken\n",
        "!pip install blobfile\n",
        "!pip install torch\n",
        "!pip install docling_core\n",
        "!pip install transformers\n",
        "!pip install --upgrade pip\n",
        "!pip install setuptools==65.5.0 wheel ninja\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install \"accelerate>=0.26.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "4s_2Dptop1lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "# Set device explicitly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using Device:\", device)\n",
        "\n",
        "# 4-bit quantization configuration\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Model path\n",
        "model_path = \"moonshotai/Kimi-VL-A3B-Instruct\"\n",
        "\n",
        "# Load model (no .to(device) needed; handled by device_map)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"  # required for bitsandbytes to correctly map to GPU\n",
        ")\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "from transformers import AutoProcessor as DocProcessor, AutoModelForVision2Seq\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "# Initialize SmolDocling model\n",
        "doc_model_path = \"ds4sd/SmolDocling-256M-preview\"\n",
        "doc_processor = DocProcessor.from_pretrained(doc_model_path)\n",
        "doc_model = AutoModelForVision2Seq.from_pretrained(\n",
        "    doc_model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    _attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\",\n",
        ")"
      ],
      "metadata": {
        "id": "hHDEH6VyqVc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "MAX_TOKENS = 512\n",
        "def generate_doctags(image_path):\n",
        "    \"\"\"Generate structured document tags from image\"\"\"\n",
        "    image = load_image(image_path)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    prompt = doc_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = doc_processor(text=prompt, images=[image], return_tensors=\"pt\").to(doc_model.device)\n",
        "    generated_ids = doc_model.generate(**inputs, max_new_tokens=8192)\n",
        "    prompt_length = inputs.input_ids.shape[1]\n",
        "    trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
        "    return doc_processor.batch_decode(trimmed_generated_ids, skip_special_tokens=True)[0].strip()\n",
        "doctags_cache = {}\n",
        "def structured_qa(image_path, question, caption):\n",
        "    \"\"\"Generate answer with structured doctags context\"\"\"\n",
        "    # Generate doctags\n",
        "    if image_path in doctags_cache:\n",
        "        doctags = doctags_cache[image_path]\n",
        "    else:\n",
        "        doctags = generate_doctags(image_path)\n",
        "        doctags_cache[image_path] = doctags\n",
        "    if len(doctags_cache) > 50:\n",
        "        doctags_cache.pop(next(iter(doctags_cache)))\n",
        "\n",
        "    # Prepare prompt with strict answer format\n",
        "    structured_prompt = f\"\"\"### Context ###\n",
        "Image Structure Analysis:\n",
        "{doctags}\n",
        "\n",
        "Image Caption:\n",
        "{caption}\n",
        "\n",
        "### Question ###\n",
        "{question}\n",
        "\n",
        "### Instructions ###\n",
        "1. Analyze the doctags structure from the image\n",
        "2. Identify relevant sections\n",
        "3. Think step by step with the image, question and doctags as context\n",
        "4. Provide ONLY the final answer in this exact format:\n",
        "REASONING_STEPS: [your reasoning steps here]\n",
        "FINAL_ANSWER: [your concise answer here]\"\"\"\n",
        "\n",
        "    # Get VLM response\n",
        "    image = Image.open(image_path)\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image_path},\n",
        "            {\"type\": \"text\", \"text\": structured_prompt}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    text=processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "    # print(\"Prompt Text is\", text)\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\"),\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=MAX_TOKENS)\n",
        "    response = processor.batch_decode(\n",
        "        generated_ids[:, inputs.input_ids.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    # Extract final answer\n",
        "    # print(response)\n",
        "    final_answer = response.split(\"FINAL_ANSWER:\")[-1].strip()\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "nSYRMDwaqpDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/HJYao00/Mulberry.git\n",
        "# %cd Mulberry"
      ],
      "metadata": {
        "id": "S4OBQAuTqy26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import torch\n",
        "# from transformers import (\n",
        "#     AutoProcessor as DocProcessor,\n",
        "#     AutoModelForVision2Seq,\n",
        "#     LlavaNextProcessor,\n",
        "#     LlavaNextForConditionalGeneration,\n",
        "#     BitsAndBytesConfig\n",
        "# )\n",
        "# from transformers.image_utils import load_image\n",
        "\n",
        "# # Quantization configuration for efficient inference\n",
        "# quant_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True\n",
        "# )\n",
        "\n",
        "# # Initialize SmolDocling model for document structure analysis\n",
        "# doc_model_path = \"ds4sd/SmolDocling-256M-preview\"\n",
        "# doc_processor = DocProcessor.from_pretrained(doc_model_path)\n",
        "# doc_model = AutoModelForVision2Seq.from_pretrained(\n",
        "#     doc_model_path,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     # _attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\",\n",
        "# )\n",
        "\n",
        "# # Initialize Mulberry model\n",
        "# mulberry_model_path = 'HuanjinYao/Mulberry_llava_8b'\n",
        "# mulberry_processor = LlavaNextProcessor.from_pretrained(mulberry_model_path)\n",
        "# # mulberry_processor.num_additional_image_tokens = 1\n",
        "\n",
        "# mulberry_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "#     mulberry_model_path,\n",
        "#     quantization_config=quant_config,\n",
        "#     device_map='auto'\n",
        "# )\n",
        "\n",
        "# mulberry_processor.num_additional_image_tokens = 1"
      ],
      "metadata": {
        "id": "LTXni139qbl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_doctags(image_path):\n",
        "#     \"\"\"Generate structured document tags from image\"\"\"\n",
        "#     image = load_image(image_path)\n",
        "#     messages = [\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"image\"},\n",
        "#                 {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
        "#             ]\n",
        "#         },\n",
        "#     ]\n",
        "#     prompt = doc_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "#     inputs = doc_processor(text=prompt, images=[image], return_tensors=\"pt\").to(doc_model.device)\n",
        "#     generated_ids = doc_model.generate(**inputs, max_new_tokens=8192)\n",
        "#     prompt_length = inputs.input_ids.shape[1]\n",
        "#     trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
        "#     return doc_processor.batch_decode(trimmed_generated_ids, skip_special_tokens=True)[0].strip()\n",
        "# doctags_cache = {}\n",
        "# def output_process(answer):\n",
        "#     \"\"\"Clean up model output to extract relevant parts\"\"\"\n",
        "#     if \"<s>\" in answer:\n",
        "#         answer = answer.replace(\"<s>\", \"\").strip()\n",
        "#     if \"[/INST]\" in answer:\n",
        "#         answer = answer.split(\"[/INST]\")[1].strip()\n",
        "#     elif \"ASSISTANT:\" in answer:\n",
        "#         answer = answer.split(\"ASSISTANT:\")[1].strip()\n",
        "#     elif \"assistant\\n\" in answer:\n",
        "#         answer = answer.split(\"assistant\\n\")[1].strip()\n",
        "#     elif \"<|end_header_id|>\\n\\n\" in answer:\n",
        "#         answer = answer.split(\"<|end_header_id|>\\n\\n\")[2].strip()\n",
        "\n",
        "#     if \"</s>\" in answer:\n",
        "#         answer = answer.split(\"</s>\")[0].strip()\n",
        "#     elif \"<|im_end|>\" in answer:\n",
        "#         answer = answer.split(\"<|im_end|>\")[0].strip()\n",
        "#     elif \"<|eot_id|>\" in answer:\n",
        "#         answer = answer.split(\"<|eot_id|>\")[0].strip()\n",
        "#     return answer\n",
        "\n",
        "# def structured_qa_with_mulberry(image_path, question, only_output_final_answer=False):\n",
        "#     \"\"\"Generate answer with structured doctags context using Mulberry model\"\"\"\n",
        "#     # Generate doctags\n",
        "\n",
        "#     if image_path in doctags_cache:\n",
        "#         doctags = doctags_cache[image_path]\n",
        "#     else:\n",
        "#         doctags = generate_doctags(image_path)\n",
        "#         doctags_cache[image_path] = doctags\n",
        "#     if len(doctags_cache) > 50:\n",
        "#         doctags_cache.pop(next(iter(doctags_cache)))\n",
        "\n",
        "#     # doctags = generate_doctags(image_path)\n",
        "\n",
        "#     # Prepare prompt with doctags and structured format\n",
        "#     structured_prompt = f\"\"\"Generate an image description based on the question.\n",
        "# Then, provide a rationale to analyze the question.\n",
        "# Next, generate a step-by-step reasoning process to solve the problem. Ensure the steps are logical and concise.\n",
        "# Finally, provide a concise summary of the final answer in the following format: 'The final answer is: xxx'. If the question is multiple-choice, provide the options along with their content. If it is free-form, directly present the final result. Do not provide any explanation.\n",
        "\n",
        "# ### Context ###\n",
        "# Image Structure Analysis:\n",
        "# {doctags}\n",
        "\n",
        "# Format your response with the following sections, separated by ###:\n",
        "# ### Image Description:\n",
        "# ### Rationales:\n",
        "# ### Let's think step by step.\n",
        "# ### Step 1:\n",
        "# ### Step 2:\n",
        "# ...\n",
        "# ### The final answer is:\n",
        "\n",
        "# {question}\"\"\"\n",
        "\n",
        "#     # Get Mulberry VLM response\n",
        "#     images = [Image.open(image_path).convert(\"RGB\")]\n",
        "#     content = [\n",
        "#         {\"type\": 'text', \"text\": structured_prompt},\n",
        "#         {\"type\": \"image\"}\n",
        "#     ]\n",
        "\n",
        "#     conversation = [\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": content,\n",
        "#         }\n",
        "#     ]\n",
        "\n",
        "#     # Generate response\n",
        "#     prompt = mulberry_processor.apply_chat_template(\n",
        "#         conversation, add_generation_prompt=True\n",
        "#     )\n",
        "#     inputs = mulberry_processor(prompt, images, return_tensors=\"pt\").to(\n",
        "#         mulberry_model.device\n",
        "#     )\n",
        "\n",
        "#     kwargs = dict(\n",
        "#         do_sample=False,\n",
        "#         temperature=0.2,\n",
        "#         max_new_tokens=512,\n",
        "#         top_p=None,\n",
        "#         num_beams=1,\n",
        "#         repetition_penalty=1.0\n",
        "#     )\n",
        "\n",
        "#     output = mulberry_model.generate(**inputs, **kwargs)\n",
        "#     answer = mulberry_processor.decode(output[0], skip_special_tokens=True)\n",
        "#     answer = output_process(answer)\n",
        "\n",
        "#     # Extract only final answer if requested\n",
        "#     if only_output_final_answer:\n",
        "#         if len(answer.split('### The final answer is:')) == 2:\n",
        "#             answer = answer.split('### The final answer is:')[-1].strip()\n",
        "\n",
        "#     return answer\n"
      ],
      "metadata": {
        "id": "xp_KMPnSqfC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"./SPIQA_testA_Images.zip\"\n",
        "extract_path = \"\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "m4qVRdR2qkl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "file_path = \"./SPIQA_testA.json\"\n",
        "with open(file_path, 'r') as file:\n",
        "    text = json.load(file)\n",
        "\n",
        "data = []\n",
        "cols = ['paper', 'question', 'answer', 'reference_figure', 'reference_figure_caption']\n",
        "for paper in text.keys():\n",
        "    for question in text[paper]['qa']:\n",
        "        data.append([paper, question['question'], question['answer'], question['reference'], text[paper]['all_figures'][question['reference']]['caption']])\n",
        "\n",
        "\n",
        "test_df = pd.DataFrame(data, columns=cols)\n",
        "test_df['generated_answer'] = np.nan\n",
        "\n",
        "# only use this if you have an existing pdf with some answers\n",
        "existing_df = pd.read_csv(\"./spiqa-caption-512.csv\")\n",
        "print(len(existing_df))\n",
        "cols.append('generated_answer')\n",
        "existing_df.columns = cols\n",
        "existing_questions = set(existing_df[\"question\"])\n",
        "test_df = test_df[~test_df[\"question\"].isin(existing_questions)].reset_index(drop=True)\n",
        "\n",
        "test_df[\"image_path\"] = \"./SPIQA_testA_Images/\" + test_df[\"paper\"] + \"/\" + test_df[\"reference_figure\"]\n",
        "# test_df['exists'] = test_df['image_path'].apply(lambda x: os.path.exists(x))\n",
        "# test_df=test_df[test_df['exists']]\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "HdQGKhUbql7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import csv\n",
        "import gc\n",
        "from IPython.display import clear_output\n",
        "\n",
        "output_csv = './spiqa-caption-512.csv'\n",
        "\n",
        "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    try:\n",
        "        image_path = row['image_path']\n",
        "        paper = row['paper']\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "        reference_figure = row['reference_figure']\n",
        "        reference_figure_caption = row['reference_figure_caption']\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_answer = structured_qa(image_path, question, reference_figure_caption)\n",
        "            # or structured_qa_with_mulberry(image_path, question, only_output_final_answer=True)\n",
        "\n",
        "        with open(output_csv, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([paper, question, answer, reference_figure, reference_figure_caption, generated_answer])\n",
        "\n",
        "        del generated_answer\n",
        "        del image_path, paper, question, answer, reference_figure, reference_figure_caption\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on row {i} (image_path={row['image_path']}, question={row['question']}): {e}\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "1u6piGxAqrm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}